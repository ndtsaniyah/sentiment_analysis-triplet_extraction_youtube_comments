# -*- coding: utf-8 -*-
"""Analysis Sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r9Db-imXwBuFGBKJdPRnk-bZs5Pug8m9
"""

import ast
import csv
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np


freq_triplets = 0
freq_reviews = 0
freq_pos =0
freq_neu = 0
freq_neg = 0
aspects_list=[]
opinion_list=[]
aspects_tokens=[]
opinion_tokens=[]


with open('dataset (5).txt', encoding='utf-8') as topo_file:
    aspects =[]
    for index,line in enumerate(topo_file):
        freq_reviews+=1
        tagging_info = line.split("####")
        tagging = tagging_info[1]
        sentence = tagging_info[0]
        sentence_split = sentence.split(" ")
        res = ast.literal_eval(tagging)


        # print(res)
        for i,x in enumerate(res):
            sentiment = x[2]
            freq_triplets+=1
            aspect_index = x[0]
            opinion_index = x[1]
            aspects_list.append(aspect_index)
            opinion_list.append(opinion_index)

            aspect = sentence_split[(aspect_index[0]):(aspect_index[-1]) + 1]
            for targets in aspect:
              aspects_tokens.append(targets)

            opinion = sentence_split[(opinion_index[0]):(opinion_index[-1]) + 1]
            for opinions in opinion:
              opinion_tokens.append(opinions)

            if(sentiment == 'POS'):
              freq_pos+=1
            elif(sentiment == 'NEU'):
              freq_neu+=1
            else:
              freq_neg+=1


print(f"num of review data: {freq_reviews}")
print(f"num of triplets: {freq_triplets}")
print(f"num of POS sentiment: {freq_pos}")
print(f"num of NEU sentiment: {freq_neu}")
print(f"num of NEG sentiment: {freq_neg}")

aspects_length = [len(x) for x in aspects_list]
opinion_length = [len(x) for x in opinion_list]
# aspects_tokens = list(set(aspects_tokens))
# opinion_tokens = list(set(opinion_tokens))
aspects_tokens_length = [len(x) for x in aspects_tokens]
opinion_tokens_length = [len(x) for x in opinion_tokens]


def visualize_outlier(list,type,percentile):
  # Calculate the average, minimum, and maximum
  average = sum(list) / len(list)
  minimum = min(list)
  maximum = max(list)

  # Calculate upper maximum value (outlier)
  outlier = np.percentile(list, percentile)  # Adjust the percentile value as desired

  # Plotting the histogram
  plt.hist(list, bins=range(min(list), max(list) + 1))
  plt.xlabel(f'Panjang {type}')
  plt.ylabel('Frekuensi')
  plt.title(f'Distribusi panjang {type}')

  # Adding a line for the upper maximum
  plt.axvline(x=outlier, color='r', linestyle='--', label=f'{percentile} Percentile')

  plt.legend()
  plt.xticks(range(min(list), max(list) + 1))  # Set x-axis tick locations explicitly
  plt.show()
  print(f"Maximum {type} Length: {maximum}")
  print(f"Minimum {type} Length: {minimum}")
  print(f"Average {type} Length: {average}")
  print(f"Percentile {type} Length: {outlier}")

visualize_outlier(aspects_length,"Aspek Target",99)
visualize_outlier(opinion_length,"Opinion Term",99)
visualize_outlier(aspects_tokens_length,"Token Aspek Target",99)
visualize_outlier(opinion_tokens_length,"Token Opinion Term",99)

# percentage_pos = freq_pos/freq_reviews*100
# percentage_neu = freq_neu/freq_reviews*100
# percentage_neg = freq_neg/freq_reviews*100
# sentiment_counts = [percentage_pos, percentage_neu, percentage_neg]
sentiment_counts = [freq_pos, freq_neu, freq_neg]
sentiment_labels = ['Positif', 'Netral', 'Negatif']
bars = plt.bar(sentiment_labels, sentiment_counts, color=['green', 'orange', 'red'])
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, height, str(int(height)), ha='center', va='bottom')

# plt.pie(sentiment_counts, labels=None, colors=['green', 'orange', 'red'], autopct='%1.1f%%', shadow=True,
#         textprops={'color': 'white', 'weight': 'bold', 'size': 'medium'}, startangle=140)
plt.legend(sentiment_labels)
# plt.tight_layout()
plt.title("Distribusi Sentimen")
plt.show()

import ast
import csv
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np

sentence_lengths_words = []
sentence_lengths_chars = []

with open('dataset (5).txt', encoding='utf-8') as topo_file:
    for index,line in enumerate(topo_file):
        tagging_info = line.split("####")
        tagging = tagging_info[1]
        sentence = tagging_info[0]
        sentence_split = sentence.split(" ")
        res = ast.literal_eval(tagging)
        sentence_lengths_words.append(len(sentence_split))
        sentence_lengths_chars.append(''.join(sentence_split).replace(' ', '').__len__())

def print_min_max_avg(list,type):
  # Calculate the average, minimum, and maximum
  average = sum(list) / len(list)
  minimum = min(list)
  maximum = max(list)

  print(f"Maximum {type} Length: {maximum}")
  print(f"Minimum {type} Length: {minimum}")
  print(f"Average {type} Length: {average}")


# Membuat histogram panjang kalimat dalam kata
plt.figure(figsize=(10, 6))
plt.hist(sentence_lengths_words, bins=30, alpha=0.7, color='blue', edgecolor='black')
plt.title('Distribusi Panjang Kata dalam Kalimat')
plt.xlabel('Jumlah Kata')
plt.ylabel('Frekuensi')
plt.show()

print_min_max_avg(sentence_lengths_words,'Kata')
# Membuat histogram panjang kalimat dalam karakter
plt.figure(figsize=(10, 6))
plt.hist(sentence_lengths_chars, bins=30, alpha=0.7, color='purple', edgecolor='black')
plt.title('Distribusi Panjang Karakter dalam Kalimat')
plt.xlabel('Jumlah Karakter (tanpa spasi)')
plt.ylabel('Frekuensi')
plt.show()

print_min_max_avg(sentence_lengths_chars,'Karakter')

visualize_outlier(sentence_lengths_chars,"Karakter dalam Kalimat",99)

import ast
import csv
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np

freq_triplets = 0
freq_reviews = 0
freq_pos =0
freq_neu = 0
freq_neg = 0
aspects_list=[]
opinion_list=[]
aspects_tokens=[]
opinion_tokens=[]


with open('test.txt', encoding='utf-8') as topo_file:
    aspects =[]
    for index,line in enumerate(topo_file):
        freq_reviews+=1
        tagging_info = line.split("####")
        tagging = tagging_info[1]
        sentence = tagging_info[0]
        sentence_split = sentence.split(" ")
        res = ast.literal_eval(tagging)


        # print(res)
        for i,x in enumerate(res):
            sentiment = x[2]
            freq_triplets+=1
            aspect_index = x[0]
            opinion_index = x[1]
            aspects_list.append(aspect_index)
            opinion_list.append(opinion_index)

            aspect = sentence_split[(aspect_index[0]):(aspect_index[-1]) + 1]
            for targets in aspect:
              aspects_tokens.append(targets)

            opinion = sentence_split[(opinion_index[0]):(opinion_index[-1]) + 1]
            for opinions in opinion:
              opinion_tokens.append(opinions)

            if(sentiment == 'POS'):
              freq_pos+=1
            elif(sentiment == 'NEU'):
              freq_neu+=1
            else:
              freq_neg+=1


print(f"num of review data: {freq_reviews}")
print(f"num of triplets: {freq_triplets}")
print(f"num of POS sentiment: {freq_pos}")
print(f"num of NEU sentiment: {freq_neu}")
print(f"num of NEG sentiment: {freq_neg}")

# file_path = "split_sentence.txt"  # Replace with the path to your text file
file_path = "dataset (5).txt"  # Replace with the path to your text file

import ast
limit_words_aspect = 4
limit_words_opinion = 4
limit_tokens_aspect = 13
limit_tokens_opinion = 11
pos_to_keep = 2000

with open(file_path, encoding='utf-8') as topo_file:
    counter = 0
    final_res = []
    for index, line in enumerate(topo_file):
        outlier = False
        txt_data = line.split("####")
        tagging = txt_data[1]
        res = ast.literal_eval(tagging)
        final_tagged = []
        sentence = txt_data[0]
        sentence_split = sentence.split(" ")
        # print(txt_data)
        for i, x in enumerate(res):
            opinion_length = len(x[1])
            aspect_length = len(x[0])
            aspect_index = x[0]
            opinion_index = x[1]

            aspect = sentence_split[(aspect_index[0]):(aspect_index[-1]) + 1]

            target_len = 0
            for targets in aspect:
                if(len(targets)>target_len):
                    target_len = len(targets)

            opinion = sentence_split[(opinion_index[0]):(opinion_index[-1]) + 1]
            opinion_len = 0
            for opin in opinion:
                if(len(opin)>opinion_len):
                    opinion_len = len(opin)

            if (opinion_length > limit_words_opinion or aspect_length > limit_words_aspect or target_len > limit_tokens_aspect or opinion_len >limit_tokens_opinion):
                print(f"outlier detected: {line}")
                outlier=True
                counter += 1

            else:
                final_tagged.append(x)

        if len(final_tagged) != 0:
            c = f"{txt_data[0]} .####{final_tagged}"
            if outlier:
                print(c)
            final_res.append(c)
    # print(f"Opinions with more than {limit_words} words: {counter}")

# with open('test filter.txt', 'w') as f:
#     for c in final_res:
#         f.write(c)
#         f.write('\n')
comments_to_keep =[]
total_pos=0
total_neg=0
total_neu=0
for line in final_res:
    sentences = line.strip()
    counter_pos=0
    counter_neg=0
    counter_neu=0
    splitted = sentences.split("####")
    tagged_info = splitted[1]
    tagged_data_list = eval(tagged_info)
    for z in tagged_data_list:
        sen = z[2]
        if sen =="POS":
            counter_pos+=1
        if sen =="NEU":
            counter_neu+=1
        if sen =="NEG":
            counter_neg+=1


    if counter_neg+counter_neu ==0 and pos_to_keep ==0 :
        continue

    if counter_neg+counter_neu ==0:
        pos_to_keep-=1
    comments_to_keep.append(sentences)
    total_pos+=counter_pos
    total_neg+=counter_neg
    total_neu+=counter_neu

print(total_pos,total_neg,total_neu)
print(f"total lines: {len(comments_to_keep)}")

with open('ready_to_split.txt', 'w', encoding='utf-8') as f:
    for c in final_res:
        f.write(c)
        f.write('\n')

import ast

file_path = "dataset (5).txt"  # Ganti dengan path ke file teks Anda
limit_words_aspect = 4
limit_words_opinion = 4
limit_tokens_aspect = 13
limit_tokens_opinion = 11
pos_to_keep = 2000

def is_valid_index(index_list, sentence_length):
    return all(0 <= idx < sentence_length for idx in index_list)

def clean_annotations(sentence, annotations):
    sentence_split = sentence.split()
    sentence_length = len(sentence_split)
    final_annotations = []
    aspect_opinions = []  # Menyimpan kata aspek dan opini serta sentimennya
    for ann in annotations:
        aspect_indices = [idx for idx in ann[0] if idx < sentence_length]
        opinion_indices = [idx for idx in ann[1] if idx < sentence_length]
        if is_valid_index(aspect_indices, sentence_length) and is_valid_index(opinion_indices, sentence_length):
            final_annotations.append([aspect_indices, opinion_indices, ann[2]])
            aspect = ' '.join(sentence_split[aspect_indices[0]: aspect_indices[-1] + 1])
            opinion = ' '.join(sentence_split[opinion_indices[0]: opinion_indices[-1] + 1])
            aspect_opinions.append((aspect, opinion, ann[2]))
    return final_annotations, aspect_opinions

with open(file_path, encoding='utf-8') as topo_file:
    counter = 0
    final_res = []
    for index, line in enumerate(topo_file):
        outlier = False
        txt_data = line.split("####")
        sentence = txt_data[0].strip()
        if len(sentence) < 200:
            continue

        tagging = txt_data[1]
        res = ast.literal_eval(tagging)
        final_tagged = []

        sentence_split = sentence.split()
        for i, x in enumerate(res):
            opinion_length = len(x[1])
            aspect_length = len(x[0])
            aspect_index = x[0]
            opinion_index = x[1]

            aspect = sentence_split[(aspect_index[0]):(aspect_index[-1]) + 1] if aspect_index else []

            target_len = 0
            for targets in aspect:
                if(len(targets) > target_len):
                    target_len = len(targets)

            opinion = sentence_split[(opinion_index[0]):(opinion_index[-1]) + 1] if opinion_index else []
            opinion_len = 0
            for opin in opinion:
                if(len(opin) > opinion_len):
                    opinion_len = len(opin)

            if (opinion_length > limit_words_opinion or aspect_length > limit_words_aspect or target_len > limit_tokens_aspect or opinion_len > limit_tokens_opinion):
                print(f"outlier detected: {line}")
                outlier = True
                counter += 1
            else:
                final_tagged.append(x)

        final_tagged, aspect_opinions = clean_annotations(sentence, final_tagged)

        if final_tagged:
            c = f"{sentence}####{final_tagged}"
            if outlier:
                print(c)
            final_res.append(c)

comments_to_keep = []
total_pos = 0
total_neg = 0
total_neu = 0
for line in final_res:
    sentences = line.strip()
    counter_pos = 0
    counter_neg = 0
    counter_neu = 0
    splitted = sentences.split("####")
    tagged_info = splitted[1]
    tagged_data_list = eval(tagged_info)
    for z in tagged_data_list:
        sen = z[2]
        if sen == "POS":
            counter_pos += 1
        if sen == "NEU":
            counter_neu += 1
        if sen == "NEG":
            counter_neg += 1

    if counter_neg + counter_neu == 0 and pos_to_keep == 0:
        continue

    if counter_neg + counter_neu == 0:
        pos_to_keep -= 1
    comments_to_keep.append(sentences)
    total_pos += counter_pos
    total_neg += counter_neg
    total_neu += counter_neu

print(total_pos, total_neg, total_neu)
print(f"total lines: {len(comments_to_keep)}")

with open('ready_to_split.txt', 'w', encoding='utf-8') as f:
    for c in comments_to_keep:
        f.write(c)
        f.write('\n')

import requests
import json
import random
import ast

# Unduh daftar kata slang
slang_url = "https://raw.githubusercontent.com/louisowen6/NLP_bahasa_resources/master/combined_slang_words.txt"
slang_response = requests.get(slang_url)
with open("combined_slang_words.txt", "wb") as f:
    f.write(slang_response.content)

# file_path = "ready_to_split.txt"  # Ganti dengan path ke file teks Anda
file_path = "ready_to_split_merged.txt"  # Ganti dengan path ke file teks Anda

# Load synonym dictionary
# Try to open the file and handle potential JSONDecodeError
with open("dict.json", "r", encoding="utf-8") as f:
  synonym_dict = json.load(f)
    # Further inspection of 'dict.json' is needed to pinpoint the issue.
    # You might want to open the file in a text editor and look for syntax errors or unexpected characters.


# Load slang words
slang_dict = {}
with open("combined_slang_words.txt", "r", encoding="utf-8") as f:
    for line in f:
        parts = line.strip().split()
        if len(parts) >= 2:
            slang = parts[0]
            standard = ' '.join(parts[1:])
            slang_dict[standard] = slang

def augment_sentence(sentence):
    words = sentence.lower().split()  # Konversi ke huruf kecil dan pecah kalimat menjadi kata-kata
    new_words = []
    has_changed = False
    for word in words:
        synonym_found = False
        slang_found = False
        # Augmentasi dengan sinonim
        for key, value in synonym_dict.items():
            if word in value['sinonim']:
                synonyms_for_word = [synonym for synonym in value['sinonim'] if synonym != word]
                if synonyms_for_word:
                    new_word = random.choice(synonyms_for_word)
                    new_words.append(new_word)
                    synonym_found = True
                    if new_word != word:
                        has_changed = True
                    break
        # Augmentasi dengan slang
        if not synonym_found:
            if word in slang_dict:
                new_word = slang_dict[word]
                new_words.append(new_word)
                has_changed = True
                slang_found = True
        # Jika tidak ditemukan kata slang atau sinonim, gunakan kata aslinya
        if not synonym_found and not slang_found:
            new_words.append(word)
    return ' '.join(new_words), has_changed


def augment_data(data):
    augmented_data = []
    for line in data:
        sentence, annotations = line.strip().split("####")
        annotations = ast.literal_eval(annotations)
        augmented_sentence, has_changed = augment_sentence(sentence)
        if has_changed:
            augmented_data.append(f"{augmented_sentence}####{annotations}")
    return augmented_data

def load_data(file_path):
    data = []
    with open(file_path, encoding='utf-8') as file:
        for line in file:
            data.append(line.strip())
    return data

def save_data(data, file_path):
    with open(file_path, 'w', encoding='utf-8') as file:
        for line in data:
            file.write(line + '\n')

# Load data
original_data = load_data(file_path)

# Augment data
augmented_data = augment_data(original_data)

# Merge original and augmented data
merged_data = original_data + augmented_data

# Save merged data
save_data(merged_data, 'ready_to_split_aug.txt')

print(f"Data augmented and saved to 'ready_to_split_aug.txt'")

import random
import ast

file_path = "ready_to_split.txt"  # Path ke file teks Anda

def group_consecutive_indices(indices):
    """Group consecutive indices together."""
    if not indices:
        return []
    grouped = [[indices[0]]]
    for i in range(1, len(indices)):
        if indices[i] == indices[i-1] + 1:
            grouped[-1].append(indices[i])
        else:
            grouped.append([indices[i]])
    return grouped

def shuffle_tokens(sentence, annotations):
    words = sentence.split()
    indices = list(range(len(words)))

    # Group consecutive indices in annotations
    grouped_indices = []
    used_indices = set()
    for aspect_indices, opinion_indices, _ in annotations:
        all_indices = aspect_indices + opinion_indices
        all_indices = sorted(set(all_indices))  # Remove duplicates and sort
        for group in group_consecutive_indices(all_indices):
            if any(idx not in used_indices for idx in group):
                grouped_indices.append(group)
                used_indices.update(group)

    remaining_indices = [i for i in indices if i not in used_indices]
    grouped_indices.extend([[i] for i in remaining_indices])

    # Shuffle groups
    random.shuffle(grouped_indices)

    # Flatten the groups to get the new order
    shuffled_indices = [idx for group in grouped_indices for idx in group]
    shuffled_words = [words[idx] for idx in shuffled_indices]

    # Reconstruct the sentence and update indices
    word_index_map = {old_idx: new_idx for new_idx, old_idx in enumerate(shuffled_indices)}

    new_annotations = []
    for aspect_indices, opinion_indices, sentiment in annotations:
        new_aspect_indices = sorted(word_index_map[idx] for idx in aspect_indices)
        new_opinion_indices = sorted(word_index_map[idx] for idx in opinion_indices)
        new_annotations.append([new_aspect_indices, new_opinion_indices, sentiment])

    return ' '.join(shuffled_words), new_annotations

def augment_data(data):
    augmented_data = []
    for line in data:
        txt_data = line.strip().split("####")
        sentence = txt_data[0].strip()
        annotations = ast.literal_eval(txt_data[1])
        new_sentence, new_annotations = shuffle_tokens(sentence, annotations)
        augmented_data.append(f"{new_sentence}####{new_annotations}")
    return augmented_data

def load_data(file_path):
    data = []
    with open(file_path, encoding='utf-8') as file:
        for line in file:
            data.append(line.strip())
    return data

def save_data(data, file_path):
    with open(file_path, 'w', encoding='utf-8') as file:
        for line in data:
            file.write(line + '\n')

# Load data
original_data = load_data(file_path)

# Augment data
augmented_data = augment_data(original_data)

# Merge original and augmented data
merged_data = original_data + augmented_data

# Save merged data
save_data(merged_data, 'ready_to_split_merged.txt')

print(f"Data augmented and saved to 'ready_to_split_merged.txt'")

import os

# Daftar nama file yang akan digabungkan
file_names = ['train.txt', 'dev.txt', 'test.txt']

# Nama folder yang berisi file-file tersebut
folder_name = '215'

# Nama file baru untuk hasil merge
output_file_name = '215.txt'

# Jalur ke folder yang berisi file-file tersebut
folder_path = os.path.join(os.getcwd(), folder_name)

# Jalur lengkap ke file hasil merge
output_file_path = os.path.join(folder_path, output_file_name)

# Buka file baru untuk menulis
with open(output_file_path, 'w', encoding='utf-8') as output_file:
    # Loop melalui setiap nama file dalam daftar
    for file_name in file_names:
        # Jalur lengkap ke file saat ini
        file_path = os.path.join(folder_path, file_name)
        # Buka file saat ini untuk dibaca
        with open(file_path, 'r', encoding='utf-8') as input_file:
            # Baca konten dari file saat ini dan tulis ke file hasil merge
            output_file.write(input_file.read())

# Tampilkan pesan jika proses telah selesai
print(f"Proses merge selesai. Hasil tersimpan dalam file '{output_file_name}' di dalam folder '{folder_name}'.")

file_x = "215/215.txt"  # Ganti dengan jalur lengkap ke file teks yang ingin Anda cek jumlah barisnya

# Menghitung jumlah baris dalam file teks
with open(file_x, 'r', encoding='utf-8') as file:
    line_count = sum(1 for line in file)

print(f"Jumlah baris dalam file '{file_x}': {line_count}")

file_y = "ready_to_split_merged.txt"  # Ganti dengan jalur lengkap ke file teks yang ingin Anda cek jumlah barisnya

# Menghitung jumlah baris dalam file teks
with open(file_y, 'r', encoding='utf-8') as file:
    line_count = sum(1 for line in file)

print(f"Jumlah baris dalam file '{file_y}': {line_count}")

import os
import random

# Set the file paths
folder_path = '518.PersonalPolitics'
if not os. path. exists(folder_path):
  os. makedirs(folder_path)

input_file = "ready_to_split_merged.txt"
# input_file = "ready_to_split_aug.txt"
train_file = os.path.join(folder_path, "train.txt")
dev_file = os.path.join(folder_path, "dev.txt")
test_file = os.path.join(folder_path, "test.txt")

# Set the percentages
train_percent = 0.7
dev_percent = 0.15
test_percent = 0.15

# Read the input file
with open(input_file, "r", encoding='utf-8') as f:
    lines = f.readlines()

# Shuffle the lines
random.shuffle(lines)

# Calculate the number of lines for each set
total_lines = len(lines)
train_lines = int(total_lines * train_percent)
dev_lines = int(total_lines * dev_percent)
test_lines = total_lines - train_lines - dev_lines

# Write the lines to the output files
with open(train_file, "w", encoding='utf-8') as f:
    f.writelines(lines[:train_lines])

with open(dev_file, "w", encoding='utf-8') as f:
    f.writelines(lines[train_lines:train_lines+dev_lines])

with open(test_file, "w", encoding='utf-8') as f:
    f.writelines(lines[train_lines+dev_lines:])

!zip -r /content/518.PersonalPolitics.zip /content/518.PersonalPolitics
# change sample_data.zip to your desired download name Ex: nothing.zip
# change sample_data to your desired download folder name Ex: ner_data

from collections import defaultdict

# Initialize dictionaries to count occurrences
aspect_count = defaultdict(int)
opinion_count = defaultdict(int)

# Function to extract aspects and opinions from a sentence
def extract_aspects_opinions(sentence, annotations):
    words = sentence.split()
    for annotation in annotations:
        aspect_indices, opinion_indices, sentiment = annotation
        aspect_words = " ".join(words[i] for i in aspect_indices if i < len(words))
        opinion_words = " ".join(words[i] for i in opinion_indices if i < len(words))
        if aspect_words and opinion_words:  # Ensure both aspect and opinion are not empty
            aspect_count[aspect_words] += 1
            opinion_count[opinion_words] += 1

# Read input data from a .txt file
file_path = "ready_to_split_merged.txt"  # Change this to your file path
with open(file_path, "r") as file:
    for line in file:
        # Split each line into sentence and annotations
        sentence, raw_annotations = line.strip().split("####")
        annotations = eval(raw_annotations)
        extract_aspects_opinions(sentence, annotations)

import pandas as pd

# Convert aspect and opinion counts to DataFrame
aspect_df = pd.DataFrame(aspect_count.items(), columns=['Aspect', 'Count'])
opinion_df = pd.DataFrame(opinion_count.items(), columns=['Opinion', 'Count'])

# Sort DataFrame by count in descending order
aspect_df = aspect_df.sort_values(by='Count', ascending=False).reset_index(drop=True)
opinion_df = opinion_df.sort_values(by='Count', ascending=False).reset_index(drop=True)

# Print the results
print("Aspect Counts:")
print(aspect_df)

print("\nOpinion Counts:")
print(opinion_df)

import matplotlib.pyplot as plt

# Plot the top N aspects
top_n = 100
plt.figure(figsize=(10, 6))
plt.bar(aspect_df['Aspect'][:top_n][::-1], aspect_df['Count'][:top_n][::-1], color='skyblue')
plt.ylabel('Count')
plt.xlabel('Aspect')
plt.xticks(rotation=45, ha='right')  # Putar label sumbu x
plt.title(f'Top {top_n} Most Common Aspects')
plt.tight_layout()
plt.show()

# Plot the top N opinions
plt.figure(figsize=(10, 6))
plt.bar(opinion_df['Opinion'][:top_n][::-1], opinion_df['Count'][:top_n][::-1], color='salmon')
plt.ylabel('Count')
plt.xlabel('Opinion')
plt.xticks(rotation=45, ha='right')  # Putar label sumbu x
plt.title(f'Top {top_n} Most Common Opinions')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Variabel tetap dan text yang sesuai dengan jumlah kemunculan kata
count_texts = {}

# Fungsi untuk mendapatkan informasi jumlah kata dengan masing-masing count
def get_count_info(count_dict):
    count_info = {}
    for count in count_dict.values():
        count_info[count] = count_info.get(count, 0) + 1
    return count_info

# Mendapatkan informasi jumlah kata dengan masing-masing count untuk aspek dan opini
aspect_count_info = get_count_info(aspect_count)
opinion_count_info = get_count_info(opinion_count)

# Gabungkan kunci-kunci dari kedua kamus
all_counts = set(aspect_count_info.keys()) | set(opinion_count_info.keys())

# Buat count_texts berdasarkan semua nilai count yang mungkin
# Buat count_texts berdasarkan semua nilai count yang mungkin
count_texts = {}
for count in all_counts:
    if aspect_count_info.get(count, 0) > 0 or opinion_count_info.get(count, 0) > 0:
        count_texts[count] = f'Count {count}'


# Plot untuk informasi jumlah kata dengan masing-masing count untuk aspek
plt.figure(figsize=(10, 5))
bars = plt.bar(count_texts.values(), [aspect_count_info.get(count, 0) for count in all_counts], color='skyblue')

# Anotasi teks untuk menampilkan jumlah kata di atas setiap batang
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, height, str(int(height)), ha='center', va='bottom')

plt.title('Distribusi Kemunculan Kata dengan Masing-Masing Count untuk Aspek')
plt.xlabel('Count')
plt.ylabel('Jumlah Kata')
plt.xticks(rotation=45, ha='right')  # Putar label sumbu x
plt.grid(axis='y')
plt.tight_layout()  # Penyesuaian layout agar label terlihat
plt.show()

# Plot untuk informasi jumlah kata dengan masing-masing count untuk opini
plt.figure(figsize=(10, 5))
bars = plt.bar(count_texts.values(), [opinion_count_info.get(count, 0) for count in all_counts], color='lightgreen')

# Anotasi teks untuk menampilkan jumlah kata di atas setiap batang
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, height, str(int(height)), ha='center', va='bottom')

plt.title('Distribusi Kemunculan Kata dengan Masing-Masing Count untuk Opini')
plt.xlabel('Count')
plt.ylabel('Jumlah Kata')
plt.xticks(rotation=45, ha='right')  # Putar label sumbu x
plt.grid(axis='y')
plt.tight_layout()  # Penyesuaian layout agar label terlihat
plt.show()

def cek_maximum_panjang_kalimat_dari_file(file_path):
    """
    Fungsi untuk mengecek panjang kalimat maksimum dalam file teks.
    Argumen:
        file_path (str): Path ke file teks.
    Returns:
        int: Panjang maksimum dari kalimat (jumlah kata dalam kalimat terpanjang).
    """
    max_panjang = 0
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            kalimat = line.strip()
            panjang = len(kalimat)
            if panjang > max_panjang:
                max_panjang = panjang
    return max_panjang

# Contoh penggunaan
file_path = 'dataset (5).txt'  # Ganti dengan path ke file teks Anda
max_panjang_kalimat = cek_maximum_panjang_kalimat_dari_file(file_path)
print("Panjang kalimat maksimum:", max_panjang_kalimat)

from wordcloud import WordCloud
import matplotlib.pyplot as plt
import ast

# Fungsi untuk menghasilkan WordCloud
def generate_wordcloud(text, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    # Plot the WordCloud image
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title)
    plt.axis('off')
    plt.show()

# Inisialisasi variabel untuk menyimpan aspek dan opini berdasarkan sentimen
aspects_pos = []
aspects_neu = []
aspects_neg = []

opinions_pos = []
opinions_neu = []
opinions_neg = []

# Baca file dan proses data
with open('dataset (5).txt', encoding='utf-8') as topo_file:
    for index, line in enumerate(topo_file):
        tagging_info = line.split("####")
        tagging = tagging_info[1]
        sentence = tagging_info[0]
        sentence_split = sentence.split(" ")
        res = ast.literal_eval(tagging)

        for i, x in enumerate(res):
            sentiment = x[2]
            aspect_index = x[0]
            opinion_index = x[1]

            aspect = ' '.join(sentence_split[aspect_index[0]:(aspect_index[-1]) + 1])
            opinion = ' '.join(sentence_split[opinion_index[0]:(opinion_index[-1]) + 1])

            if sentiment == 'POS':
                aspects_pos.append(aspect)
                opinions_pos.append(opinion)
            elif sentiment == 'NEU':
                aspects_neu.append(aspect)
                opinions_neu.append(opinion)
            else:
                aspects_neg.append(aspect)
                opinions_neg.append(opinion)

# Gabungkan semua aspek dan opini untuk setiap sentimen menjadi satu string
aspects_pos_text = ' '.join(aspects_pos)
opinions_pos_text = ' '.join(opinions_pos)

aspects_neu_text = ' '.join(aspects_neu)
opinions_neu_text = ' '.join(opinions_neu)

aspects_neg_text = ' '.join(aspects_neg)
opinions_neg_text = ' '.join(opinions_neg)

# Generate WordCloud untuk Aspek dan Opini Berdasarkan Sentimen
# Aspek Positif
generate_wordcloud(aspects_pos_text, 'WordCloud Aspek (Positif)')

# Opini Positif
generate_wordcloud(opinions_pos_text, 'WordCloud Opini (Positif)')

# Aspek Netral
generate_wordcloud(aspects_neu_text, 'WordCloud Aspek (Netral)')

# Opini Netral
generate_wordcloud(opinions_neu_text, 'WordCloud Opini (Netral)')

# Aspek Negatif
generate_wordcloud(aspects_neg_text, 'WordCloud Aspek (Negatif)')

# Opini Negatif
generate_wordcloud(opinions_neg_text, 'WordCloud Opini (Negatif)')

from collections import Counter
import matplotlib.pyplot as plt
import numpy as np
# Fungsi untuk plot top 10 aspek berdasarkan sentimen
def plot_top_aspects(aspects, sentiment_label, color):
    aspect_counts = Counter(aspects)
    top_aspects = aspect_counts.most_common(3)

    labels, values = zip(*top_aspects)
    x = np.arange(len(labels))

    plt.figure(figsize=(12, 8))
    plt.bar(x, values, color=color)
    plt.xlabel('Aspects')
    plt.ylabel('Frequency')
    plt.title(f'Top 10 Aspects ({sentiment_label})')
    plt.xticks(x, labels, rotation='vertical')
    plt.tight_layout()
    plt.show()

    return top_aspects

# Plot top 10 aspek berdasarkan sentimen
top_aspects_pos = plot_top_aspects(aspects_pos, 'Positif', '#006400')
top_aspects_neu = plot_top_aspects(aspects_neu, 'Netral', '#FF8C00')
top_aspects_neg = plot_top_aspects(aspects_neg, 'Negatif', '#8B0000')

# Generate WordCloud untuk opini dari masing-masing top aspek pada setiap sentimen
def generate_opinion_wordclouds(aspects, opinions, top_aspects, sentiment_label):
    for aspect, _ in top_aspects:
        related_opinions = [opinion for asp, opinion in zip(aspects, opinions) if asp == aspect]
        text = ' '.join(related_opinions)
        generate_wordcloud(text, f'WordCloud Opini ({sentiment_label}): {aspect}')

# Generate WordCloud untuk opini berdasarkan top aspek
# Opini Positif
generate_opinion_wordclouds(aspects_pos, opinions_pos, top_aspects_pos, 'Positif')

# Opini Netral
generate_opinion_wordclouds(aspects_neu, opinions_neu, top_aspects_neu, 'Netral')

# Opini Negatif
generate_opinion_wordclouds(aspects_neg, opinions_neg, top_aspects_neg, 'Negatif')

longest_aspect = ""
longest_aspect_length = 0
if len(aspect_index) > longest_aspect_length:
    longest_aspect_length = len(aspect_index)
    longest_aspect = aspect
print(f"Aspek dengan index terpanjang: {longest_aspect}")

longest_opinion = ""
longest_opinion_length = 0
if len(opinion_index) > longest_opinion_length:
    longest_opinion_length = len(opinion_index)
    longest_opinion = opinion
print(f"Opini dengan index terpanjang: {longest_opinion}")

import ast
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Inisialisasi variabel untuk menyimpan semua aspek
all_aspects = []

# Baca file dan proses data
with open('dataset (5).txt', encoding='utf-8') as topo_file:
    for index, line in enumerate(topo_file):
        tagging_info = line.split("####")
        tagging = tagging_info[1]
        sentence = tagging_info[0]
        sentence_split = sentence.split(" ")
        res = ast.literal_eval(tagging)

        for i, x in enumerate(res):
            aspect_index = x[0]

            # Menggabungkan token untuk aspek
            aspect = ' '.join(sentence_split[aspect_index[0]:(aspect_index[-1]) + 1])

            # Menambahkan aspek ke daftar semua aspek
            all_aspects.append(aspect)

# Gabungkan semua aspek menjadi satu string
all_aspects_text = ' '.join(all_aspects)

# Generate WordCloud untuk Semua Aspek
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_aspects_text)

# Plot the WordCloud image
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title('WordCloud Aspek (Tanpa Sentimen)')
plt.axis('off')
plt.show()

import ast

# Dictionary untuk menyimpan hubungan aspek dengan opini dan sentimennya
aspect_opinion_sentiment = {}

# Baca file dan proses data
with open('ready_to_split.txt', encoding='utf-8') as topo_file:
    for index, line in enumerate(topo_file):
        tagging_info = line.split("####")
        tagging = tagging_info[1]
        sentence = tagging_info[0]
        sentence_split = sentence.split(" ")
        res = ast.literal_eval(tagging)

        for i, x in enumerate(res):
            aspect_index = x[0]
            opinion_index = x[1]
            sentiment = x[2]

            # Ekstrak aspek, opini, dan sentimen
            aspect = ' '.join(sentence_split[aspect_index[0]:(aspect_index[-1]) + 1])
            opinion = ' '.join(sentence_split[opinion_index[0]:(opinion_index[-1]) + 1])

            # Simpan hubungan antara aspek dengan opini dan sentimennya
            if aspect not in aspect_opinion_sentiment:
                aspect_opinion_sentiment[aspect] = []
            aspect_opinion_sentiment[aspect].append((opinion, sentiment))

# Print hasilnya
for aspect, opinions_sentiments in aspect_opinion_sentiment.items():
    print(f"Aspek: {aspect}")
    for opinion, sentiment in opinions_sentiments:
        print(f"Opini: {opinion}, Sentimen: {sentiment}")
    print()

from collections import Counter
import matplotlib.pyplot as plt

# Dictionary untuk menyimpan frekuensi kemunculan setiap aspek
aspect_frequency = {}

# Baca file dan proses data
with open('dataset (5).txt', encoding='utf-8') as topo_file:
    for line in topo_file:
        tagging_info = line.split("####")
        sentence = tagging_info[0]
        res = ast.literal_eval(tagging_info[1])

        for x in res:
            aspect_index = x[0]
            sentence_split = sentence.split(" ")
            aspect = ' '.join(sentence_split[aspect_index[0]:(aspect_index[-1]) + 1])

            # Update frekuensi kemunculan aspek dalam kamus
            aspect_frequency[aspect] = aspect_frequency.get(aspect, 0) + 1

# Ambil 10 aspek dengan frekuensi tertinggi
top_10_aspects = Counter(aspect_frequency).most_common(10)

# Print hasilnya
print("Top 10 Aspek Paling Sering Muncul:")
for aspect, frequency in top_10_aspects:
    print(f"Aspek: {aspect}, Frekuensi: {frequency}")

# Ambil data dari top_10_aspects
aspects = [aspect[0] for aspect in top_10_aspects]
frequencies = [aspect[1] for aspect in top_10_aspects]

# Buat bar plot
plt.figure(figsize=(10, 6))
plt.bar(aspects, frequencies, color='orange')
plt.xlabel('Aspek')
plt.ylabel('Frekuensi Kemunculan')
plt.title('Top 10 Aspek Paling Sering Muncul')
plt.xticks(rotation=45, ha='right') # Rotasi label x untuk legibilitas
plt.tight_layout() # Agar tidak tumpang tindih
plt.show()

from collections import Counter
import matplotlib.pyplot as plt

# Dictionary to store aspect frequencies
aspect_frequency = {}

# Read file and process data
with open('dataset (5).txt', encoding='utf-8') as topo_file:
  for line in topo_file:
    tagging_info = line.split("####")
    sentence = tagging_info[0]
    res = ast.literal_eval(tagging_info[1])

    for x in res:
      aspect_index = x[0]
      sentence_split = sentence.split(" ")
      aspect = ' '.join(sentence_split[aspect_index[0]:(aspect_index[-1]) + 1])

      # Update aspect frequency in the dictionary
      aspect_frequency[aspect] = aspect_frequency.get(aspect, 0) + 1

# Get top 10 aspects (modify as needed)
top_10_aspects = Counter(aspect_frequency).most_common(10)

# Extract aspects and frequencies for plotting
aspects = [aspect[0] for aspect in top_10_aspects]
frequencies = [aspect[1] for aspect in top_10_aspects]

# Create a color list for each bar (you can customize the colors)
color_list = plt.cm.tab10.colors  # Use a colormap for variety

# Create the bar plot with individual colors
plt.figure(figsize=(10, 6))
plt.bar(aspects, frequencies, color=color_list[:len(aspects)])  # Use colors for all aspects
plt.xlabel('Aspek')
plt.ylabel('Frekuensi Kemunculan')
plt.title('Top 10 Aspek Paling Sering Muncul')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability
plt.tight_layout()  # Adjust spacing for clarity
plt.show()

from collections import Counter
import matplotlib.pyplot as plt
import ast
import numpy as np

# Dictionary untuk menyimpan frekuensi kemunculan setiap aspek dan sentimennya
aspect_sentiment_frequency = {}

# Baca file dan proses data
with open('dataset (5).txt', encoding='utf-8') as topo_file:
    for line in topo_file:
        tagging_info = line.split("####")
        sentence = tagging_info[0]
        res = ast.literal_eval(tagging_info[1])

        for x in res:
            aspect_index = x[0]
            sentiment = x[2]
            sentence_split = sentence.split(" ")
            aspect = ' '.join(sentence_split[aspect_index[0]:(aspect_index[-1]) + 1])

            # Update frekuensi kemunculan aspek dan sentimen dalam kamus
            if aspect not in aspect_sentiment_frequency:
                aspect_sentiment_frequency[aspect] = {'POS': 0, 'NEU': 0, 'NEG': 0}
            aspect_sentiment_frequency[aspect][sentiment] += 1

# Ambil 10 aspek dengan frekuensi tertinggi secara keseluruhan
overall_aspect_frequency = {aspect: sum(sentiments.values()) for aspect, sentiments in aspect_sentiment_frequency.items()}
top_10_aspects_overall = Counter(overall_aspect_frequency).most_common(10)

# Extract aspects and their sentiment frequencies
top_aspects = [aspect[0] for aspect in top_10_aspects_overall]
top_aspects_frequencies = [aspect_sentiment_frequency[aspect] for aspect in top_aspects]

# Buat plot bar untuk membandingkan frekuensi sentimen untuk 10 aspek paling umum
x = np.arange(len(top_aspects))  # Label locations
width = 0.25  # Width of the bars

fig, ax = plt.subplots(figsize=(14, 8))

# Sentimen counts
pos_counts = [freq['POS'] for freq in top_aspects_frequencies]
neu_counts = [freq['NEU'] for freq in top_aspects_frequencies]
neg_counts = [freq['NEG'] for freq in top_aspects_frequencies]

rects1 = ax.bar(x - width, pos_counts, width, label='Positive', color='g')
rects2 = ax.bar(x, neu_counts, width, label='Neutral', color='y')
rects3 = ax.bar(x + width, neg_counts, width, label='Negative', color='r')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_xlabel('Aspects')
ax.set_ylabel('Sentiment Counts')
ax.set_title('Sentiment Comparison for Top 10 Aspects')
ax.set_xticks(x)
ax.set_xticklabels(top_aspects, rotation=45, ha='right')
ax.legend()

fig.tight_layout()
plt.show()

from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import ast

# Function to generate and display a WordCloud
def generate_wordcloud(text, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title)
    plt.axis('off')
    plt.show()

# Dictionary to store the frequency of each aspect and its sentiments and opinions
aspect_sentiment_opinions = {}

# Read the file and process data
with open('dataset (5).txt', encoding='utf-8') as topo_file:
    for line in topo_file:
        tagging_info = line.split("####")
        sentence = tagging_info[0]
        res = ast.literal_eval(tagging_info[1])

        for x in res:
            aspect_index = x[0]
            opinion_index = x[1]
            sentiment = x[2]
            sentence_split = sentence.split(" ")
            aspect = ' '.join(sentence_split[aspect_index[0]:(aspect_index[-1]) + 1])
            opinion = ' '.join(sentence_split[opinion_index[0]:(opinion_index[-1]) + 1])

            # Update the frequency and opinions of aspects and sentiments
            if aspect not in aspect_sentiment_opinions:
                aspect_sentiment_opinions[aspect] = {'POS': [], 'NEU': [], 'NEG': []}
            aspect_sentiment_opinions[aspect][sentiment].append(opinion)

# Get the top 10 aspects with the highest overall frequency
overall_aspect_frequency = {aspect: sum(len(opinions[sent]) for sent in opinions) for aspect, opinions in aspect_sentiment_opinions.items()}
top_10_aspects_overall = Counter(overall_aspect_frequency).most_common(10)
top_aspects = [aspect[0] for aspect in top_10_aspects_overall]

# Generate WordCloud for opinions from the top 10 aspects for each sentiment
for aspect in top_aspects:
    if aspect in aspect_sentiment_opinions:
        for sentiment, opinions in aspect_sentiment_opinions[aspect].items():
            if len(opinions) > 0:
                opinion_text = ' '.join(opinions)
                generate_wordcloud(opinion_text, f'Opini untuk {aspect} dengan sentimen {sentiment}')
            else:
                print(f'Opini tidak ditemukan untuk {aspect} dengan sentimen {sentiment}')

import re
import ast
from collections import Counter
import matplotlib.pyplot as plt
import string

# custom_stopwords = ["dan", "di", "ini", "itu", "yang", "tapi", "saja", "kalau", "sama", "dalam", "ke", "dari", "untuk", "pada", "dengan", "adalah", "seperti", "nya"]

def remove_stopwords(words, stop_words):
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return filtered_words

def pre_text(text):
    # Tokenisasi teks
    words = text.split()
    # Menghapus tanda baca dari setiap kata
    words = [word.strip(string.punctuation) for word in words]
    # Menghapus kata-kata yang kosong setelah penghapusan tanda baca
    words = [word for word in words if word]
    # words = remove_stopwords(words, custom_stopwords)
    return words

# Baca teks dari file dan proses setiap baris
word_list = []
with open("dataset (5).txt", "r", encoding="utf-8") as file:
    for line in file:
        txt_data = line.split("####")
        tagging = txt_data[1]
        res = ast.literal_eval(tagging)
        sentence = txt_data[0]
        # Preprocessing teks
        words = pre_text(sentence)
        word_list.extend(words)

# Menghitung frekuensi kemunculan setiap kata
word_counts = Counter(word_list)

# Mendapatkan 20 kata teratas berdasarkan frekuensi
top_words = word_counts.most_common(20)

# Membuat visualisasi
plt.figure(figsize=(10, 6))
plt.bar([word[0] for word in top_words], [word[1] for word in top_words])
plt.title('Top 20 Words')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()

from PIL import Image
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import ast
# Baca gambar mask
# mask_image = Image.open("pngegg 1.png").convert("L")
# mask_image = np.array(mask_image)

# Pastikan hanya dua warna (hitam dan putih)
# mask_image = np.where(mask_image > 128, 255, 0).astype(np.uint8)

# Membuat wordcloud menggunakan mask
wordcloud = WordCloud(width=800, height=800,
                      background_color='white',
                      # mask=mask_image,
                      contour_width=0,
                      # contour_color='black',
                      # collocations=False,  # Disable collocations for individual words
                      # max_words=200,       # Limit the number of words
                      # max_font_size=100,   # Maximum font size
                      # random_state=42,     # For reproducibility
                      ).generate_from_frequencies(word_counts)

# Menampilkan wordcloud
plt.figure(figsize=(10, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

import json

# Input and output file paths
input_file = "result.json"
output_file = "results_filtered.json"

# Load data from JSON
with open(input_file, "r") as file:
    data = json.load(file)

# Initialize a list to store filtered entries
filtered_data = []

# Filter entries that have errors or missing Triplets/True Triplets
for entry in data:
    if 'Triplets' not in entry or 'True Triplets' not in entry:
        continue  # Skip entries without Triplets or True Triplets

    triplets = entry['Triplets']
    true_triplets = entry['True Triplets']

    # Check if either Triplets or True Triplets are empty
    if not triplets or not true_triplets:
        continue  # Skip entries with empty Triplets or True Triplets

    # Check if Triplets and True Triplets have the same length
    if len(triplets) != len(true_triplets):
        continue  # Skip entries where the lengths of Triplets and True Triplets don't match

    # Validate each triplet structure (Aspect, Opinion, Polarity)
    valid_triplets = True
    for triplet in triplets:
        if not all(key in triplet for key in ['Aspect', 'Opinion', 'Polarity']):
            valid_triplets = False
            break
    if not valid_triplets:
        continue  # Skip entries with malformed Triplets

    for triplet in true_triplets:
        if not all(key in triplet for key in ['Aspect', 'Opinion', 'Polarity']):
            valid_triplets = False
            break
    if not valid_triplets:
        continue  # Skip entries with malformed True Triplets

    # If all checks pass, add the entry to filtered_data
    filtered_data.append(entry)

# Save filtered data back to JSON
with open(output_file, "w") as file:
    json.dump(filtered_data, file, indent=2)

print(f"Filtered data has been saved to {output_file}")

import json
import csv
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# Load data from results.json
with open('updated_results.json', 'r') as f:
    data = json.load(f)

# Prepare predictions and actuals data from the JSON
predictions = []
actuals = []

for entry in data:
    sentence_id = entry['sentence_id']
    for triplet in entry['Triplets']:
        aspect = triplet['Aspect']
        opinion = triplet['Opinion']
        polarity = triplet['Polarity']
        predictions.append((sentence_id, aspect, opinion, polarity))
    for triplet in entry['True Triplets']:
        aspect = triplet['Aspect']
        opinion = triplet['Opinion']
        polarity = triplet['Polarity']
        actuals.append((sentence_id, aspect, opinion, polarity))

# Write predictions to CSV (if needed)
with open('predictions.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['Sentence ID', 'Aspect', 'Opinion', 'Sentiment'])
    writer.writerows(predictions)

# Write actuals to CSV (if needed)
with open('actuals.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['Sentence ID', 'Aspect', 'Opinion', 'Sentiment'])
    writer.writerows(actuals)

# Match predictions with actuals
matched_aspects_opinions = []

for prediction in predictions:
    for actual in actuals:
        if prediction[0] == actual[0] and prediction[1] == actual[1] and prediction[2] == actual[2]:
            matched_aspects_opinions.append((prediction[0], prediction[1], prediction[2], prediction[3], actual[3]))

# Write matched aspects and opinions to CSV (if needed)
with open('matched_aspects_opinions.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['Sentence ID', 'Aspect', 'Opinion', 'Predicted Sentiment', 'Actual Sentiment'])
    writer.writerows(matched_aspects_opinions)

# Extract predicted and actual sentiments
predicted_sentiments = [entry[3] for entry in matched_aspects_opinions]
actual_sentiments = [entry[4] for entry in matched_aspects_opinions]

# Define labels for confusion matrix and classification report
labels = ['Negative', 'Neutral', 'Positive']  # Adjust as needed based on your actual sentiment classes

# Build confusion matrix
cm = confusion_matrix(actual_sentiments, predicted_sentiments, labels=labels)

# Convert confusion matrix to DataFrame
cm_df = pd.DataFrame(cm, index=labels, columns=labels)
sns.set(font_scale=1.2)
# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='d' )#, cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Sentiment')
plt.ylabel('Actual Sentiment')
plt.show()

# Build classification report
class_report = classification_report(actual_sentiments, predicted_sentiments, target_names=labels)

print('Classification Report:')
print(class_report)

import json
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# Load data from results.json
with open('results_filter.json', 'r') as f:
    data = json.load(f)

# Prepare predictions and actuals data from the JSON
predictions = []
actuals = []

for entry in data:
    sentence_id = entry['sentence_id']
    for triplet in entry['Triplets']:
        aspect = triplet['Aspect']
        opinion = triplet['Opinion']
        polarity = triplet['Polarity']
        predictions.append((sentence_id, aspect, opinion, polarity))
    for triplet in entry['True Triplets']:
        aspect = triplet['Aspect']
        opinion = triplet['Opinion']
        polarity = triplet['Polarity']
        actuals.append((sentence_id, aspect, opinion, polarity))

# Convert lists to DataFrames
df_predictions = pd.DataFrame(predictions, columns=['Sentence ID', 'Aspect', 'Opinion', 'Predicted Sentiment'])
df_actuals = pd.DataFrame(actuals, columns=['Sentence ID', 'Aspect', 'Opinion', 'Actual Sentiment'])

# Merge DataFrames on Sentence ID, Aspect, and Opinion
df_matched = pd.merge(df_predictions, df_actuals, on=['Sentence ID', 'Aspect', 'Opinion'])

# Remove duplicates (if any)
df_matched = df_matched.drop_duplicates()

# Write matched aspects and opinions to CSV (if needed)
df_matched.to_csv('matched_aspects_opinions.csv', index=False)

# Extract predicted and actual sentiments
predicted_sentiments = df_matched['Predicted Sentiment']
actual_sentiments = df_matched['Actual Sentiment']

# Define labels for confusion matrix and classification report
labels = ['Negative', 'Neutral', 'Positive']  # Adjust as needed based on your actual sentiment classes

# Build confusion matrix
cm = confusion_matrix(actual_sentiments, predicted_sentiments, labels=labels)

# Convert confusion matrix to DataFrame
cm_df = pd.DataFrame(cm, index=labels, columns=labels)

sns.set(font_scale=1.2)
# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='d')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Sentiment')
plt.ylabel('Actual Sentiment')
plt.show()

# Build classification report
class_report = classification_report(actual_sentiments, predicted_sentiments, target_names=labels)

print('Classification Report:')
print(class_report)

print("Unique values in actual sentiments:", np.unique(actual_sentiments))
print("Unique values in predicted sentiments:", np.unique(predicted_sentiments))

import json
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# Load data from results.json
with open('results_filter.json', 'r') as f:
    data = json.load(f)

# Prepare predictions and actuals data from the JSON
predictions = []
actuals = []

for entry in data:
    sentence_id = entry['sentence_id']
    for triplet in entry['Triplets']:
        aspect = triplet['Aspect']
        predictions.append((sentence_id, aspect))
    for triplet in entry['True Triplets']:
        aspect = triplet['Aspect']
        actuals.append((sentence_id, aspect))

# Convert lists to DataFrames
df_predictions = pd.DataFrame(predictions, columns=['Sentence ID', 'Aspect'])
df_actuals = pd.DataFrame(actuals, columns=['Sentence ID', 'Aspect'])

# Merge DataFrames on Sentence ID, Aspect, and Opinion
df_matched = pd.merge(df_predictions, df_actuals, on=['Sentence ID', 'Aspect'], how='outer', indicator=True)

# Determine the category (TP, FP, FN)
df_matched['Category'] = df_matched['_merge'].map({
    'both': 'TP',
    'left_only': 'FP',
    'right_only': 'FN'
})

# Drop the merge indicator column
df_matched.drop(columns=['_merge'], inplace=True)

# Create labels for aspects and opinions based on categories
df_matched['Predicted'] = df_matched['Category'].apply(lambda x: 1 if x == 'TP' or x == 'FP' else 0)
df_matched['Actual'] = df_matched['Category'].apply(lambda x: 1 if x == 'TP' or x == 'FN' else 0)

# Extract predicted and actual categories
predicted_categories = df_matched['Predicted']
actual_categories = df_matched['Actual']

# Define labels for confusion matrix and classification report
labels = [0, 1]  # 0: Not Mentioned, 1: Mentioned

# Build confusion matrix
cm = confusion_matrix(actual_categories, predicted_categories, labels=labels)

# Convert confusion matrix to DataFrame
cm_df = pd.DataFrame(cm, index=['Not Mentioned', 'Mentioned'], columns=['Not Mentioned', 'Mentioned'])

sns.set(font_scale=1.2)
# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='d')
plt.title('Confusion Matrix for Aspects')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Build classification report
class_report = classification_report(actual_categories, predicted_categories, labels=labels, target_names=['Not Mentioned', 'Mentioned'])

print('Classification Report:')
print(class_report)

import json
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# Load data from results.json
with open('results_filter.json', 'r') as f:
    data = json.load(f)

# Prepare predictions and actuals data from the JSON
predictions = []
actuals = []

for entry in data:
    sentence_id = entry['sentence_id']
    for triplet in entry['Triplets']:
        opinion = triplet['Opinion']
        predictions.append((sentence_id, opinion))
    for triplet in entry['True Triplets']:
        opinion = triplet['Opinion']
        actuals.append((sentence_id, opinion))

# Convert lists to DataFrames
df_predictions = pd.DataFrame(predictions, columns=['Sentence ID', 'Opinion'])
df_actuals = pd.DataFrame(actuals, columns=['Sentence ID', 'Opinion'])

# Merge DataFrames on Sentence ID and Opinion
df_matched = pd.merge(df_predictions, df_actuals, on=['Sentence ID', 'Opinion'], how='outer', indicator=True)

# Determine the opinion category (TP, FP, FN, TN)
df_matched['Category'] = df_matched['_merge'].map({
    'both': 'TP',
    'left_only': 'FP',
    'right_only': 'FN'
})

# Drop the merge indicator column
df_matched.drop(columns=['_merge'], inplace=True)

# Create labels for opinions based on categories
df_matched['Predicted'] = df_matched['Category'].apply(lambda x: 1 if x == 'TP' or x == 'FP' else 0)
df_matched['Actual'] = df_matched['Category'].apply(lambda x: 1 if x == 'TP' or x == 'FN' else 0)

# Extract predicted and actual opinions
predicted_opinions = df_matched['Predicted']
actual_opinions = df_matched['Actual']

# Define labels for confusion matrix and classification report
labels = [0, 1]  # 0: Not Mentioned, 1: Mentioned

# Build confusion matrix
cm = confusion_matrix(actual_opinions, predicted_opinions, labels=labels)

# Convert confusion matrix to DataFrame
cm_df = pd.DataFrame(cm, index=['Not Mentioned', 'Mentioned'], columns=['Not Mentioned', 'Mentioned'])

sns.set(font_scale=1.2)
# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='d')
plt.title('Confusion Matrix for Opinions')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Build classification report
class_report = classification_report(actual_opinions, predicted_opinions, labels=labels, target_names=['Not Mentioned', 'Mentioned'])

print('Classification Report:')
print(class_report)

import json
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# Load data from results.json
with open('results_filter_dev.json', 'r') as f:
    data = json.load(f)

# Prepare predictions and actuals data from the JSON
predictions = []
actuals = []

for entry in data:
    sentence_id = entry['sentence_id']
    for triplet in entry['Triplets']:
        aspect = triplet['Aspect']
        opinion = triplet['Opinion']
        predictions.append((sentence_id, aspect, opinion))
    for triplet in entry['True Triplets']:
        aspect = triplet['Aspect']
        opinion = triplet['Opinion']
        actuals.append((sentence_id, aspect, opinion))

# Convert lists to DataFrames
df_predictions = pd.DataFrame(predictions, columns=['Sentence ID', 'Aspect', 'Opinion'])
df_actuals = pd.DataFrame(actuals, columns=['Sentence ID', 'Aspect', 'Opinion'])

# Merge DataFrames on Sentence ID, Aspect, and Opinion
df_matched = pd.merge(df_predictions, df_actuals, on=['Sentence ID', 'Aspect', 'Opinion'], how='outer', indicator=True)

# Determine the category (TP, FP, FN, TN)
df_matched['Category'] = df_matched['_merge'].map({
    'both': 'TP',
    'left_only': 'FP',
    'right_only': 'FN',
    'not both': 'TN'
})

# Drop the merge indicator column
df_matched.drop(columns=['_merge'], inplace=True)

# Create labels for aspects and opinions based on categories
df_matched['Predicted'] = df_matched['Category'].apply(lambda x: 1 if x == 'TP' or x == 'FP' else 0)
df_matched['Actual'] = df_matched['Category'].apply(lambda x: 1 if x == 'TP' or x == 'FN' else 0)

# Extract predicted and actual categories
predicted_categories = df_matched['Predicted']
actual_categories = df_matched['Actual']

# Define labels for confusion matrix and classification report
labels = [0, 1]  # 0: Not Mentioned, 1: Mentioned

# Build confusion matrix
cm = confusion_matrix(actual_categories, predicted_categories, labels=labels)

# Convert confusion matrix to DataFrame
cm_df = pd.DataFrame(cm, index=['Not Mentioned', 'Mentioned'], columns=['Not Mentioned', 'Mentioned'])

sns.set(font_scale=1.2)
# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='d')
plt.title('Confusion Matrix for Aspects and Opinions')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Build classification report
class_report = classification_report(actual_categories, predicted_categories, labels=labels, target_names=['Not Mentioned', 'Mentioned'])

print('Classification Report:')
print(class_report)

import json
import pandas as pd

# Load data from results.json
with open('updated_results.json', 'r') as f:
    data = json.load(f)

# Prepare predictions and actuals data from the JSON
predictions = []
actuals = []

for entry in data:
    sentence_id = entry['sentence_id']
    for triplet in entry['Triplets']:
        aspect = triplet['Aspect']
        opinion = triplet['Opinion']
        predictions.append((sentence_id, aspect, opinion))
    for triplet in entry['True Triplets']:
        aspect = triplet['Aspect']
        opinion = triplet['Opinion']
        actuals.append((sentence_id, aspect, opinion))

# Convert lists to DataFrames
df_predictions = pd.DataFrame(predictions, columns=['Sentence ID', 'Aspect', 'Opinion'])
df_actuals = pd.DataFrame(actuals, columns=['Sentence ID', 'Aspect', 'Opinion'])

# Count unique aspects and opinions in predictions and actuals
unique_aspects_predictions = df_predictions['Aspect'].nunique()
unique_opinions_predictions = df_predictions['Opinion'].nunique()

unique_aspects_actuals = df_actuals['Aspect'].nunique()
unique_opinions_actuals = df_actuals['Opinion'].nunique()

print(f'Unique Aspects in Predictions: {unique_aspects_predictions}')
print(f'Unique Opinions in Predictions: {unique_opinions_predictions}')

print(f'Unique Aspects in Actuals: {unique_aspects_actuals}')
print(f'Unique Opinions in Actuals: {unique_opinions_actuals}')

# Extract predicted and actual sentiments for opinions only
predicted_opinions = []
actual_opinions = []

for obj in data:
    for triplet in obj["Triplets"]:
        predicted_opinions.append(triplet["Opinion"])  # Memasukkan opini saja tanpa memperhitungkan aspek

    for true_triplet in obj["True Triplets"]:
        actual_opinions.append(true_triplet["Opinion"])  # Memasukkan opini saja tanpa memperhitungkan aspek

# Define unique opinions (classes) based on data
unique_opinions = list(set(actual_opinions + predicted_opinions))

# Create confusion matrix for opinions
cm_opinions = confusion_matrix(actual_opinions, predicted_opinions, labels=unique_opinions)

# Convert confusion matrix to DataFrame for visualization
cm_df_opinions = pd.DataFrame(cm_opinions, index=unique_opinions, columns=unique_opinions)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_df_opinions, annot=True, cmap="Blues", fmt="d")
plt.title("Confusion Matrix for Opinions")
plt.xlabel("Predicted Opinion")
plt.ylabel("Actual Opinion")
plt.show()

# Create classification report for opinions only
classification_rep_opinions = classification_report(actual_opinions, predicted_opinions, target_names=unique_opinions, zero_division=0)

print("Classification Report for Opinions Only:")
print(classification_rep_opinions)

import json
import pandas as pd

# Input file path
# input_file = "results_filtered.json"

# # Load data from JSON
# with open(input_file, "r") as file:
#     data = json.load(file)

# Initialize a list to store mismatches
mismatches = []

# Extract mismatches between true and predicted triplets
for entry in data:
    true_triplets = set((triplet['Aspect'], triplet['Opinion'], triplet['Polarity']) for triplet in entry['True Triplets'])
    predicted_triplets = set((triplet['Aspect'], triplet['Opinion'], triplet['Polarity']) for triplet in entry['Triplets'])

    if true_triplets != predicted_triplets:
        mismatches.append({
            'Sentence ID': entry['sentence_id'],
            'Sentence': entry['sentence'],
            'True Triplets': true_triplets,
            'Predicted Triplets': predicted_triplets
        })

# Convert mismatches list to DataFrame
mismatches_df = pd.DataFrame(mismatches)

# Specify output file path
output_file = "mismatches.csv"

# Save mismatches DataFrame to CSV
mismatches_df.to_csv(output_file, index=False)

print(f"Mismatches have been saved to {output_file}")

import json

# Input and output file paths
input_file = "updated_results_dev.json"
output_file = "results_filter_dev.json"

# input_file = "updated_results.json"
# output_file = "updated_results_filter.json"

# Load data from JSON
with open(input_file, "r") as file:
    data = json.load(file)

# Initialize a list to store filtered entries
filtered_data = []

# Filter entries that have errors or missing Triplets/True Triplets
for entry in data:
    if 'Triplets' not in entry or 'True Triplets' not in entry:
        continue  # Skip entries without Triplets or True Triplets

    triplets = entry['Triplets']
    true_triplets = entry['True Triplets']

    # # Check if either Triplets or True Triplets are empty
    # if not triplets or not true_triplets:
    #     continue  # Skip entries with empty Triplets or True Triplets
    if not true_triplets:
        continue  # Skip entries with empty Triplets or True Triplets


    # # Check if Triplets and True Triplets have the same length
    # if len(triplets) != len(true_triplets):
    #     continue  # Skip entries where the lengths of Triplets and True Triplets don't match

    # # Validate each triplet structure (Aspect, Opinion, Polarity)
    # valid_triplets = True
    # for triplet in triplets:
    #     if not all(key in triplet for key in ['Aspect', 'Opinion', 'Polarity']):
    #         valid_triplets = False
    #         break
    # if not valid_triplets:
    #     continue  # Skip entries with malformed Triplets

    # for triplet in true_triplets:
    #     if not all(key in triplet for key in ['Aspect', 'Opinion', 'Polarity']):
    #         valid_triplets = False
    #         break
    # if not valid_triplets:
    #     continue  # Skip entries with malformed True Triplets

    # If all checks pass, add the entry to filtered_data
    filtered_data.append(entry)

# Save filtered data back to JSON
with open(output_file, "w") as file:
    json.dump(filtered_data, file, indent=2)

print(f"Filtered data has been saved to {output_file}")

import json
import pandas as pd

# Input file path
input_file = "updated_results.json"

# Load data from JSON
with open(input_file, "r") as file:
    data = json.load(file)

# Initialize a list to store mismatches
mismatches = []

# Extract mismatches between true and predicted triplets
for entry in data:
    true_triplets = set((triplet['Aspect'], triplet['Opinion'], triplet['Polarity']) for triplet in entry['True Triplets'])
    predicted_triplets = set((triplet['Aspect'], triplet['Opinion'], triplet['Polarity']) for triplet in entry['Triplets'])

    if true_triplets != predicted_triplets:
        mismatches.append({
            'Sentence ID': entry['sentence_id'],
            'Sentence': entry['sentence'],
            'True Triplets': true_triplets,
            'Predicted Triplets': predicted_triplets
        })

# Convert mismatches list to DataFrame
mismatches_df = pd.DataFrame(mismatches)

# Specify output file path
output_file = "mismatches.csv"

# Save mismatches DataFrame to CSV
mismatches_df.to_csv(output_file, index=False)

print(f"Mismatches have been saved to {output_file}")

# prompt: count data result.json vs count row mismatches.csv

import json
import pandas as pd

# Count data in result.json
with open('results_filter.json', 'r') as f:
  rdata = json.load(f)
count_result = len(rdata)
print("Jumlah data dalam result.json:", count_result)

with open('updated_results.json', 'r') as f:
  udata = json.load(f)
count_uresult = len(udata)
print("Jumlah data dalam updated result.json:", count_uresult)

# Count rows in mismatches.csv
mismatches_df = pd.read_csv('mismatches.csv')
count_mismatches = mismatches_df.shape[0]
print("Jumlah baris dalam mismatches.csv:", count_mismatches)

actual_df = pd.read_csv('actuals.csv')
count_actual = actual_df.shape[0]
print("Jumlah baris dalam actual.csv:", count_actual)

predict_df = pd.read_csv('predictions.csv')
count_predict = predict_df.shape[0]
print("Jumlah baris dalam prediction.csv:", count_predict)

matches_df = pd.read_csv('matched_aspects_opinions.csv')
count_matches = matches_df.shape[0]
print("Jumlah baris dalam matched_aspects_opinions.csv:", count_matches)

print(count_result-count_mismatches)



import json
import pandas as pd

# Load data from JSON
with open('results_filtered.json', 'r') as file:
    data = json.load(file)

# Initialize a list to store partial matches
partial_matches = []

# Extract partial matches between true and predicted triplets
for entry in data:
    sentence_id = entry['sentence_id']
    sentence = entry['sentence']
    true_triplets = entry['True Triplets']
    predicted_triplets = entry['Triplets']

    true_set = set((t['Aspect'], t['Opinion'], t['Polarity']) for t in true_triplets)
    predicted_set = set((t['Aspect'], t['Opinion'], t['Polarity']) for t in predicted_triplets)

    # Check for partial matches
    partial_true = true_set - predicted_set
    partial_pred = predicted_set - true_set
    intersection = true_set & predicted_set

    if partial_true or partial_pred:
        for t in partial_true:
            partial_matches.append({
                'Sentence ID': sentence_id,
                'Sentence': sentence,
                'True Aspect': t[0],
                'True Opinion': t[1],
                'True Polarity': t[2],
                'Predicted Aspect': None,
                'Predicted Opinion': None,
                'Predicted Polarity': None
            })
        for p in partial_pred:
            partial_matches.append({
                'Sentence ID': sentence_id,
                'Sentence': sentence,
                'True Aspect': None,
                'True Opinion': None,
                'True Polarity': None,
                'Predicted Aspect': p[0],
                'Predicted Opinion': p[1],
                'Predicted Polarity': p[2]
            })
        for i in intersection:
            partial_matches.append({
                'Sentence ID': sentence_id,
                'Sentence': sentence,
                'True Aspect': i[0],
                'True Opinion': i[1],
                'True Polarity': i[2],
                'Predicted Aspect': i[0],
                'Predicted Opinion': i[1],
                'Predicted Polarity': i[2]
            })

# Convert to DataFrame
partial_matches_df = pd.DataFrame(partial_matches)

# Specify output file path
output_file = "partial_matches.csv"

# Save to CSV
partial_matches_df.to_csv(output_file, index=False)

print(f"Partial matches have been saved to {output_file}")

import matplotlib.pyplot as plt

values = [0.77, 0.61, 0.74, 0.60] #ganti value
labels = ['SS', 'SM', 'MS', 'MM'] #ganti value

colors = ['red', 'blue', 'green', 'orange']

plt.bar(labels, values, color=colors)
plt.xlabel('Categories')
plt.ylabel('Values')
plt.title('Perbandingan F1-Score Test')

plt.show()

import pandas as pd

# Daftar file CSV dan output yang sesuai
file_paths = ['for pred gibran.csv', 'for pred prabowo.csv', 'for pred imin.csv', 'for pred mahfud.csv']
output_files = ['pred_gibran.txt', 'pred_prabowo.txt', 'pred_imin.txt', 'pred_mahfud.txt']

# Loop melalui setiap file CSV dan simpan kolom 'textDisplay' ke file teks
for file_path, output_file in zip(file_paths, output_files):
    # Membaca file CSV
    df = pd.read_csv(file_path)

    # Mengambil kolom 'textDisplay'
    text_display_column = df['textDisplay']

    # Menyimpan nilai-nilai kolom ke file teks tanpa header
    with open(output_file, 'w', encoding='utf-8') as f:
        for text in text_display_column:
            f.write(f"{text}\n")

    print(f"File '{file_path}' telah berhasil dikonversi dan disimpan sebagai '{output_file}'")

# prompt: zip pred_{name}.txt

!zip pred.zip pred_anis.txt pred_gibran.txt pred_prabowo.txt pred_imin.txt pred_mahfud.txt

with open('results_filter_ganjar.json', 'r') as f:
  data = json.load(f)
count_result = len(data)
print("Jumlah data dalam result.json:", count_result)

import json
import matplotlib.pyplot as plt
from collections import Counter
from wordcloud import WordCloud
import numpy as np

# Fungsi untuk plot top 10 aspek berdasarkan sentimen
def plot_top_aspects(aspects, sentiment_label, color):
    aspect_counts = Counter(aspects)
    top_aspects = aspect_counts.most_common(10)

    labels, values = zip(*top_aspects)
    x = np.arange(len(labels))

    plt.figure(figsize=(12, 8))
    plt.bar(x, values, color=color)
    plt.xlabel('Aspects')
    plt.ylabel('Frequency')
    plt.title(f'Top 10 Aspects ({sentiment_label})')
    plt.xticks(x, labels, rotation='vertical')
    plt.tight_layout()
    plt.show()

    return [aspect for aspect, _ in top_aspects]

# Fungsi untuk membuat wordcloud dari opini
def create_wordcloud(opinions, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(opinions))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title)
    plt.axis('off')
    plt.show()

# Load data dari JSON
input_file = "result.json"
with open(input_file, "r") as file:
    data = json.load(file)

# Mengumpulkan aspek dan opini berdasarkan sentimen
aspects_pos, aspects_neu, aspects_neg = [], [], []
opinions_pos, opinions_neu, opinions_neg = [], [], []

for obj in data:
    for triplet in obj["True Triplets"]:
        aspect = triplet["Aspect"]
        opinion = triplet["Opinion"]
        polarity = triplet["Polarity"]
        if polarity == "Positive":
            aspects_pos.append(aspect)
            opinions_pos.append(opinion)
        elif polarity == "Neutral":
            aspects_neu.append(aspect)
            opinions_neu.append(opinion)
        elif polarity == "Negative":
            aspects_neg.append(aspect)
            opinions_neg.append(opinion)

# Plot top 10 aspek berdasarkan sentimen
top_aspects_pos = plot_top_aspects(aspects_pos, 'Positif', '#006400')
top_aspects_neu = plot_top_aspects(aspects_neu, 'Netral', '#FF8C00')
top_aspects_neg = plot_top_aspects(aspects_neg, 'Negatif', '#8B0000')

# Mengumpulkan opini dari top aspek
top_opinions_pos = [opinion for aspect, opinion in zip(aspects_pos, opinions_pos) if aspect in top_aspects_pos]
top_opinions_neu = [opinion for aspect, opinion in zip(aspects_neu, opinions_neu) if aspect in top_aspects_neu]
top_opinions_neg = [opinion for aspect, opinion in zip(aspects_neg, opinions_neg) if aspect in top_aspects_neg]

# Membuat wordcloud untuk setiap sentimen
create_wordcloud(top_opinions_pos, "Wordcloud Opini (Positif)")
create_wordcloud(top_opinions_neu, "Wordcloud Opini (Netral)")
create_wordcloud(top_opinions_neg, "Wordcloud Opini (Negatif)")

import json
import matplotlib.pyplot as plt
from collections import Counter
from wordcloud import WordCloud
import numpy as np

# Load data dari JSON
input_file = "results_filter_ganjar.json"
with open(input_file, "r") as file:
    data = json.load(file)

# Mengumpulkan aspek dan opini berdasarkan sentimen
aspects_pos, aspects_neu, aspects_neg = [], [], []
opinions_pos, opinions_neu, opinions_neg = [], [], []

for obj in data:
    for triplet in obj["Triplets"]:
        aspect = triplet["Aspect"]
        opinion = triplet["Opinion"]
        polarity = triplet["Polarity"]
        if polarity == "Positive":
            aspects_pos.append(aspect)
            opinions_pos.append(opinion)
        elif polarity == "Neutral":
            aspects_neu.append(aspect)
            opinions_neu.append(opinion)
        elif polarity == "Negative":
            aspects_neg.append(aspect)
            opinions_neg.append(opinion)

# Fungsi untuk plot top 10 aspek berdasarkan sentimen
def plot_top_aspects(aspects, sentiment_label, color):
    aspect_counts = Counter(aspects)
    top_aspects = aspect_counts.most_common(10)

    labels, values = zip(*top_aspects)
    x = np.arange(len(labels))

    plt.figure(figsize=(12, 8))
    plt.bar(x, values, color=color)
    plt.xlabel('Aspects')
    plt.ylabel('Frequency')
    plt.title(f'Top 10 Aspects ({sentiment_label})')
    plt.xticks(x, labels, rotation='vertical')
    plt.tight_layout()
    plt.show()

    return [aspect for aspect, _ in top_aspects]

# Fungsi untuk membuat wordcloud dari opini
def create_wordcloud(opinions, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(opinions))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title)
    plt.axis('off')
    plt.show()

# Plot top 10 aspek berdasarkan sentimen
top_aspects_pos = plot_top_aspects(aspects_pos, 'Positif', '#006400')
top_aspects_neu = plot_top_aspects(aspects_neu, 'Netral', '#FF8C00')
top_aspects_neg = plot_top_aspects(aspects_neg, 'Negatif', '#8B0000')

# Mengumpulkan opini dari top aspek
top_opinions_pos = [opinion for aspect, opinion in zip(aspects_pos, opinions_pos) if aspect in top_aspects_pos]
top_opinions_neu = [opinion for aspect, opinion in zip(aspects_neu, opinions_neu) if aspect in top_aspects_neu]
top_opinions_neg = [opinion for aspect, opinion in zip(aspects_neg, opinions_neg) if aspect in top_aspects_neg]

# Membuat wordcloud untuk setiap sentimen
create_wordcloud(aspects_pos, "Wordcloud Aspek (Positif)")
create_wordcloud(opinions_pos, "Wordcloud Opini (Positif)")

create_wordcloud(aspects_neu, "Wordcloud Aspek (Netral)")
create_wordcloud(opinions_neu, "Wordcloud Opini (Netral)")

create_wordcloud(aspects_neg, "Wordcloud Aspek (Negatif)")
create_wordcloud(opinions_neg, "Wordcloud Opini (Negatif)")

# Plot sentimen dari top 10 aspek secara keseluruhan
# Fungsi untuk plot bar bertumpuk membandingkan frekuensi sentimen untuk 10 aspek paling umum
def plot_sentiment_comparison(aspects, sentiments, sentiment_labels):
    aspect_sentiment_frequency = {}

    for aspect, sentiment in zip(aspects, sentiments):
        if aspect not in aspect_sentiment_frequency:
            aspect_sentiment_frequency[aspect] = {label: 0 for label in sentiment_labels}
        aspect_sentiment_frequency[aspect][sentiment] += 1

    # Ambil 10 aspek dengan frekuensi tertinggi secara keseluruhan
    overall_aspect_frequency = {aspect: sum(sentiments.values()) for aspect, sentiments in aspect_sentiment_frequency.items()}
    top_10_aspects_overall = Counter(overall_aspect_frequency).most_common(10)

    # Extract aspects and their sentiment frequencies
    top_aspects = [aspect[0] for aspect in top_10_aspects_overall]
    top_aspects_frequencies = [aspect_sentiment_frequency[aspect] for aspect in top_aspects]

    # Buat plot bar bertumpuk untuk membandingkan frekuensi sentimen untuk 10 aspek paling umum
    x = np.arange(len(top_aspects))  # Label locations
    width = 0.5  # Width of the bars

    fig, ax = plt.subplots(figsize=(14, 8))

    # Sentimen counts
    pos_counts = [freq['Positive'] for freq in top_aspects_frequencies]
    neu_counts = [freq['Neutral'] for freq in top_aspects_frequencies]
    neg_counts = [freq['Negative'] for freq in top_aspects_frequencies]

    # Buat plot bertumpuk
    ax.bar(x, pos_counts, width, label='Positive', color='g')
    ax.bar(x, neu_counts, width, label='Neutral', color='y', bottom=pos_counts)
    ax.bar(x, neg_counts, width, label='Negative', color='r', bottom=np.array(pos_counts) + np.array(neu_counts))

    # Add some text for labels, title and custom x-axis tick labels, etc.
    ax.set_xlabel('Aspects')
    ax.set_ylabel('Sentiment Counts')
    ax.set_title('Sentiment Comparison for Top 10 Aspects')
    ax.set_xticks(x)
    ax.set_xticklabels(top_aspects, rotation=45, ha='right')
    ax.legend()

    fig.tight_layout()
    plt.show()

# Plot perbandingan sentimen dari top 10 aspek secara keseluruhan
plot_sentiment_comparison(aspects_pos + aspects_neu + aspects_neg,
                          ['Positive'] * len(aspects_pos) + ['Neutral'] * len(aspects_neu) + ['Negative'] * len(aspects_neg),
                          ['Positive', 'Neutral', 'Negative'])

# Fungsi untuk membuat wordcloud opini berdasarkan top aspek
def create_wordcloud(opinions, title):
    text = ' '.join(opinions)
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title)
    plt.axis('off')
    plt.tight_layout(pad=0)
    plt.show()

# Ekstraksi dan plot wordcloud untuk opini berdasarkan top aspek
for aspect in top_aspects_pos:
    opinions_for_aspect = [opinion for aspect_item, opinion in zip(aspects_pos, opinions_pos) if aspect_item == aspect]
    create_wordcloud(opinions_for_aspect, f'Word Cloud for Top Aspect: {aspect} (Positive)')

for aspect in top_aspects_neu:
    opinions_for_aspect = [opinion for aspect_item, opinion in zip(aspects_neu, opinions_neu) if aspect_item == aspect]
    create_wordcloud(opinions_for_aspect, f'Word Cloud for Top Aspect: {aspect} (Neutral)')

for aspect in top_aspects_neg:
    opinions_for_aspect = [opinion for aspect_item, opinion in zip(aspects_neg, opinions_neg) if aspect_item == aspect]
    create_wordcloud(opinions_for_aspect, f'Word Cloud for Top Aspect: {aspect} (Negative)')

# Fungsi untuk plot distribusi sentimen
def plot_sentiment_distribution(freq_pos, freq_neu, freq_neg):
    sentiment_counts = [freq_pos, freq_neu, freq_neg]
    sentiment_labels = ['Positif', 'Netral', 'Negatif']
    bars = plt.bar(sentiment_labels, sentiment_counts, color=['green', 'orange', 'red'])
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2, height, str(int(height)), ha='center', va='bottom')

    plt.legend(sentiment_labels)
    plt.title("Distribusi Sentimen")
    plt.show()

# Hitung frekuensi sentimen
freq_pos = len(aspects_pos)
freq_neu = len(aspects_neu)
freq_neg = len(aspects_neg)

# Plot distribusi sentimen
plot_sentiment_distribution(freq_pos, freq_neu, freq_neg)

# Fungsi untuk membuat WordCloud dari semua aspek
def create_aspect_wordcloud(aspects):
    all_aspects_text = ' '.join(aspects)
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_aspects_text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title('WordCloud Aspek (Tanpa Sentimen)')
    plt.axis('off')
    plt.show()

# Gabungkan semua aspek dari berbagai sentimen
all_aspects = aspects_pos + aspects_neu + aspects_neg

# Buat WordCloud untuk semua aspek
create_aspect_wordcloud(all_aspects)

aspects = []
for obj in data:
    for triplet in obj["Triplets"]:
        aspect = triplet["Aspect"]
        opinion = triplet["Opinion"]
        polarity = triplet["Polarity"]
        aspects.append(aspect)
        if polarity == "Positive":
            opinions_pos.append(opinion)
        elif polarity == "Neutral":
            opinions_neu.append(opinion)
        elif polarity == "Negative":
            opinions_neg.append(opinion)

# Fungsi untuk plot top 10 aspek
def plot_top_aspects(aspects):
    aspect_counts = Counter(aspects)
    top_aspects = aspect_counts.most_common(10)

    labels, values = zip(*top_aspects)
    x = np.arange(len(labels))

    plt.figure(figsize=(12, 8))
    plt.bar(x, values, color='blue')
    plt.xlabel('Aspects')
    plt.ylabel('Frequency')
    plt.title('Top 10 Aspects (Overall)')
    plt.xticks(x, labels, rotation='vertical')
    plt.tight_layout()
    plt.show()

    return [aspect for aspect, _ in top_aspects]

top_aspects = plot_top_aspects(aspects)

import json
import ast

# Load data from results_filter.json
with open('results_filter.json', 'r') as f:
    data = json.load(f)

# Load text data from file
text_file_path = 'dataset (5).txt'
with open(text_file_path, 'r', encoding='utf-8') as f:
    lines = f.readlines()

# Extract true triplets from the text data
true_triplets_from_text = []
for line in lines:
    sentence, triplets_str = line.strip().split("####")
    triplets = ast.literal_eval(triplets_str)
    sentence_split = sentence.split(" ")

    triplet_list = []
    for aspect_index, opinion_index, polarity in triplets:
        aspect = ' '.join([sentence_split[i] for i in aspect_index])
        opinion = ' '.join([sentence_split[i] for i in opinion_index])
        triplet_list.append({
            "Aspect": aspect,
            "Opinion": opinion,
            "Polarity": polarity
        })
    true_triplets_from_text.append({
        "sentence": sentence,
        "True Triplets": triplet_list
    })

# Update data from results_filter.json with true triplets from text data
for entry in data:
    sentence = entry['sentence']
    for true_triplet_entry in true_triplets_from_text:
        if true_triplet_entry['sentence'] == sentence:
            entry['True Triplets'] = true_triplet_entry['True Triplets']
            break

# Save the updated data to a new JSON file
with open('updated_results.json', 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=4)

print("Updated results have been saved to 'updated_results.json'.")

import json
import ast

# Step 1: Load data from results_filter.json
with open('result.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# Step 2: Load data from dataset (5).txt and prepare a dictionary of true triplets by sentence
true_triplets_from_text = {}
with open('dataset (5).txt', 'r', encoding='utf-8') as f:
    for line in f:
        sentence, triplets_str = line.strip().split("####")
        triplets = ast.literal_eval(triplets_str)
        true_triplets_from_text[sentence] = triplets

# Step 3: Match sentences and update True Triplets in data
for entry in data:
    sentence = entry['sentence']
    if sentence in true_triplets_from_text:
        entry['True Triplets'] = true_triplets_from_text[sentence]

# Step 4: Save the updated data to a new JSON file
with open('updated_results.json', 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=4)

print("Updated results have been saved to 'updated_results.json'.")

import json
import ast

# Load data from results_filter.json
with open('updated_results.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# Load data from dataset (5).txt and prepare a dictionary of true triplets by sentence
true_triplets_from_text = {}
with open('dataset (5).txt', 'r', encoding='utf-8') as f:
    for line in f:
        parts = line.strip().split("####")
        sentence = parts[0]
        triplets_str = parts[1]
        triplets = ast.literal_eval(triplets_str)
        true_triplets_from_text[sentence] = triplets

# Filter entries with empty True Triplets and print them
entries_with_empty_triplets = []
for entry in data:
    if not entry['True Triplets']:
        entries_with_empty_triplets.append(entry)

# Print the filtered entries
print("Entries with empty True Triplets:")
for entry in entries_with_empty_triplets:
    print(entry)

# Example of what you might do next:
# You can further process or modify these entries as needed.

# If you want to save these entries to a JSON file, you can do so as follows:
# with open('entries_with_empty_triplets.json', 'w', encoding='utf-8') as f:
#     json.dump(entries_with_empty_triplets, f, ensure_ascii=False, indent=4)

import ast

# Buat list untuk menyimpan baris yang memiliki triplet kosong
entries_with_empty_triplets = []

# Buka file dataset (5).txt dan proses baris per baris
with open('dataset (5).txt', 'r', encoding='utf-8') as f:
    for line in f:
        parts = line.strip().split("####")
        sentence = parts[0]
        triplets_str = parts[1]

        # Coba mengubah string triplet menjadi bentuk yang bisa dibaca Python
        try:
            triplets = ast.literal_eval(triplets_str)
        except ValueError:
            triplets = []

        # Jika triplet kosong, tambahkan baris ke list
        if not triplets:
            entries_with_empty_triplets.append(line.strip())

# Cetak baris-baris yang memiliki triplet kosong
print("Entries with empty triplets:")
for entry in entries_with_empty_triplets:
    print(entry)

"""analisis kesalahan"""

import json
import ast

# Step 1: Load data from result.json
with open('resulttest.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# Step 2: Load data from dataset (5).txt and prepare a dictionary of true triplets by sentence
true_triplets_from_text = {}
with open('test.txt', 'r', encoding='utf-8') as f:
    for line in f:
        sentence, triplets_str = line.strip().split("####")
        triplets = ast.literal_eval(triplets_str)
        true_triplets_from_text[sentence.strip()] = triplets

# Step 3: Match sentences and update True Triplets in data
for entry in data:
    if 'sentence' in entry:
        sentence = entry['sentence'].strip()
        if sentence in true_triplets_from_text:
            true_triplet_indices = true_triplets_from_text[sentence]
            true_triplets = []
            for indices in true_triplet_indices:
                aspect_indices, opinion_indices, polarity = indices
                aspect = " ".join(sentence.split()[i] for i in aspect_indices)
                opinion = " ".join(sentence.split()[i] for i in opinion_indices)
                polarity_dict = {'POS': 'Positive', 'NEG': 'Negative', 'NEU': 'Neutral'}
                true_triplets.append({
                    "Aspect": aspect,
                    "Opinion": opinion,
                    "Polarity": polarity_dict[polarity]
                })
            entry['True Triplets'] = true_triplets

# Step 4: Save the updated data to a new JSON file
with open('updated_results_test.json', 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=4)

print("Updated results have been saved to 'updated_results.json'.")

import json
import pandas as pd

# Input and output file paths
input_file = "updated_results_test.json"

# Output files for each condition
error_file = "error.csv"
cant_predict_file = "cant_predict.csv"
mispredict_file = "mispredict.csv"
matched_file = "matched.csv"

# Load data from JSON
with open(input_file, "r") as file:
    data = json.load(file)

# Initialize lists to store entries for each condition
error_data = []
cant_predict_data = []
mispredict_data = []
matched_data = []

# Filter entries based on conditions
for entry in data:
    if 'Triplets' not in entry or 'True Triplets' not in entry:
        error_data.append(entry)
        continue  # Skip entries without Triplets or True Triplets

    triplets = entry['Triplets']
    true_triplets = entry['True Triplets']

    if not true_triplets:
        error_data.append(entry)  # Both Triplets and True Triplets are empty
    elif not triplets and true_triplets:
        cant_predict_data.append(entry)  # Triplets are empty but True Triplets are not
    # elif triplets and not true_triplets:
    #     error_data.append(entry)  # True Triplets are empty but Triplets are not
    else:
        # If both Triplets and True Triplets are present
        triplet_set = {(t['Aspect'], t['Opinion'], t['Polarity']) for t in triplets}
        true_triplet_set = {(t['Aspect'], t['Opinion'], t['Polarity']) for t in true_triplets}

        if triplet_set == true_triplet_set:
            matched_data.append(entry)  # Triplets match True Triplets
        else:
            mispredict_data.append(entry)  # Triplets do not match True Triplets

# Convert lists to DataFrames
error_df = pd.DataFrame(error_data)
cant_predict_df = pd.DataFrame(cant_predict_data)
mispredict_df = pd.DataFrame(mispredict_data)
matched_df = pd.DataFrame(matched_data)

# Save DataFrames to CSV files
error_df.to_csv(error_file, index=False, encoding="utf-8")
cant_predict_df.to_csv(cant_predict_file, index=False, encoding="utf-8")
mispredict_df.to_csv(mispredict_file, index=False, encoding="utf-8")
matched_df.to_csv(matched_file, index=False, encoding="utf-8")

print(f"Filtered data has been saved to {error_file}, {cant_predict_file}, {mispredict_file}, and {matched_file}.")

import json
import pandas as pd
import re

# Input and output file paths
mispredict_file = "mispredict.csv"

# Output files for each condition
cond1_partial_mistake_file = "cond1_partial_mistake.csv"
cond1_matched_aspect_opinion_file = "cond1_matched_aspect_opinion.csv"
cond1_all_wrong_file = "cond1_all_wrong.csv"
cond2_pred_more_all_match_file = "cond2_pred_more_all_match.csv"
cond2_pred_more_some_match_file = "cond2_pred_more_some_match.csv"
cond2_pred_less_some_match_file = "cond2_pred_less_some_match.csv"
cond2_pred_no_match_file = "cond2_pred_no_match.csv"

# Load data from mispredict.csv
mispredict_df = pd.read_csv(mispredict_file)

# Initialize lists to store entries for each condition
cond1_partial_mistake_data = []
cond1_matched_aspect_opinion_data = []
cond1_all_wrong_data = []
cond2_pred_more_all_match_data = []
cond2_pred_more_some_match_data = []
cond2_pred_less_some_match_data = []
cond2_pred_no_match_data = []

# Function to convert JSON string to list of dictionaries
def json_to_list(json_str):
    if pd.isna(json_str) or json_str == '[]':
        return []
    try:
        # Replace single quotes with double quotes
        json_str = json_str.replace("'", '"')
        # Fix common JSON errors
        json_str = re.sub(r',\s*}', '}', json_str)  # trailing commas before }
        json_str = re.sub(r',\s*]', ']', json_str)  # trailing commas before ]
        json_str = re.sub(r'(\w+):', r'"\1":', json_str)  # missing quotes for keys
        json_str = re.sub(r'""', r'"', json_str)  # remove double double quotes
        json_str = re.sub(r'"{', '{', json_str)  # remove quotes before {
        json_str = re.sub(r'}"', '}', json_str)  # remove quotes after }
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {json_str}\nError: {e}")
        raise

# Process each entry in the DataFrame
for index, row in mispredict_df.iterrows():
    triplets = json_to_list(row['Triplets'])
    true_triplets = json_to_list(row['True Triplets'])

    triplet_set = {(t['Aspect'], t['Opinion'], t['Polarity']) for t in triplets}
    true_triplet_set = {(t['Aspect'], t['Opinion'], t['Polarity']) for t in true_triplets}

    # Kondisi 1: Jumlah triplet sama antara "true triplet" dan "triplet"
    if len(triplets) == len(true_triplets):
        partially_mistake = False
        matched_aspect_opinion = False
        all_wrong = False

        for t in triplets:
            t_tuple = (t['Aspect'], t['Opinion'])
            if t_tuple not in {(tt['Aspect'], tt['Opinion']) for tt in true_triplets}:
                partially_mistake = True
            elif any(t_tuple == (tt['Aspect'], tt['Opinion']) and t['Polarity'] != tt['Polarity'] for tt in true_triplets):
                matched_aspect_opinion = True
            elif t_tuple not in true_triplet_set:
                all_wrong = True

        if partially_mistake:
            cond1_partial_mistake_data.append(row.to_dict())
        elif matched_aspect_opinion:
            cond1_matched_aspect_opinion_data.append(row.to_dict())
        elif all_wrong:
            cond1_all_wrong_data.append(row.to_dict())

    # Kondisi 2 : Jumlah triplet tidak sama antara "true triplet" dan "triplet"
    else:
        if len(triplets) > len(true_triplets):
            all_match = all(tt in triplet_set for tt in true_triplet_set)
            some_match = any(tt in triplet_set for tt in true_triplet_set)

            if all_match:
                cond2_pred_more_all_match_data.append(row.to_dict())
            elif some_match:
                cond2_pred_more_some_match_data.append(row.to_dict())
            else:
                cond2_pred_no_match_data.append(row.to_dict())

        elif len(triplets) < len(true_triplets):
            some_match = any(tt in triplet_set for tt in true_triplet_set)
            no_extra_triplet = all(t in true_triplet_set for t in triplet_set)

            if some_match and no_extra_triplet:
                cond2_pred_less_some_match_data.append(row.to_dict())
            elif some_match and not no_extra_triplet:
                cond2_pred_more_some_match_data.append(row.to_dict())
            else:
                cond2_pred_no_match_data.append(row.to_dict())

# Convert lists to DataFrames
cond1_partial_mistake_df = pd.DataFrame(cond1_partial_mistake_data)
cond1_matched_aspect_opinion_df = pd.DataFrame(cond1_matched_aspect_opinion_data)
cond1_all_wrong_df = pd.DataFrame(cond1_all_wrong_data)
cond2_pred_more_all_match_df = pd.DataFrame(cond2_pred_more_all_match_data)
cond2_pred_more_some_match_df = pd.DataFrame(cond2_pred_more_some_match_data)
cond2_pred_less_some_match_df = pd.DataFrame(cond2_pred_less_some_match_data)
cond2_pred_no_match_df = pd.DataFrame(cond2_pred_no_match_data)

# Save DataFrames to CSV files
cond1_partial_mistake_df.to_csv(cond1_partial_mistake_file, index=False, encoding="utf-8")
cond1_matched_aspect_opinion_df.to_csv(cond1_matched_aspect_opinion_file, index=False, encoding="utf-8")
cond1_all_wrong_df.to_csv(cond1_all_wrong_file, index=False, encoding="utf-8")
cond2_pred_more_all_match_df.to_csv(cond2_pred_more_all_match_file, index=False, encoding="utf-8")
cond2_pred_more_some_match_df.to_csv(cond2_pred_more_some_match_file, index=False, encoding="utf-8")
cond2_pred_less_some_match_df.to_csv(cond2_pred_less_some_match_file, index=False, encoding="utf-8")
cond2_pred_no_match_df.to_csv(cond2_pred_no_match_file, index=False, encoding="utf-8")

print(f"Filtered mispredict data has been saved to the respective files.")

# prompt: count masing2 file dari code diatas

import pandas as pd

# File paths
cond1_partial_mistake_file = "cond1_partial_mistake.csv"
cond1_matched_aspect_opinion_file = "cond1_matched_aspect_opinion.csv"
cond1_all_wrong_file = "cond1_all_wrong.csv"
cond2_pred_more_all_match_file = "cond2_pred_more_all_match.csv"
cond2_pred_more_some_match_file = "cond2_pred_more_some_match.csv"
cond2_pred_less_some_match_file = "cond2_pred_less_some_match.csv"
cond2_pred_no_match_file = "cond2_pred_no_match.csv"

# Count rows in each file
cond1_partial_mistake_count = len(pd.read_csv(cond1_partial_mistake_file)) if os.path.exists(cond1_partial_mistake_file) else 0
cond1_matched_aspect_opinion_count = len(pd.read_csv(cond1_matched_aspect_opinion_file)) if os.path.exists(cond1_matched_aspect_opinion_file) else 0
cond1_all_wrong_count = len(pd.read_csv(cond1_all_wrong_file)) if os.path.exists(cond1_all_wrong_file) else 0
cond2_pred_more_all_match_count = len(pd.read_csv(cond2_pred_more_all_match_file)) if os.path.exists(cond2_pred_more_all_match_file) else 0
cond2_pred_more_some_match_count = len(pd.read_csv(cond2_pred_more_some_match_file)) if os.path.exists(cond2_pred_more_some_match_file) else 0
cond2_pred_less_some_match_count = len(pd.read_csv(cond2_pred_less_some_match_file)) if os.path.exists(cond2_pred_less_some_match_file) else 0
cond2_pred_no_match_count = len(pd.read_csv(cond2_pred_no_match_file)) if os.path.exists(cond2_pred_no_match_file) else 0

# Print the counts
print(f"Number of entries in cond1_partial_mistake.csv: {cond1_partial_mistake_count}")
print(f"Number of entries in cond1_matched_aspect_opinion.csv: {cond1_matched_aspect_opinion_count}")
print(f"Number of entries in cond1_all_wrong.csv: {cond1_all_wrong_count}")
print(f"Number of entries in cond2_pred_more_all_match.csv: {cond2_pred_more_all_match_count}")
print(f"Number of entries in cond2_pred_more_some_match.csv: {cond2_pred_more_some_match_count}")
print(f"Number of entries in cond2_pred_less_some_match.csv: {cond2_pred_less_some_match_count}")
print(f"Number of entries in cond2_pred_no_match.csv: {cond2_pred_no_match_count}")

import pandas as pd
import json
import re

# File paths for saving results
cond1_partial_mistake_aspect_incorrect_file = "cond1_partial_mistake_aspect_incorrect.csv"
cond1_partial_mistake_opinion_incorrect_file = "cond1_partial_mistake_opinion_incorrect.csv"
cond1_partial_mistake_polarity_correct_file = "cond1_partial_mistake_polarity_correct.csv"
cond1_partial_mistake_aspect_correct_file = "cond1_partial_mistake_aspect_correct.csv"
cond1_partial_mistake_opinion_correct_file = "cond1_partial_mistake_opinion_correct.csv"
cond1_all_wrong_file = "cond1_all_wrong.csv"

# Function to convert JSON string to list of dictionaries
def json_to_list(json_str):
    if pd.isna(json_str) or json_str == '[]':
        return []
    try:
        # Replace single quotes with double quotes
        json_str = json_str.replace("'", '"')
        # Fix common JSON errors
        json_str = re.sub(r',\s*}', '}', json_str)  # trailing commas before }
        json_str = re.sub(r',\s*]', ']', json_str)  # trailing commas before ]
        json_str = re.sub(r'(\w+):', r'"\1":', json_str)  # missing quotes for keys
        json_str = re.sub(r'""', r'"', json_str)  # remove double double quotes
        json_str = re.sub(r'"{', '{', json_str)  # remove quotes before {
        json_str = re.sub(r'}"', '}', json_str)  # remove quotes after }
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {json_str}\nError: {e}")
        raise

# Load data from cond1_partial_mistake.csv
cond1_partial_mistake_file = "cond1_partial_mistake.csv"
cond1_partial_mistake_df = pd.read_csv(cond1_partial_mistake_file)

# Initialize lists to store data for each category
cond1_partial_mistake_aspect_incorrect_data = []
cond1_partial_mistake_opinion_incorrect_data = []
cond1_partial_mistake_polarity_correct_data = []
cond1_partial_mistake_aspect_correct_data = []
cond1_partial_mistake_opinion_correct_data = []
cond1_all_wrong_data = []

# Process each entry in the DataFrame
for index, row in cond1_partial_mistake_df.iterrows():
    triplets = json_to_list(row['Triplets'])
    true_triplets = json_to_list(row['True Triplets'])

    aspect_incorrect = False
    opinion_incorrect = False
    polarity_correct = False
    aspect_correct = False
    opinion_correct = False
    all_wrong = True

    # Check if there's only one triplet
    if len(triplets) == 1:
        t = triplets[0]
        true_triplet = true_triplets[0]

        if t['Aspect'] != true_triplet['Aspect'] and t['Opinion'] == true_triplet['Opinion'] and t['Polarity'] == true_triplet['Polarity']:
            aspect_incorrect = True
            all_wrong = False
        elif t['Opinion'] != true_triplet['Opinion'] and t['Aspect'] == true_triplet['Aspect'] and t['Polarity'] == true_triplet['Polarity']:
            opinion_incorrect = True
            all_wrong = False
        elif t['Opinion'] != true_triplet['Opinion'] and t['Aspect'] == true_triplet['Aspect'] and t['Polarity'] != true_triplet['Polarity']:
            aspect_correct = True
            all_wrong = False
        elif t['Opinion'] == true_triplet['Opinion'] and t['Aspect'] != true_triplet['Aspect'] and t['Polarity'] != true_triplet['Polarity']:
            opinion_correct = True
            all_wrong = False
        elif t['Opinion'] != true_triplet['Opinion'] and t['Aspect'] != true_triplet['Aspect'] and t['Polarity'] == true_triplet['Polarity']:
            polarity_correct = True
            all_wrong = False

    # Check if there are multiple triplets (handled as before)
    else:
        for t in triplets:
            t_tuple = (t['Aspect'], t['Opinion'])
            true_tuple_set = {(tt['Aspect'], tt['Opinion']) for tt in true_triplets}

            if not (aspect_incorrect or opinion_incorrect or polarity_correct or aspect_correct or opinion_correct):
                if any(t['Aspect'] != tt['Aspect'] and t['Opinion'] == tt['Opinion'] and t['Polarity'] == tt['Polarity'] for tt in true_triplets):
                    aspect_incorrect = True
                    all_wrong = False
                elif any(t['Opinion'] != tt['Opinion'] and t['Aspect'] == tt['Aspect'] and t['Polarity'] == tt['Polarity'] for tt in true_triplets):
                    opinion_incorrect = True
                    all_wrong = False
                elif any(t['Opinion'] != tt['Opinion'] and t['Aspect'] == tt['Aspect'] and t['Polarity'] != tt['Polarity'] for tt in true_triplets):
                    aspect_correct = True
                    all_wrong = False
                elif any(t['Opinion'] == tt['Opinion'] and t['Aspect'] != tt['Aspect'] and t['Polarity'] != tt['Polarity'] for tt in true_triplets):
                    opinion_correct = True
                    all_wrong = False
                elif any(t['Opinion'] != tt['Opinion'] and t['Aspect'] != tt['Aspect'] and t['Polarity'] == tt['Polarity'] for tt in true_triplets):
                    polarity_correct = True
                    all_wrong = False

    # Append data to respective lists
    if aspect_incorrect and not (opinion_incorrect or polarity_correct or aspect_correct or opinion_correct):
        cond1_partial_mistake_aspect_incorrect_data.append(row.to_dict())
    elif opinion_incorrect and not (aspect_incorrect or polarity_correct or aspect_correct or opinion_correct):
        cond1_partial_mistake_opinion_incorrect_data.append(row.to_dict())
    elif polarity_correct and not (aspect_incorrect or opinion_incorrect or aspect_correct or opinion_correct):
        cond1_partial_mistake_polarity_correct_data.append(row.to_dict())
    elif aspect_correct and not (aspect_incorrect or opinion_incorrect or polarity_correct or opinion_correct):
        cond1_partial_mistake_aspect_correct_data.append(row.to_dict())
    elif opinion_correct and not (aspect_incorrect or opinion_incorrect or polarity_correct or aspect_correct):
        cond1_partial_mistake_opinion_correct_data.append(row.to_dict())
    elif all_wrong and not (aspect_incorrect or opinion_incorrect or polarity_correct or aspect_correct or opinion_correct):
        cond1_all_wrong_data.append(row.to_dict())

# Convert lists to DataFrames
cond1_partial_mistake_aspect_incorrect_df = pd.DataFrame(cond1_partial_mistake_aspect_incorrect_data)
cond1_partial_mistake_opinion_incorrect_df = pd.DataFrame(cond1_partial_mistake_opinion_incorrect_data)
cond1_partial_mistake_polarity_correct_df = pd.DataFrame(cond1_partial_mistake_polarity_correct_data)
cond1_partial_mistake_aspect_correct_df = pd.DataFrame(cond1_partial_mistake_aspect_correct_data)
cond1_partial_mistake_opinion_correct_df = pd.DataFrame(cond1_partial_mistake_opinion_correct_data)
cond1_all_wrong_df = pd.DataFrame(cond1_all_wrong_data)

# Save DataFrames to CSV files
cond1_partial_mistake_aspect_incorrect_df.to_csv(cond1_partial_mistake_aspect_incorrect_file, index=False, encoding="utf-8")
cond1_partial_mistake_opinion_incorrect_df.to_csv(cond1_partial_mistake_opinion_incorrect_file, index=False, encoding="utf-8")
cond1_partial_mistake_polarity_correct_df.to_csv(cond1_partial_mistake_polarity_correct_file, index=False, encoding="utf-8")
cond1_partial_mistake_aspect_correct_df.to_csv(cond1_partial_mistake_aspect_correct_file, index=False, encoding="utf-8")
cond1_partial_mistake_opinion_correct_df.to_csv(cond1_partial_mistake_opinion_correct_file, index=False, encoding="utf-8")
cond1_all_wrong_df.to_csv(cond1_all_wrong_file, index=False, encoding="utf-8")

print("Data has been saved for each category.")

import pandas as pd

# Define file paths for each category
# cond1_all_wrong_file = "cond1_all_wrong.csv"
cond1_partial_mistake_aspect_incorrect_file = "cond1_partial_mistake_aspect_incorrect.csv"
cond1_partial_mistake_opinion_incorrect_file = "cond1_partial_mistake_opinion_incorrect.csv"
# cond1_partial_mistake_polarity_correct_file = "cond1_partial_mistake_polarity_correct.csv"
# cond1_partial_mistake_aspect_correct_file = "cond1_partial_mistake_aspect_correct.csv"
# cond1_partial_mistake_opinion_correct_file = "cond1_partial_mistake_opinion_correct.csv"
# cond1_partial_mistake_file = "cond1_partial_mistake.csv"

# Load CSV files for each category
# cond1_all_wrong_df = pd.read_csv(cond1_all_wrong_file)
cond1_partial_mistake_aspect_incorrect_df = pd.read_csv(cond1_partial_mistake_aspect_incorrect_file)
cond1_partial_mistake_opinion_incorrect_df = pd.read_csv(cond1_partial_mistake_opinion_incorrect_file)
# cond1_partial_mistake_polarity_correct_df = pd.read_csv(cond1_partial_mistake_polarity_correct_file)
# cond1_partial_mistake_aspect_correct_df = pd.read_csv(cond1_partial_mistake_aspect_correct_file)
# cond1_partial_mistake_opinion_correct_df = pd.read_csv(cond1_partial_mistake_opinion_correct_file)
# cond1_partial_mistake_df = pd.read_csv(cond1_partial_mistake_file)


# Merge or concat the DataFrames
merged_df = pd.concat([
    # cond1_all_wrong_df,
    cond1_partial_mistake_aspect_incorrect_df,
    cond1_partial_mistake_opinion_incorrect_df,
    # cond1_partial_mistake_polarity_correct_df,
    # cond1_partial_mistake_aspect_correct_df,
    # cond1_partial_mistake_opinion_correct_df
    # cond1_partial_mistake_df
], ignore_index=True)

# Check for duplicate sentences across categories
duplicate_sentences = merged_df[merged_df.duplicated(subset=['sentence'])]

# Print or save duplicate sentences
print("Duplicate Sentences across Categories:")
print(duplicate_sentences['sentence'])

# Save to CSV if needed
duplicate_sentences.to_csv("duplicate_sentences_across_categories.csv", index=False, encoding="utf-8")

# prompt: count diatas dan totalnya bandingkan dengan cond1_partial_mistake

import pandas as pd
# Count the number of entries in each category
num_aspect_incorrect = len(cond1_partial_mistake_aspect_incorrect_df)
num_opinion_incorrect = len(cond1_partial_mistake_opinion_incorrect_df)
num_polarity_correct = len(cond1_partial_mistake_polarity_correct_df)
num_aspect_correct = len(cond1_partial_mistake_aspect_correct_df)
num_opinion_correct = len(cond1_partial_mistake_opinion_correct_df)
num_all_wrong = len(cond1_all_wrong_df)

# Calculate the total
total_categories = num_aspect_incorrect + num_opinion_incorrect + num_polarity_correct + num_aspect_correct + num_opinion_correct + num_all_wrong

# Load the original cond1_partial_mistake data
cond1_partial_mistake_df = pd.read_csv(cond1_partial_mistake_file)
total_cond1_partial_mistake = len(cond1_partial_mistake_df)

# Print the results
print("Count for each category:")
print(f"Aspect Incorrect: {num_aspect_incorrect}")
print(f"Opinion Incorrect: {num_opinion_incorrect}")
print(f"Polarity Correct: {num_polarity_correct}")
print(f"Aspect Correct: {num_aspect_correct}")
print(f"Opinion Correct: {num_opinion_correct}")
print(f"All Wrong: {num_all_wrong}")
print(f"Total from Categories: {total_categories}")
print(f"Total from cond1_partial_mistake.csv: {total_cond1_partial_mistake}")

# Check if the total matches the original count
if total_categories == total_cond1_partial_mistake:
    print("The total count from categories matches the count in cond1_partial_mistake.csv.")
else:
    print("The total count from categories does not match the count in cond1_partial_mistake.csv. There might be overlapping entries.")

import pandas as pd
import json
import re

# File paths for saving results
cond2_pred_more_some_match_aspect_incorrect_file = "cond2_pred_more_some_match_aspect_incorrect.csv"
cond2_pred_more_some_match_opinion_incorrect_file = "cond2_pred_more_some_match_opinion_incorrect.csv"
cond2_pred_more_some_match_polarity_correct_file = "cond2_pred_more_some_match_polarity_correct.csv"
cond2_pred_more_some_match_polarity_incorrect_file = "cond2_pred_more_some_match_polarity_incorrect.csv"
cond2_pred_more_some_match_aspect_correct_file = "cond2_pred_more_some_match_aspect_correct.csv"
cond2_pred_more_some_match_opinion_correct_file = "cond2_pred_more_some_match_opinion_correct.csv"
cond2_all_wrong_file = "cond2_all_wrong.csv"

# Function to convert JSON string to list of dictionaries
def json_to_list(json_str):
    if pd.isna(json_str) or json_str == '[]':
        return []
    try:
        # Replace single quotes with double quotes
        json_str = json_str.replace("'", '"')
        # Fix common JSON errors
        json_str = re.sub(r',\s*}', '}', json_str)  # trailing commas before }
        json_str = re.sub(r',\s*]', ']', json_str)  # trailing commas before ]
        json_str = re.sub(r'(\w+):', r'"\1":', json_str)  # missing quotes for keys
        json_str = re.sub(r'""', r'"', json_str)  # remove double double quotes
        json_str = re.sub(r'"{', '{', json_str)  # remove quotes before {
        json_str = re.sub(r'}"', '}', json_str)  # remove quotes after }
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {json_str}\nError: {e}")
        raise

# Load data from cond2_pred_more_some_match.csv
cond2_pred_more_some_match_file = "cond2_pred_more_some_match.csv"
cond2_pred_more_some_match_df = pd.read_csv(cond2_pred_more_some_match_file)

# Initialize lists to store data for each category
cond2_pred_more_some_match_aspect_incorrect_data = []
cond2_pred_more_some_match_opinion_incorrect_data = []
cond2_pred_more_some_match_polarity_correct_data = []
cond2_pred_more_some_match_polarity_incorrect_data = []
cond2_pred_more_some_match_aspect_correct_data = []
cond2_pred_more_some_match_opinion_correct_data = []
cond2_all_wrong_data = []

# Process each entry in the DataFrame
for index, row in cond2_pred_more_some_match_df.iterrows():
    triplets = json_to_list(row['Triplets'])
    true_triplets = json_to_list(row['True Triplets'])

    aspect_incorrect = False
    opinion_incorrect = False
    polarity_correct = False
    polarity_incorrect = False
    aspect_correct = False
    opinion_correct = False
    all_wrong = True

    # Check if there's only one triplet
    if len(triplets) == 1:
        t = triplets[0]
        true_triplet = true_triplets[0]

        if t['Aspect'] != true_triplet['Aspect'] and t['Opinion'] == true_triplet['Opinion'] and t['Polarity'] == true_triplet['Polarity']:
            aspect_incorrect = True
            all_wrong = False
        elif t['Opinion'] != true_triplet['Opinion'] and t['Aspect'] == true_triplet['Aspect'] and t['Polarity'] == true_triplet['Polarity']:
            opinion_incorrect = True
            all_wrong = False
        elif t['Opinion'] != true_triplet['Opinion'] and t['Aspect'] == true_triplet['Aspect'] and t['Polarity'] != true_triplet['Polarity']:
            aspect_correct = True
            all_wrong = False
        elif t['Opinion'] == true_triplet['Opinion'] and t['Aspect'] != true_triplet['Aspect'] and t['Polarity'] != true_triplet['Polarity']:
            opinion_correct = True
            all_wrong = False
        elif t['Opinion'] != true_triplet['Opinion'] and t['Aspect'] != true_triplet['Aspect'] and t['Polarity'] == true_triplet['Polarity']:
            polarity_correct = True
            all_wrong = False
        elif t['Opinion'] == true_triplet['Opinion'] and t['Aspect'] == true_triplet['Aspect'] and t['Polarity'] != true_triplet['Polarity']:
            polarity_incorrect = True
            all_wrong = False

    # Check if there are multiple triplets (handled as before)
    else:
        for t in triplets:
            t_tuple = (t['Aspect'], t['Opinion'])
            true_tuple_set = {(tt['Aspect'], tt['Opinion']) for tt in true_triplets}

            if not (aspect_incorrect or opinion_incorrect or polarity_correct or aspect_correct or opinion_correct):
                if any(t['Aspect'] != tt['Aspect'] and t['Opinion'] == tt['Opinion'] and t['Polarity'] == tt['Polarity'] for tt in true_triplets):
                    aspect_incorrect = True
                    all_wrong = False
                elif any(t['Opinion'] != tt['Opinion'] and t['Aspect'] == tt['Aspect'] and t['Polarity'] == tt['Polarity'] for tt in true_triplets):
                    opinion_incorrect = True
                    all_wrong = False
                elif any(t['Opinion'] != tt['Opinion'] and t['Aspect'] == tt['Aspect'] and t['Polarity'] != tt['Polarity'] for tt in true_triplets):
                    aspect_correct = True
                    all_wrong = False
                elif any(t['Opinion'] == tt['Opinion'] and t['Aspect'] != tt['Aspect'] and t['Polarity'] != tt['Polarity'] for tt in true_triplets):
                    opinion_correct = True
                    all_wrong = False
                elif any(t['Opinion'] != tt['Opinion'] and t['Aspect'] != tt['Aspect'] and t['Polarity'] == tt['Polarity'] for tt in true_triplets):
                    polarity_correct = True
                    all_wrong = False
                elif any(t['Opinion'] == tt['Opinion'] and t['Aspect'] == tt['Aspect'] and t['Polarity'] != tt['Polarity'] for tt in true_triplets):
                    polarity_incorrect = True
                    all_wrong = False

    # Append data to respective lists
    if aspect_incorrect and not (opinion_incorrect or polarity_correct or aspect_correct or opinion_correct or polarity_incorrect):
        cond2_pred_more_some_match_aspect_incorrect_data.append(row.to_dict())
    elif opinion_incorrect and not (aspect_incorrect or polarity_correct or aspect_correct or opinion_correct or polarity_incorrect):
        cond2_pred_more_some_match_opinion_incorrect_data.append(row.to_dict())
    elif polarity_correct and not (aspect_incorrect or opinion_incorrect or aspect_correct or opinion_correct or polarity_incorrect):
        cond2_pred_more_some_match_polarity_correct_data.append(row.to_dict())
    elif aspect_correct and not (aspect_incorrect or opinion_incorrect or polarity_correct or opinion_correct or polarity_incorrect):
        cond2_pred_more_some_match_aspect_correct_data.append(row.to_dict())
    elif opinion_correct and not (aspect_incorrect or opinion_incorrect or polarity_correct or aspect_correct or polarity_incorrect):
        cond2_pred_more_some_match_opinion_correct_data.append(row.to_dict())
    elif polarity_incorrect and not (aspect_incorrect or opinion_incorrect or polarity_correct or aspect_correct or opinion_correct):
        cond2_pred_more_some_match_polarity_incorrect_data.append(row.to_dict())
    elif all_wrong and not (aspect_incorrect or opinion_incorrect or polarity_correct or aspect_correct or opinion_correct or polarity_incorrect):
        cond2_all_wrong_data.append(row.to_dict())

# Convert lists to DataFrames
cond2_pred_more_some_match_aspect_incorrect_df = pd.DataFrame(cond2_pred_more_some_match_aspect_incorrect_data)
cond2_pred_more_some_match_opinion_incorrect_df = pd.DataFrame(cond2_pred_more_some_match_opinion_incorrect_data)
cond2_pred_more_some_match_polarity_correct_df = pd.DataFrame(cond2_pred_more_some_match_polarity_correct_data)
cond2_pred_more_some_match_aspect_correct_df = pd.DataFrame(cond2_pred_more_some_match_aspect_correct_data)
cond2_pred_more_some_match_opinion_correct_df = pd.DataFrame(cond2_pred_more_some_match_opinion_correct_data)
cond2_pred_more_some_match_polarity_incorrect_df = pd.DataFrame(cond2_pred_more_some_match_polarity_incorrect_data)
cond2_all_wrong_df = pd.DataFrame(cond2_all_wrong_data)

# Save DataFrames to CSV files
cond2_pred_more_some_match_aspect_incorrect_df.to_csv(cond2_pred_more_some_match_aspect_incorrect_file, index=False, encoding="utf-8")
cond2_pred_more_some_match_opinion_incorrect_df.to_csv(cond2_pred_more_some_match_opinion_incorrect_file, index=False, encoding="utf-8")
cond2_pred_more_some_match_polarity_correct_df.to_csv(cond2_pred_more_some_match_polarity_correct_file, index=False, encoding="utf-8")
cond2_pred_more_some_match_aspect_correct_df.to_csv(cond2_pred_more_some_match_aspect_correct_file, index=False, encoding="utf-8")
cond2_pred_more_some_match_opinion_correct_df.to_csv(cond2_pred_more_some_match_opinion_correct_file, index=False, encoding="utf-8")
cond2_pred_more_some_match_polarity_incorrect_df.to_csv(cond2_pred_more_some_match_polarity_incorrect_file, index=False, encoding="utf-8")
cond2_all_wrong_df.to_csv(cond2_all_wrong_file, index=False, encoding="utf-8")

print("Data has been saved for each category.")

import pandas as pd
import json
import re

# File paths for saving results
cond2_pred_no_match_aspect_incorrect_file = "cond2_pred_no_match_aspect_incorrect.csv"
cond2_pred_no_match_opinion_incorrect_file = "cond2_pred_no_match_opinion_incorrect.csv"
cond2_pred_no_match_polarity_correct_file = "cond2_pred_no_match_polarity_correct.csv"
cond2_pred_no_match_aspect_correct_file = "cond2_pred_no_match_aspect_correct.csv"
cond2_pred_no_match_opinion_correct_file = "cond2_pred_no_match_opinion_correct.csv"
cond2_all_wrong_file = "cond2_all_wrong.csv"
cond2_pred_no_match_polarity_incorrect_file = "cond2_pred_no_match_polarity_incorrect.csv"

# Function to convert JSON string to list of dictionaries
def json_to_list(json_str):
    if pd.isna(json_str) or json_str == '[]':
        return []
    try:
        # Replace single quotes with double quotes
        json_str = json_str.replace("'", '"')
        # Fix common JSON errors
        json_str = re.sub(r',\s*}', '}', json_str)  # trailing commas before }
        json_str = re.sub(r',\s*]', ']', json_str)  # trailing commas before ]
        json_str = re.sub(r'(\w+):', r'"\1":', json_str)  # missing quotes for keys
        json_str = re.sub(r'""', r'"', json_str)  # remove double double quotes
        json_str = re.sub(r'"{', '{', json_str)  # remove quotes before {
        json_str = re.sub(r'}"', '}', json_str)  # remove quotes after }
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {json_str}\nError: {e}")
        raise

# Load data from cond2_pred_no_match.csv
cond2_pred_no_match_file = "cond2_pred_no_match.csv"
cond2_pred_no_match_df = pd.read_csv(cond2_pred_no_match_file)

# Initialize lists to store data for each category
cond2_pred_no_match_aspect_incorrect_data = []
cond2_pred_no_match_opinion_incorrect_data = []
cond2_pred_no_match_polarity_correct_data = []
cond2_pred_no_match_aspect_correct_data = []
cond2_pred_no_match_opinion_correct_data = []
cond2_all_wrong_data = []
cond2_pred_no_match_polarity_incorrect_data = []

# Process each entry in the DataFrame
for index, row in cond2_pred_no_match_df.iterrows():
    triplets = json_to_list(row['Triplets'])
    true_triplets = json_to_list(row['True Triplets'])

    aspect_incorrect = False
    opinion_incorrect = False
    polarity_correct = False
    aspect_correct = False
    opinion_correct = False
    all_wrong = True
    polarity_incorrect = False

    # Check if there's only one triplet
    if len(triplets) == 1:
        t = triplets[0]
        true_triplet = true_triplets[0]

        if t['Aspect'] != true_triplet['Aspect'] and t['Opinion'] == true_triplet['Opinion'] and t['Polarity'] == true_triplet['Polarity']:
            aspect_incorrect = True
            all_wrong = False
        elif t['Opinion'] != true_triplet['Opinion'] and t['Aspect'] == true_triplet['Aspect'] and t['Polarity'] == true_triplet['Polarity']:
            opinion_incorrect = True
            all_wrong = False
        elif t['Opinion'] != true_triplet['Opinion'] and t['Aspect'] == true_triplet['Aspect'] and t['Polarity'] != true_triplet['Polarity']:
            aspect_correct = True
            all_wrong = False
        elif t['Opinion'] == true_triplet['Opinion'] and t['Aspect'] != true_triplet['Aspect'] and t['Polarity'] != true_triplet['Polarity']:
            opinion_correct = True
            all_wrong = False
        elif t['Polarity'] == true_triplet['Polarity']:
            polarity_correct = True
            all_wrong = False
        elif t['Opinion'] == true_triplet['Opinion'] and t['Aspect'] == true_triplet['Aspect'] and t['Polarity'] != true_triplet['Polarity']:
            polarity_incorrect = True
            all_wrong = False

    # Check if there are multiple triplets (handled as before)
    else:
        for t in triplets:
            t_tuple = (t['Aspect'], t['Opinion'])
            true_tuple_set = {(tt['Aspect'], tt['Opinion']) for tt in true_triplets}

            if not (aspect_incorrect or opinion_incorrect or polarity_correct or aspect_correct or opinion_correct or polarity_incorrect):
                if any(t['Aspect'] != tt['Aspect'] and t['Opinion'] == tt['Opinion'] and t['Polarity'] == tt['Polarity'] for tt in true_triplets):
                    aspect_incorrect = True
                    all_wrong = False
                elif any(t['Opinion'] != tt['Opinion'] and t['Aspect'] == tt['Aspect'] and t['Polarity'] == tt['Polarity'] for tt in true_triplets):
                    opinion_incorrect = True
                    all_wrong = False
                elif t_tuple in true_tuple_set and t['Polarity'] != next((tt['Polarity'] for tt in true_triplets if (tt['Aspect'], tt['Opinion']) == t_tuple), None):
                    polarity_incorrect = True
                    all_wrong = False
                elif any(t['Opinion'] != tt['Opinion'] and t['Aspect'] == tt['Aspect'] and t['Polarity'] != tt['Polarity'] for tt in true_triplets):
                    aspect_correct = True
                    all_wrong = False
                elif any(t['Opinion'] == tt['Opinion'] and t['Aspect'] != tt['Aspect'] and t['Polarity'] != tt['Polarity'] for tt in true_triplets):
                    opinion_correct = True
                    all_wrong = False
                elif t_tuple in true_tuple_set and t['Polarity'] == next((tt['Polarity'] for tt in true_triplets if (tt['Aspect'], tt['Opinion']) != t_tuple), None):
                    polarity_correct = True
                    all_wrong = False


    # Append data to respective lists
    if aspect_incorrect and not (opinion_incorrect or polarity_correct or aspect_correct or opinion_correct or polarity_incorrect):
        cond2_pred_no_match_aspect_incorrect_data.append(row.to_dict())
    elif opinion_incorrect and not (aspect_incorrect or polarity_correct or aspect_correct or opinion_correct or polarity_incorrect):
        cond2_pred_no_match_opinion_incorrect_data.append(row.to_dict())
    elif polarity_incorrect and not (aspect_incorrect or opinion_incorrect or polarity_correct or aspect_correct or opinion_correct):
        cond2_pred_no_match_polarity_incorrect_data.append(row.to_dict())
    elif polarity_correct and not (aspect_incorrect or opinion_incorrect or aspect_correct or opinion_correct or polarity_incorrect):
        cond2_pred_no_match_polarity_correct_data.append(row.to_dict())
    elif aspect_correct and not (aspect_incorrect or opinion_incorrect or polarity_correct or opinion_correct or polarity_incorrect):
        cond2_pred_no_match_aspect_correct_data.append(row.to_dict())
    elif opinion_correct and not (aspect_incorrect or opinion_incorrect or polarity_correct or aspect_correct or polarity_incorrect):
        cond2_pred_no_match_opinion_correct_data.append(row.to_dict())
    elif all_wrong and not (aspect_incorrect or opinion_incorrect or polarity_correct or aspect_correct or opinion_correct or polarity_incorrect):
        cond2_all_wrong_data.append(row.to_dict())

# Convert lists to DataFrames
cond2_pred_no_match_aspect_incorrect_df = pd.DataFrame(cond2_pred_no_match_aspect_incorrect_data)
cond2_pred_no_match_opinion_incorrect_df = pd.DataFrame(cond2_pred_no_match_opinion_incorrect_data)
cond2_pred_no_match_polarity_correct_df = pd.DataFrame(cond2_pred_no_match_polarity_correct_data)
cond2_pred_no_match_aspect_correct_df = pd.DataFrame(cond2_pred_no_match_aspect_correct_data)
cond2_pred_no_match_opinion_correct_df = pd.DataFrame(cond2_pred_no_match_opinion_correct_data)
cond2_pred_no_match_polarity_incorrect_df = pd.DataFrame(cond2_pred_no_match_polarity_incorrect_data)
cond2_all_wrong_df = pd.DataFrame(cond2_all_wrong_data)

# Save DataFrames to CSV files
cond2_pred_no_match_aspect_incorrect_df.to_csv(cond2_pred_no_match_aspect_incorrect_file, index=False, encoding="utf-8")
cond2_pred_no_match_opinion_incorrect_df.to_csv(cond2_pred_no_match_opinion_incorrect_file, index=False, encoding="utf-8")
cond2_pred_no_match_polarity_correct_df.to_csv(cond2_pred_no_match_polarity_correct_file, index=False, encoding="utf-8")
cond2_pred_no_match_aspect_correct_df.to_csv(cond2_pred_no_match_aspect_correct_file, index=False, encoding="utf-8")
cond2_pred_no_match_opinion_correct_df.to_csv(cond2_pred_no_match_opinion_correct_file, index=False, encoding="utf-8")
cond2_pred_no_match_polarity_incorrect_df.to_csv(cond2_pred_no_match_polarity_incorrect_file, index=False, encoding="utf-8")
cond2_all_wrong_df.to_csv(cond2_all_wrong_file, index=False, encoding="utf-8")

print("Data has been saved for each category.")

# prompt: count hasil diatas dan totalnya kemudian bandingkan dengan cond2_more_some_match dan cond2_no_match

import pandas as pd

# Count the number of entries in each category for cond2_pred_more_some_match
num_aspect_incorrect_some_match = len(pd.read_csv("cond2_pred_more_some_match_aspect_incorrect.csv"))
num_opinion_incorrect_some_match = len(pd.read_csv("cond2_pred_more_some_match_opinion_incorrect.csv"))
num_polarity_correct_some_match = len(pd.read_csv("cond2_pred_more_some_match_polarity_correct.csv"))
num_aspect_correct_some_match = len(pd.read_csv("cond2_pred_more_some_match_aspect_correct.csv"))
# num_opinion_correct_some_match = len(pd.read_csv("cond2_pred_more_some_match_opinion_correct.csv"))
# num_all_wrong_some_match = len(pd.read_csv("cond2_all_wrong.csv"))
# num_polarity_incorrect_some_match = len(pd.read_csv("cond2_pred_more_some_match_polarity_incorrect.csv"))

# Calculate the total for cond2_pred_more_some_match
total_categories_some_match = num_aspect_incorrect_some_match + num_opinion_incorrect_some_match + num_polarity_correct_some_match + \
                               num_aspect_correct_some_match #+ num_polarity_incorrect_some_match #+ num_opinion_correct_some_match + num_all_wrong_some_match

# Load the original cond2_pred_more_some_match data
cond2_pred_more_some_match_df = pd.read_csv("cond2_pred_more_some_match.csv")
total_cond2_pred_more_some_match = len(cond2_pred_more_some_match_df)

# Print the results for cond2_pred_more_some_match
print("\nCount for each category in cond2_pred_more_some_match:")
print(f"Aspect Incorrect: {num_aspect_incorrect_some_match}")
print(f"Opinion Incorrect: {num_opinion_incorrect_some_match}")
print(f"Polarity Correct: {num_polarity_correct_some_match}")
print(f"Aspect Correct: {num_aspect_correct_some_match}")
# print(f"Opinion Correct: {num_opinion_correct_some_match}")
# print(f"All Wrong: {num_all_wrong_some_match}")
print(f"Total from Categories: {total_categories_some_match}")
print(f"Total from cond2_pred_more_some_match.csv: {total_cond2_pred_more_some_match}")

# Check if the total matches the original count for cond2_pred_more_some_match
if total_categories_some_match == total_cond2_pred_more_some_match:
    print("The total count from categories matches the count in cond2_pred_more_some_match.csv.")
else:
    print("The total count from categories does not match the count in cond2_pred_more_some_match.csv. There might be overlapping entries.")

# Count the number of entries in each category for cond2_pred_more_no_match
num_aspect_incorrect_no_match = len(pd.read_csv("cond2_pred_no_match_aspect_incorrect.csv"))
num_opinion_incorrect_no_match = len(pd.read_csv("cond2_pred_no_match_opinion_incorrect.csv"))
num_polarity_correct_no_match = len(pd.read_csv("cond2_pred_no_match_polarity_correct.csv"))
num_aspect_correct_no_match = len(pd.read_csv("cond2_pred_no_match_aspect_correct.csv"))
num_opinion_correct_no_match = len(pd.read_csv("cond2_pred_no_match_opinion_correct.csv"))
num_all_wrong_no_match = len(pd.read_csv("cond2_all_wrong.csv"))  # Assuming you use the same 'all wrong' file
# num_polarity_incorrect_no_match = len(pd.read_csv("cond2_pred_no_match_polarity_incorrect.csv"))

# Calculate the total for cond2_pred_more_no_match
total_categories_no_match = num_aspect_incorrect_no_match + num_opinion_incorrect_no_match + num_polarity_correct_no_match + \
                             num_aspect_correct_no_match + num_opinion_correct_no_match + num_all_wrong_no_match #+ num_polarity_incorrect_no_match

# Load the original cond2_pred_more_no_match data
cond2_pred_no_match_df = pd.read_csv("cond2_pred_no_match.csv")
total_cond2_pred_no_match = len(cond2_pred_no_match_df)

# Print the results for cond2_pred_more_no_match
print("\nCount for each category in cond2_pred_more_no_match:")
print(f"Aspect Incorrect: {num_aspect_incorrect_no_match}")
print(f"Opinion Incorrect: {num_opinion_incorrect_no_match}")
print(f"Polarity Correct: {num_polarity_correct_no_match}")
print(f"Aspect Correct: {num_aspect_correct_no_match}")
print(f"Opinion Correct: {num_opinion_correct_no_match}")
# print(f"Polarity Incorrect: {num_polarity_incorrect_no_match}")
print(f"All Wrong: {num_all_wrong_no_match}")
print(f"Total from Categories: {total_categories_no_match}")
print(f"Total from cond2_pred_no_match.csv: {total_cond2_pred_no_match}")

# Check if the total matches the original count for cond2_pred_more_no_match
if total_categories_no_match == total_cond2_pred_no_match:
    print("The total count from categories matches the count in cond2_pred_no_match.csv.")
else:
    print("The total count from categories does not match the count in cond2_pred_no_match.csv. There might be overlapping entries.")

# prompt: zip all content except dataset (5).txt, except result.json, except updated_results.json except sample data folder with file in there

!zip -r my_archive.zip * -x "test.txt" -x "resulttest.json" -x "sample_data/*"

"""hitung data pred keseluruhan"""

# prompt: count updated_results.json as "jumlah data yang diprediksi", count error.csv as "hasil prediksi error", count cant_predict.csv as "model tidak dapat memprediksi hasil", count mispredict.csv as "model salah memprediksi hasil", and count matched.csv as "model memprediksi benar"

import pandas as pd

# Count entries in updated_results.json
with open('updated_results_test.json', 'r') as f:
    updated_results_count = len(json.load(f))

# Count rows in CSV files
error_count = len(pd.read_csv('error.csv'))
cant_predict_count = len(pd.read_csv('cant_predict.csv'))
mispredict_count = len(pd.read_csv('mispredict.csv'))
matched_count = len(pd.read_csv('matched.csv'))

# Print the counts with labels
print("Jumlah data yang diprediksi:", updated_results_count)
print("Hasil prediksi error:", error_count)
print("Model tidak dapat memprediksi hasil:", cant_predict_count)
print("Model salah memprediksi hasil:", mispredict_count)
print("Model memprediksi benar:", matched_count)

# prompt: buat prosentase dari code diatasnya buat plotnya gunakan legend dan hapus  labels namun menampilkan angkanya  font perbesar warna putih

import pandas as pd
import matplotlib.pyplot as plt
# Count entries in updated_results.json
with open('updated_results_test.json', 'r') as f:
    updated_results_count = len(json.load(f))

# Count rows in CSV files
error_count = len(pd.read_csv('error.csv'))
cant_predict_count = len(pd.read_csv('cant_predict.csv'))
mispredict_count = len(pd.read_csv('mispredict.csv'))
matched_count = len(pd.read_csv('matched.csv'))

# Calculate percentages
total_count = updated_results_count
error_percent = (error_count / total_count) * 100
cant_predict_percent = (cant_predict_count / total_count) * 100
mispredict_percent = (mispredict_count / total_count) * 100
matched_percent = (matched_count / total_count) * 100

# Data for the pie chart
labels = ['Error', 'Cannot Predict', 'Mispredict', 'Matched']
sizes = [error_percent, cant_predict_percent, mispredict_percent, matched_percent]
colors = ['red', 'orange', 'yellow', 'green']

# Create the pie chart
fig, ax = plt.subplots(figsize=(8, 5))
wedges, texts, autotexts = ax.pie(sizes, colors=colors, autopct='%1.1f%%', startangle=140, textprops={ 'fontsize': 11, 'weight': 'bold'})

# Add legend
ax.legend(wedges, labels, title="Prediction Results", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Set title
ax.set_title("Prediction Results Percentage", fontsize=16)

# Equal aspect ratio ensures that pie is drawn as a circle.
ax.axis('equal')

plt.tight_layout()
plt.show()

# prompt: buat plot bar dari code di atas

import ast
import csv
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np
import requests
import json
import random
import os
from collections import defaultdict
import pandas as pd
from wordcloud import WordCloud
import re
import string
from PIL import Image
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# ... (Your existing code) ...

# Count entries in updated_results.json
with open('updated_results_test.json', 'r') as f:
    updated_results_count = len(json.load(f))

# Count rows in CSV files
error_count = len(pd.read_csv('error.csv'))
cant_predict_count = len(pd.read_csv('cant_predict.csv'))
mispredict_count = len(pd.read_csv('mispredict.csv'))
matched_count = len(pd.read_csv('matched.csv'))

# Data for the bar chart
categories = ['Error', 'Cannot Predict', 'Mispredict', 'Matched']
counts = [error_count, cant_predict_count, mispredict_count, matched_count]

# Create the bar chart
plt.figure(figsize=(10, 6))
bars = plt.barh(categories, counts, color=['red', 'orange', 'yellow', 'green'])
# plt.ylabel('Prediction Results', fontsize=12)
# plt.xlabel('Number of Entries', fontsize=12)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.title('Distribution of Prediction Results', fontsize=14)

# Add count labels on top of each bar
# for i, count in enumerate(counts):
#     plt.text(i, count + 5, str(count), ha='center', va='bottom', fontsize=10)

for bar in bars:
    width = bar.get_width()
    plt.text(width + 1, bar.get_y() + bar.get_height() / 2, str(width), ha='center', va='bottom', rotation=-90, fontsize=14)

plt.tight_layout()
plt.show()

"""hitung data pred pada mismatch"""

# prompt: count cond1_partial_mistake.csv as "jumlah sama a/o salahi", cond1_matched_aspect_opinion.csv as "a&o benar sent salah", cond1_all_wrong.csv as "gagal memprediksi benar", cond2_pred_more_all_match.csv as "lebih semua benar", cond2_pred_more_some_match.csv as "lebih kurang partial benar plus" , cond2_pred_less_some_match as "kurang no plus" and mcond2_pred_no_match.csv as "gagal memprediksi benar"

import pandas as pd
# Count rows in CSV files
cond1_partial_mistake_count = len(pd.read_csv('cond1_partial_mistake.csv'))
cond1_matched_aspect_opinion_count = len(pd.read_csv('cond1_matched_aspect_opinion.csv'))
cond1_all_wrong_count = 0 #len(pd.read_csv('cond1_all_wrong.csv'))
cond2_pred_more_all_match_count = len(pd.read_csv('cond2_pred_more_all_match.csv'))
cond2_pred_more_some_match_count = len(pd.read_csv('cond2_pred_more_some_match.csv'))
cond2_pred_less_some_match_count = len(pd.read_csv('cond2_pred_less_some_match.csv'))
cond2_pred_no_match_count = len(pd.read_csv('cond2_pred_no_match.csv'))

# Print the counts with labels
print("Jumlah sama a/o salah:", cond1_partial_mistake_count)
print("A&O benar, sentimen salah:", cond1_matched_aspect_opinion_count)
print("Gagal memprediksi benar (jumlah sama):", cond1_all_wrong_count)
print("Lebih, semua benar:", cond2_pred_more_all_match_count)
print("Lebih, beberapa benar (+prediksi ekstra):", cond2_pred_more_some_match_count)
print("Kurang, beberapa benar (tidak ada prediksi ekstra):", cond2_pred_less_some_match_count)
print("Gagal memprediksi benar (jumlah beda):", cond2_pred_no_match_count)

total = cond1_partial_mistake_count + cond1_matched_aspect_opinion_count + cond2_pred_more_all_match_count + cond2_pred_more_some_match_count + cond2_pred_less_some_match_count + cond2_pred_no_match_count #+ cond1_all_wrong_count
print("Jumlah mismatch : " + (str(total)))

# prompt: buat prosentase dari code diatasnya buat plotnya gunakan legend dan hapus  labels namun menampilkan angkanya  font perbesar warna plot memiliki saturasi cerah

import pandas as pd
import matplotlib.pyplot as plt
# Count rows in CSV files
cond1_partial_mistake_count = len(pd.read_csv('cond1_partial_mistake.csv'))
cond1_matched_aspect_opinion_count = len(pd.read_csv('cond1_matched_aspect_opinion.csv'))
cond1_all_wrong_count = len(pd.read_csv('cond1_all_wrong.csv'))
cond2_pred_more_all_match_count = len(pd.read_csv('cond2_pred_more_all_match.csv'))
cond2_pred_more_some_match_count = len(pd.read_csv('cond2_pred_more_some_match.csv'))
cond2_pred_less_some_match_count = len(pd.read_csv('cond2_pred_less_some_match.csv'))
cond2_pred_no_match_count = len(pd.read_csv('cond2_pred_no_match.csv'))

# Calculate percentages
total_count = cond1_partial_mistake_count + cond1_matched_aspect_opinion_count + cond2_pred_more_all_match_count + cond2_pred_more_some_match_count + cond2_pred_less_some_match_count + cond2_pred_no_match_count

partial_mistake_percent = (cond1_partial_mistake_count / total_count) * 100
matched_aspect_opinion_percent = (cond1_matched_aspect_opinion_count / total_count) * 100
all_wrong_percent = (cond1_all_wrong_count / total_count) * 100
pred_more_all_match_percent = (cond2_pred_more_all_match_count / total_count) * 100
pred_more_some_match_percent = (cond2_pred_more_some_match_count / total_count) * 100
pred_less_some_match_percent = (cond2_pred_less_some_match_count / total_count) * 100
pred_no_match_percent = (cond2_pred_no_match_count / total_count) * 100

# Data for the pie chart
labels = ['Prediksi Polaritas Salah (Jumlah Sama)',
          'Prediksi Sebagian Triplet Salah (Jumlah Sama)',
          'Prediksi Keseluruhan Triplet Salah (Jumlah Sama)',
          'Aktual Terprediksi Keseluruhan (Jumlah Berbeda)',
          'Aktual Sebagian Terprediksi (Jumlah Berbeda)',
          'Aktual Sebagian Tidak Terprediksi (Jumlah Berbeda)',
          'Aktual Tidak Terprediksi Keseluruhan (Jumlah Berbeda)']

sizes = [matched_aspect_opinion_percent,
         partial_mistake_percent,
         all_wrong_percent,
         pred_more_all_match_percent,
         pred_more_some_match_percent,
         pred_less_some_match_percent,
         pred_no_match_percent]

colors = ['orange', 'yellow', 'green', 'magenta', 'pink', 'red', 'blue']  # Bright and saturated colors

# Create the pie chart
fig, ax = plt.subplots(figsize=(10, 8))
wedges, texts, autotexts = ax.pie(sizes, colors=colors, autopct='%1.1f%%', startangle=90, textprops={'fontsize': 14})

# Add legend
ax.legend(wedges, labels, title="Prediction Results", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Set title
ax.set_title("Prediction Results Percentage (Mismatch)", fontsize=16)

# Remove labels on the chart, but keep the percentage values
plt.setp(autotexts, size=12, weight="bold")  # Increase font size of percentage values
# plt.setp(texts, size=0)  # Remove labels

# Equal aspect ratio ensures that pie is drawn as a circle.
ax.axis('equal')

plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from textwrap import wrap

# Load CSV files and count rows
cond1_partial_mistake_count = len(pd.read_csv('cond1_partial_mistake.csv'))
cond1_matched_aspect_opinion_count = len(pd.read_csv('cond1_matched_aspect_opinion.csv'))
cond1_all_wrong_count = len(pd.read_csv('cond1_all_wrong.csv'))
cond2_pred_more_all_match_count = len(pd.read_csv('cond2_pred_more_all_match.csv'))
cond2_pred_more_some_match_count = len(pd.read_csv('cond2_pred_more_some_match.csv'))
cond2_pred_less_some_match_count = len(pd.read_csv('cond2_pred_less_some_match.csv'))
cond2_pred_no_match_count = len(pd.read_csv('cond2_pred_no_match.csv'))
cond2_all_wrong_count = len(pd.read_csv('cond2_all_wrong.csv'))

# Data for the bar chart
# labels = ['Prediksi Sebagian Triplet Salah (Jumlah Sama)',
#           'Prediksi Keseluruhan Triplet Salah (Jumlah Sama)',
#           'Aktual Terprediksi Keseluruhan (Jumlah Berbeda)',
#           'Aktual Sebagian Terprediksi (Jumlah Berbeda)',
#           'Aktual Sebagian Tidak Terprediksi (Jumlah Berbeda)',
#           'Aktual Tidak Terprediksi Keseluruhan (Jumlah Berbeda)',
#           'Triplet Aktual Tidak Terprediksi Keseluruhan dan Prediksi Keseluruhan Triplet Salah (Jumlah Berbeda)']
labels = ['Aktual Terprediksi Keseluruhan (Jumlah Berbeda)',
          'Aktual Sebagian Terprediksi (Jumlah Berbeda)',
          'Aktual Sebagian Tidak Terprediksi (Jumlah Berbeda)',
          'Aktual Tidak Terprediksi Keseluruhan (Jumlah Berbeda)',
          'Triplet Aktual Tidak Terprediksi Keseluruhan dan Prediksi Keseluruhan Triplet Salah (Jumlah Berbeda)']

# counts = [cond1_matched_aspect_opinion_count + cond1_partial_mistake_count - cond1_all_wrong_count,
#           cond1_all_wrong_count,
#           cond2_pred_more_all_match_count,
#           cond2_pred_more_some_match_count,
#           cond2_pred_less_some_match_count,
#           cond2_pred_no_match_count - cond2_all_wrong_count,
#           cond2_all_wrong_count]
counts = [cond2_pred_more_all_match_count,
          cond2_pred_more_some_match_count,
          cond2_pred_less_some_match_count,
          cond2_pred_no_match_count - cond2_all_wrong_count,
          cond2_all_wrong_count]

colors = ['#276221', '#e69b00', '#52a447', '#e6b400', '#FF0000']  # Bright and saturated colors
# '#e47200', '#FF4433', sama
# Create the bar chart
x = range(len(counts))
fig, ax = plt.subplots(figsize=(12, 8))
bars = ax.bar(x, counts, color=colors)

# Add count labels on the bars
for bar in bars:
    height = bar.get_height()
    ax.annotate(f'{height}',
                xy=(bar.get_x() + bar.get_width() / 2, height),
                xytext=(0, 3),  # 3 points vertical offset
                textcoords="offset points",
                ha='center', va='bottom', fontsize=14, fontweight='bold')

# Remove x and y labels
ax.set_xlabel('')
ax.set_ylabel('')

# Set title
ax.set_title("Distribution Prediction Results (Mismatch)", fontsize=16)

# Wrap text for xtick labels
wrapped_labels = [ '\n'.join(wrap(label, 20)) for label in labels]  # Adjust 15 to the desired wrap length
ax.set_xticks(x)
ax.set_xticklabels(wrapped_labels, ha='center')

# Tight layout to fit everything nicely
plt.tight_layout()
plt.show()
# fig, ax = plt.subplots(figsize=(12, 8))
# bars = ax.barh(labels, counts, color=colors)

# # Add count labels on the bars
# for bar in bars:
#     width = bar.get_width()
#     ax.annotate(f'{width}',
#                 xy=(width, bar.get_y() + bar.get_height() / 2),
#                 xytext=(3, 0),  # 3 points horizontal offset
#                 textcoords="offset points",
#                 ha='left', va='center', fontsize=14, fontweight='bold')

# # Remove x and y labels
# ax.set_xlabel('Count')
# ax.set_ylabel('')

# # Set title
# ax.set_title("Distribution Prediction Results (Mismatch)", fontsize=16)

# # Adjust layout to fit everything nicely
# plt.tight_layout()
# plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from textwrap import wrap

# Load CSV files and count rows
cond1_partial_mistake_count = len(pd.read_csv('cond1_partial_mistake.csv'))
cond1_matched_aspect_opinion_count = len(pd.read_csv('cond1_matched_aspect_opinion.csv'))
cond1_all_wrong_count = len(pd.read_csv('cond1_all_wrong.csv'))
cond2_pred_more_all_match_count = len(pd.read_csv('cond2_pred_more_all_match.csv'))
cond2_pred_more_some_match_count = len(pd.read_csv('cond2_pred_more_some_match.csv'))
cond2_pred_less_some_match_count = len(pd.read_csv('cond2_pred_less_some_match.csv'))
cond2_pred_no_match_count = len(pd.read_csv('cond2_pred_no_match.csv'))
cond2_all_wrong_count = len(pd.read_csv('cond2_all_wrong.csv'))

# Data for the bar chart
labels = ['Aktual Terprediksi Keseluruhan (Jumlah Berbeda)',
          'Aktual Sebagian Terprediksi (Jumlah Berbeda)',
          'Aktual Sebagian Tidak Terprediksi (Jumlah Berbeda)',
          'Aktual Tidak Terprediksi Keseluruhan (Jumlah Berbeda)',
          'Triplet Aktual Tidak Terprediksi Keseluruhan dan Prediksi Keseluruhan Triplet Salah (Jumlah Berbeda)']

counts = [cond2_pred_more_all_match_count,
          cond2_pred_more_some_match_count,
          cond2_pred_less_some_match_count,
          cond2_pred_no_match_count - cond2_all_wrong_count,
          cond2_all_wrong_count]

# Create a DataFrame for sorting
data = pd.DataFrame({
    'Label': labels,
    'Count': counts
})

# Sort the DataFrame by counts in descending order
data_sorted = data.sort_values(by='Count', ascending=False)

# Plotting
fig, ax = plt.subplots(figsize=(12, 8))
bars = ax.bar(data_sorted['Label'], data_sorted['Count'], color=['#276221', '#e69b00', '#e6b400', '#52a447', '#FF0000'])

# Add count labels on the bars
for bar in bars:
    height = bar.get_height()
    ax.annotate(f'{height}',
                xy=(bar.get_x() + bar.get_width() / 2, height),
                xytext=(0, 3),  # 3 points vertical offset
                textcoords="offset points",
                ha='center', va='bottom', fontsize=14, fontweight='bold')

# Remove x and y labels
ax.set_xlabel('')
ax.set_ylabel('')

# Set title
ax.set_title("Distribution Prediction Results (Mismatch)", fontsize=16)

# Wrap text for xtick labels
wrapped_labels = [ '\n'.join(wrap(label, 20)) for label in data_sorted['Label']]  # Adjust 20 to the desired wrap length
ax.set_xticklabels(wrapped_labels, ha='center')

# Tight layout to fit everything nicely
plt.tight_layout()
plt.show()

import pandas as pd

# Load DataFrames from CSV files
matched_df = pd.read_csv("matched.csv")
mispredict_df = pd.read_csv("mispredict.csv")
error_df = pd.read_csv("error.csv")
cant_predict_df = pd.read_csv("cant_predict.csv")

# Calculate counts
TP = len(matched_df)
FP = len(mispredict_df)
FN = len(error_df) + len(cant_predict_df)

# Calculate Precision, Recall, and F1 Score
precision = TP / (TP + FP) if (TP + FP) > 0 else 0
recall = TP / (TP + FN) if (TP + FN) > 0 else 0
f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1_score:.4f}")

import pandas as pd
from sklearn.metrics import confusion_matrix, classification_report

# Load DataFrames from CSV files
matched_df = pd.read_csv("matched.csv")
mispredict_df = pd.read_csv("mispredict.csv")
error_df = pd.read_csv("error.csv")
cant_predict_df = pd.read_csv("cant_predict.csv")

# Define true labels and predictions
y_true = []
y_pred = []

# True positives (matched)
y_true.extend([1] * len(matched_df))
y_pred.extend([1] * len(matched_df))

# False positives (mispredict)
y_true.extend([0] * len(mispredict_df))
y_pred.extend([1] * len(mispredict_df))

# False negatives (error + cant_predict)
y_true.extend([1] * (len(error_df) + len(cant_predict_df)))
y_pred.extend([0] * (len(error_df) + len(cant_predict_df)))

# Calculate confusion matrix
conf_matrix = confusion_matrix(y_true, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Generate classification report
class_report = classification_report(y_true, y_pred, target_names=['Negative', 'Positive'])
print("\nClassification Report:")
print(class_report)

import json
import pandas as pd
from sklearn.metrics import classification_report, f1_score

# Input and output file paths
input_file = "updated_results_test.json"

# Output files for each condition
error_file = "error.csv"
cant_predict_file = "cant_predict.csv"
mispredict_file = "mispredict.csv"
matched_file = "matched.csv"

# Load data from JSON
with open(input_file, "r") as file:
    data = json.load(file)

# Initialize lists to store entries for each condition
error_data = []
cant_predict_data = []
mispredict_data = []
matched_data = []

# Initialize lists to store sentiments for F1 calculation
true_sentiments = []
predicted_sentiments = []

# Filter entries based on conditions
for entry in data:
    if 'Triplets' not in entry or 'True Triplets' not in entry:
        error_data.append(entry)
        continue  # Skip entries without Triplets or True Triplets

    triplets = entry['Triplets']
    true_triplets = entry['True Triplets']

    if not true_triplets:
        error_data.append(entry)  # Both Triplets and True Triplets are empty
    elif not triplets and true_triplets:
        cant_predict_data.append(entry)  # Triplets are empty but True Triplets are not
    # elif triplets and not true_triplets:
    #     error_data.append(entry)  # True Triplets are empty but Triplets are not
    else:
        # If both Triplets and True Triplets are present
        triplet_set = {(t['Aspect'], t['Opinion'], t['Polarity']) for t in triplets}
        true_triplet_set = {(t['Aspect'], t['Opinion'], t['Polarity']) for t in true_triplets}

        if triplet_set == true_triplet_set:
            matched_data.append(entry)  # Triplets match True Triplets
            # Add to sentiments list for F1 calculation
            for t in true_triplets:
                true_sentiments.append(t['Polarity'])
                predicted_sentiments.append(t['Polarity'])
        else:
            mispredict_data.append(entry)  # Triplets do not match True Triplets
            # Ensure matching lengths
            min_len = min(len(true_triplets), len(triplets))
            for i in range(min_len):
                true_sentiments.append(true_triplets[i]['Polarity'])
                predicted_sentiments.append(triplets[i]['Polarity'])

# Convert lists to DataFrames
error_df = pd.DataFrame(error_data)
cant_predict_df = pd.DataFrame(cant_predict_data)
mispredict_df = pd.DataFrame(mispredict_data)
matched_df = pd.DataFrame(matched_data)

# Save DataFrames to CSV files
error_df.to_csv(error_file, index=False, encoding="utf-8")
cant_predict_df.to_csv(cant_predict_file, index=False, encoding="utf-8")
mispredict_df.to_csv(mispredict_file, index=False, encoding="utf-8")
matched_df.to_csv(matched_file, index=False, encoding="utf-8")

print(f"Filtered data has been saved to {error_file}, {cant_predict_file}, {mispredict_file}, and {matched_file}.")

# Calculate and print F1-score
f1 = f1_score(true_sentiments, predicted_sentiments, average='weighted')
print(f"F1-score: {f1:.2f}")

# Print classification report
report = classification_report(true_sentiments, predicted_sentiments)
print("Classification Report:")
print(report)

# # Define labels based on unique sentiment values
# labels = list(set(true_sentiments + predicted_sentiments))

# # Build confusion matrix
# cm = confusion_matrix(true_sentiments, predicted_sentiments, labels=labels)

# # Convert confusion matrix to DataFrame
# cm_df = pd.DataFrame(cm, index=labels, columns=labels)

# # Plot confusion matrix
# plt.figure(figsize=(8, 6))
# sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='d', cbar=False)
# plt.title('Confusion Matrix')
# plt.xlabel('Predicted Sentiment')
# plt.ylabel('Actual Sentiment')
# plt.show()
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Ensure labels are in the desired order: positive, neutral, negative
labels = ['Positive', 'Neutral', 'Negative']

# Build confusion matrix
cm = confusion_matrix(true_sentiments, predicted_sentiments, labels=labels)

# Convert confusion matrix to DataFrame
cm_df = pd.DataFrame(cm, index=labels, columns=labels)

# Plot confusion matrix with customization
plt.figure(figsize=(8, 6))
sns.heatmap(
    cm_df,
    annot=True,  # Annotate cells with counts
    cmap='Blues',  # Colormap for better visualization of imbalanced classes (optional)
    fmt='d',      # Format for integer labels
    cbar=False   # Remove colorbar (optional)
)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Sentiment')
plt.ylabel('Actual Sentiment')
plt.show()

import json
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report, f1_score

# Input and output file paths
input_file = "updated_results_test.json"

# Output files for each condition
error_file = "error.csv"
cant_predict_file = "cant_predict.csv"
mispredict_file = "mispredict.csv"
matched_file = "matched.csv"

# Output files for aspect and opinion analysis
aspect_tp_file = "aspect_tp.csv"
aspect_fp_file = "aspect_fp.csv"
aspect_fn_file = "aspect_fn.csv"

opinion_tp_file = "opinion_tp.csv"
opinion_fp_file = "opinion_fp.csv"
opinion_fn_file = "opinion_fn.csv"

# Load data from JSON
with open(input_file, "r") as file:
    data = json.load(file)

# Initialize lists to store entries for each condition
error_data = []
cant_predict_data = []
mispredict_data = []
matched_data = []

# Initialize lists to store entries for aspect and opinion analysis
aspect_tp_data = []
aspect_fp_data = []
aspect_fn_data = []

opinion_tp_data = []
opinion_fp_data = []
opinion_fn_data = []

# Initialize lists to store sentiments for F1 calculation
true_sentiments = []
predicted_sentiments = []

# Initialize lists for true and predicted aspects and opinions
true_aspects = []
predicted_aspects = []
true_opinions = []
predicted_opinions = []

# Filter entries based on conditions
for entry in data:
    if 'Triplets' not in entry or 'True Triplets' not in entry:
        error_data.append(entry)
        continue  # Skip entries without Triplets or True Triplets

    triplets = entry['Triplets']
    true_triplets = entry['True Triplets']

    if not true_triplets:
        error_data.append(entry)  # Both Triplets and True Triplets are empty
    elif not triplets and true_triplets:
        cant_predict_data.append(entry)  # Triplets are empty but True Triplets are not
    else:
        # If both Triplets and True Triplets are present
        triplet_set = {(t['Aspect'], t['Opinion'], t['Polarity']) for t in triplets}
        true_triplet_set = {(t['Aspect'], t['Opinion'], t['Polarity']) for t in true_triplets}

        if triplet_set == true_triplet_set:
            matched_data.append(entry)  # Triplets match True Triplets
            # Add to sentiments list for F1 calculation
            for t in true_triplets:
                true_sentiments.append(t['Polarity'])
                predicted_sentiments.append(t['Polarity'])
        else:
            mispredict_data.append(entry)  # Triplets do not match True Triplets
            # Ensure matching lengths
            min_len = min(len(true_triplets), len(triplets))
            for i in range(min_len):
                true_sentiments.append(true_triplets[i]['Polarity'])
                predicted_sentiments.append(triplets[i]['Polarity'])

        # Aspect analysis
        for t in true_triplets:
            t_aspect = t['Aspect']
            predicted_triplet = next((pt for pt in triplets if pt['Aspect'] == t_aspect), None)
            if predicted_triplet:
                if predicted_triplet['Aspect'] == t_aspect:
                    aspect_tp_data.append(entry)  # True Positive
                    true_aspects.append(1)
                    predicted_aspects.append(1)
                else:
                    aspect_fp_data.append(entry)  # False Positive
                    true_aspects.append(0)
                    predicted_aspects.append(1)
            else:
                aspect_fn_data.append(entry)  # False Negative
                true_aspects.append(1)
                predicted_aspects.append(0)

        for pt in triplets:
            if not any(tt['Aspect'] == pt['Aspect'] for tt in true_triplets):
                aspect_fp_data.append(entry)  # False Positive
                true_aspects.append(0)
                predicted_aspects.append(1)

        # Opinion analysis
        for t in true_triplets:
            t_opinion = t['Opinion']
            predicted_triplet = next((pt for pt in triplets if pt['Opinion'] == t_opinion), None)
            if predicted_triplet:
                if predicted_triplet['Opinion'] == t_opinion:
                    opinion_tp_data.append(entry)  # True Positive
                    true_opinions.append(1)
                    predicted_opinions.append(1)
                else:
                    opinion_fp_data.append(entry)  # False Positive
                    true_opinions.append(0)
                    predicted_opinions.append(1)
            else:
                opinion_fn_data.append(entry)  # False Negative
                true_opinions.append(1)
                predicted_opinions.append(0)

        for pt in triplets:
            if not any(tt['Opinion'] == pt['Opinion'] for tt in true_triplets):
                opinion_fp_data.append(entry)  # False Positive
                true_opinions.append(0)
                predicted_opinions.append(1)

# Convert lists to DataFrames
error_df = pd.DataFrame(error_data)
cant_predict_df = pd.DataFrame(cant_predict_data)
mispredict_df = pd.DataFrame(mispredict_data)
matched_df = pd.DataFrame(matched_data)

aspect_tp_df = pd.DataFrame(aspect_tp_data)
aspect_fp_df = pd.DataFrame(aspect_fp_data)
aspect_fn_df = pd.DataFrame(aspect_fn_data)

opinion_tp_df = pd.DataFrame(opinion_tp_data)
opinion_fp_df = pd.DataFrame(opinion_fp_data)
opinion_fn_df = pd.DataFrame(opinion_fn_data)

# Save DataFrames to CSV files
error_df.to_csv(error_file, index=False, encoding="utf-8")
cant_predict_df.to_csv(cant_predict_file, index=False, encoding="utf-8")
mispredict_df.to_csv(mispredict_file, index=False, encoding="utf-8")
matched_df.to_csv(matched_file, index=False, encoding="utf-8")

aspect_tp_df.to_csv(aspect_tp_file, index=False, encoding="utf-8")
aspect_fp_df.to_csv(aspect_fp_file, index=False, encoding="utf-8")
aspect_fn_df.to_csv(aspect_fn_file, index=False, encoding="utf-8")

opinion_tp_df.to_csv(opinion_tp_file, index=False, encoding="utf-8")
opinion_fp_df.to_csv(opinion_fp_file, index=False, encoding="utf-8")
opinion_fn_df.to_csv(opinion_fn_file, index=False, encoding="utf-8")

print(f"Filtered data has been saved to {error_file}, {cant_predict_file}, {mispredict_file}, and {matched_file}.")
print(f"Aspect analysis data has been saved to {aspect_tp_file}, {aspect_fp_file}, and {aspect_fn_file}.")
print(f"Opinion analysis data has been saved to {opinion_tp_file}, {opinion_fp_file}, and {opinion_fn_file}.")

# Calculate and print F1-score for aspects
aspect_f1 = f1_score(true_aspects, predicted_aspects, average='weighted')
print(f"Aspect F1-score: {aspect_f1:.4f}")

# Print classification report for aspects
aspect_report = classification_report(true_aspects, predicted_aspects, target_names=['Not Mentioned', 'Mentioned'])
print("Aspect Classification Report:")
print(aspect_report)

# Build and plot confusion matrix for aspects
aspect_cm = confusion_matrix(true_aspects, predicted_aspects, labels=[0, 1])
aspect_cm_df = pd.DataFrame(aspect_cm, index=['Not Mentioned', 'Mentioned'], columns=['Not Mentioned', 'Mentioned'])

plt.figure(figsize=(8, 6))
sns.heatmap(aspect_cm_df, annot=True, cmap='Blues', fmt='d', cbar=False)
plt.title('Aspect Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Calculate and print F1-score for opinions
opinion_f1 = f1_score(true_opinions, predicted_opinions, average='weighted')
print(f"Opinion F1-score: {opinion_f1:.4f}")

# Print classification report for opinions
opinion_report = classification_report(true_opinions, predicted_opinions, target_names=['Not Mentioned', 'Mentioned'])
print("Opinion Classification Report:")
print(opinion_report)

# Build and plot confusion matrix for opinions
opinion_cm = confusion_matrix(true_opinions, predicted_opinions, labels=[0, 1])
opinion_cm_df = pd.DataFrame(opinion_cm, index=['Not Mentioned', 'Mentioned'], columns=['Not Mentioned', 'Mentioned'])

plt.figure(figsize=(8, 6))
sns.heatmap(opinion_cm_df, annot=True, cmap='Blues', fmt='d', cbar=False)
plt.title('Opinion Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""Implement"""

import json
import os

# List of input and output file paths
input_files = ["result_anis.json", "result_ganjar.json", "result_prabowo.json", "result_gibran.json", "result_imin.json", "result_mahfud.json"]
output_files = ["results_filtered_anis.json", "results_filtered_ganjar.json", "results_filtered_prabowo.json", "results_filtered_gibran.json", "results_filtered_imin.json", "results_filtered_mahfud.json"]

# Function to filter and save data
def filter_and_save(input_file, output_file):
    # Load data from JSON
    with open(input_file, "r") as file:
        data = json.load(file)

    # Initialize a list to store filtered entries
    filtered_data = []

    # Filter entries that have errors or missing Triplets/True Triplets
    for entry in data:
        if 'Triplets' not in entry or 'True Triplets' not in entry:
            continue  # Skip entries without Triplets or True Triplets

        triplets = entry['Triplets']
        true_triplets = entry['True Triplets']

        # Check if either Triplets or True Triplets are empty
        if not triplets: # or not true_triplets:
            continue  # Skip entries with empty Triplets or True Triplets

        # If all checks pass, add the entry to filtered_data
        filtered_data.append(entry)

    # Save filtered data back to JSON
    with open(output_file, "w") as file:
        json.dump(filtered_data, file, indent=2)

    print(f"Filtered data from {input_file} has been saved to {output_file}")

# Process each input file and save filtered results
for input_file, output_file in zip(input_files, output_files):
    filter_and_save(input_file, output_file)

# prompt: cara count line tiap file dari banyak file txt seperti pada code dibawahnya ini

# List of file names
file_names = ["pred_anis.txt", "pred_ganjar.txt", "pred_prabowo.txt", "pred_imin.txt", "pred mahfud.txt", "pred gibran.txt"]  # Replace with your actual file names

# Iterate through the file names and count lines
for file_name in file_names:
    try:
        with open(file_name, "r") as file:
            line_count = len(file.readlines())
        print(f"File: {file_name}, Line Count: {line_count}")
    except FileNotFoundError:
        print(f"File not found: {file_name}")

# prompt: cara count tiap file dari banyak file xlsx seperti pada code diatasnya ini

import os
import pandas as pd

# Directory containing the xlsx files
directory = 'raw'  # Replace with the actual directory path

# Iterate through files in the directory
for filename in os.listdir(directory):
    if filename.endswith('.xlsx'):
        filepath = os.path.join(directory, filename)
        try:
            # Read the xlsx file into a pandas DataFrame
            df = pd.read_excel(filepath)
            # Count the number of rows (assuming each row represents an entry)
            count = len(df)
            print(f"File: {filename}, Count: {count}")
        except Exception as e:
            print(f"Error reading file {filename}: {e}")

# prompt: count row json "result_gibran.json", "result_imin.json", "result_mahfud.json" dan buat plotnya label custom

import json
import matplotlib.pyplot as plt

# File paths
file_paths = ["result_anis.json", "result_prabowo.json", "result_ganjar.json", "result_imin.json", "result_gibran.json", "result_mahfud.json"]

# Count rows in each JSON file
row_counts = []
for file_path in file_paths:
  with open(file_path, "r") as file:
    data = json.load(file)
    row_counts.append(len(data))

for i, count in enumerate(row_counts):
  plt.text(i, count, str(count), ha='center', va='bottom')
# Create bar chart
labels = ["Anies Baswedan","Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"]  # Custom labels
plt.bar(labels, row_counts)
plt.xlabel("Candidates Names")
plt.xticks(labels, rotation=45, ha='right')
plt.ylabel("Data Counts")
plt.title("Distribusi Data per Masing Calon")
plt.show()

# prompt: code diatas urutkan dari count terbanyak

import matplotlib.pyplot as plt
# Sort the labels and row_counts based on row_counts in descending order
labels, row_counts = zip(*sorted(zip(labels, row_counts), key=lambda x: x[1], reverse=True))

for i, count in enumerate(row_counts):
  plt.text(i, count, str(count), ha='center', va='bottom')

# cmap = plt.get_cmap('autumn_r')  # Choose a colormap (e.g., 'viridis')

# Create bar chart with sorted data
plt.bar(labels, row_counts)
plt.xlabel("Candidates Names")
plt.xticks(labels, rotation=45, ha='right')
plt.ylabel("Data Counts")
plt.title("Distribusi Data per Masing Calon")
plt.show()

import json
import numpy as np
import matplotlib.pyplot as plt

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]
names = ["Anies Baswedan", "Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"]

# Count rows in each JSON file
row_counts = []
for file_path in file_paths:
    with open(file_path, "r") as file:
        data = json.load(file)
        row_counts.append(len(data))

# Sort counts and corresponding names
sorted_indices = np.argsort(row_counts)[::-1]
sorted_row_counts = np.array(row_counts)[sorted_indices]
sorted_names = np.array(names)[sorted_indices]

# Normalize row counts to range between 0.2 and 1 (ensuring even the smallest counts are not too light)
normalized_counts = (sorted_row_counts - np.min(sorted_row_counts)) / (np.max(sorted_row_counts) - np.min(sorted_row_counts)) * 0.8 + 0.2

# Choose a colormap (e.g., 'viridis')
cmap = plt.get_cmap('autumn_r')

# Create bar chart with color intensity based on normalized counts
bars = plt.bar(sorted_names, sorted_row_counts, color=cmap(normalized_counts))

# Add labels to bars
for i, count in enumerate(sorted_row_counts):
    plt.text(i, count, str(count), ha='center', va='bottom')

plt.xlabel("Candidates Names")
plt.xticks(rotation=45, ha='right')
plt.ylabel("Data Counts")
plt.title("Distribusi Data per Masing Calon")
plt.show()

import json
import matplotlib.pyplot as plt
import numpy as np

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Initialize sentiment counts for each person
sentiment_counts = {
    "Anies Baswedan": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Prabowo Subianto": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Ganjar Pranowo": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Muhaimin Iskandar": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Gibran Rakabuming": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Mahfud MD": {"Positive": 0, "Neutral": 0, "Negative": 0}
}

# Iterate over files and count sentiments
for i, file_path in enumerate(file_paths):
    name = ["Anies Baswedan","Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"][i]
    with open(file_path, "r") as file:
        data = json.load(file)
        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Polarity' in triplet:
                        sentiment = triplet['Polarity']
                        sentiment_counts[name][sentiment] += 1
                    else:
                        print(f"Warning: 'Polarity' key missing in entry for {name}")

# Prepare data for plot
names = list(sentiment_counts.keys())
sentiment_labels = ['Positive', 'Neutral', 'Negative']
counts = np.array([[sentiment_counts[name][label] for label in sentiment_labels] for name in names])

# Create bar chart
fig, ax = plt.subplots()

# Set bar width
bar_width = 0.25

# Set position of bar on X axis
br1 = np.arange(len(names))
br2 = [x + bar_width for x in br1]
br3 = [x + bar_width for x in br2]

# Plot bars
bars1 = ax.bar(br1, counts[:, 0], color='g', width=bar_width, label='Positive')
bars2 = ax.bar(br2, counts[:, 1], color='y', width=bar_width, label='Neutral')
bars3 = ax.bar(br3, counts[:, 2], color='r', width=bar_width, label='Negative')

# Add value labels on top of bars
for bars in [bars1, bars2, bars3]:
    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.05, round(yval, 2), ha='center', va='bottom')

# Adding Xticks
plt.xlabel('Candidates Names')
plt.ylabel('Sentiment Counts')
plt.title('Sentiment Comparison for Individuals')
plt.xticks([r + bar_width for r in range(len(names))], names, rotation=45, ha='right')
plt.legend()

plt.tight_layout()
plt.show()

# prompt: bar stack dari code diatas

import matplotlib.pyplot as plt
# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Initialize sentiment counts for each person
sentiment_counts = {
    "Anies Baswedan": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Prabowo Subianto": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Ganjar Pranowo": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Muhaimin Iskandar": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Gibran Rakabuming": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Mahfud MD": {"Positive": 0, "Neutral": 0, "Negative": 0}
}

# Iterate over files and count sentiments
for i, file_path in enumerate(file_paths):
    name = ["Anies Baswedan","Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"][i]
    with open(file_path, "r") as file:
        data = json.load(file)
        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Polarity' in triplet:
                        sentiment = triplet['Polarity']
                        sentiment_counts[name][sentiment] += 1
                    else:
                        print(f"Warning: 'Polarity' key missing in entry for {name}")

# Prepare data for plot
names = list(sentiment_counts.keys())
positive_counts = [sentiment_counts[name]['Positive'] for name in names]
neutral_counts = [sentiment_counts[name]['Neutral'] for name in names]
negative_counts = [sentiment_counts[name]['Negative'] for name in names]

# Create stacked bar chart
fig, ax = plt.subplots()

ax.bar(names, positive_counts, label='Positive', color='g')
ax.bar(names, neutral_counts, bottom=positive_counts, label='Neutral', color='y')
ax.bar(names, negative_counts, bottom=[i+j for i,j in zip(positive_counts, neutral_counts)], label='Negative', color='r')

# Add value labels on top of bars
for i, name in enumerate(names):
    total_count = positive_counts[i] + neutral_counts[i] + negative_counts[i]
    ax.text(i, total_count + 0.05, str(total_count), ha='center', va='bottom')

plt.xlabel('Candidates Names')
plt.ylabel('Sentiment Counts')
plt.title('Sentiment Comparison for Individuals (Stacked)')
plt.xticks(rotation=45, ha='right')
plt.legend()

plt.tight_layout()
plt.show()

# prompt: dari code diatas buat plot pie berdasarkan prosentase sentimen

import matplotlib.pyplot as plt

# ... (Your existing code) ...

# Calculate percentages for each sentiment
total_counts = {name: sum(counts[i]) for i, name in enumerate(names)}
sentiment_percentages = {
    name: {label: count / total_counts[name] * 100 for label, count in sentiment_counts[name].items()}
    for name in names
}

# Create pie charts
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for i, name in enumerate(names):
    ax = axes[i]
    sizes = list(sentiment_percentages[name].values())
    labels = sentiment_percentages[name].keys()
    colors = ['green', 'gold', 'red']  # Customize colors as needed
    ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    ax.set_title(f'Sentiment Distribution for {name}')

plt.tight_layout()
plt.show()

import json
import matplotlib.pyplot as plt
import numpy as np
from collections import Counter
from wordcloud import WordCloud

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Extract aspects and their counts from all files
all_aspects = []
all_sentiments = []
for file_path in file_paths:
    with open(file_path, "r") as file:
        data = json.load(file)
        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet:
                        all_aspects.append(triplet['Aspect'])
                    if 'Polarity' in triplet:
                        all_sentiments.append(triplet['Polarity'])

# Count aspect occurrences
aspect_counts = Counter(all_aspects)

# Get top 10 aspects
top_25_aspects = aspect_counts.most_common(25)

# Prepare data for plot
aspects = [aspect for aspect, count in top_25_aspects]
counts = [count for aspect, count in top_25_aspects]

# Count sentiment occurrences
sentiment_counts = Counter(all_sentiments)
sentiments = list(sentiment_counts.keys())
sentiment_sizes = list(sentiment_counts.values())

# Create word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(aspect_counts)

# Create subplots for word cloud, horizontal bar chart, and pie chart
fig, axes = plt.subplots(1, 2, figsize=(24, 8))

# Display word cloud
# axes[0].imshow(wordcloud, interpolation='bilinear')
# axes[0].axis("off")
# axes[0].set_title('Word Cloud of Aspects')
def make_autopct(values):
    def my_autopct(pct):
        total = sum(values)
        val = int(round(pct*total/100.0))
        return f'{pct:.1f}%'
    return my_autopct

# Display horizontal bar chart
axes[1].barh(aspects, counts)
# axes[1].set_xlabel('Count', fontsize=14)
# axes[1].set_ylabel('Aspect', fontsize=14)
# axes[1].set_title('Top 25 Aspects Across All Data', fontsize=16)
axes[1].invert_yaxis()
axes[1].tick_params(axis='y', labelsize=14)
# for bar in bars:
#     width = bar.get_width()
#     axes[1].text(width + 0.5, bar.get_y() + bar.get_height()/2, str(width), ha='center', va='center', fontsize=14)


# # Add count labels to bars in the subplot
for i, v in enumerate(counts):
    axes[1].text(v + 0.5, i, str(v), color='black', va='center', size=14)

# Display pie chart for sentiments
wedges, texts, autotexts = axes[0].pie(sentiment_sizes, labels=sentiments, autopct=make_autopct(sentiment_sizes), colors=['green', 'red', 'yellow'])
# axes[0].set_title('Sentiment Distribution', fontsize=16)

for text in texts:
    text.set_fontsize(14)
for autotext in autotexts:
    autotext.set_fontweight('bold')
    autotext.set_fontsize(14)
    # autotext.set_color('white')
# Adjust layout and display the figure
plt.tight_layout()
plt.show()

import json
import matplotlib.pyplot as plt
import numpy as np
from collections import Counter
from wordcloud import WordCloud

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Extract aspects and their counts from all files
all_aspects = []
all_sentiments = []
for file_path in file_paths:
    with open(file_path, "r") as file:
        data = json.load(file)
        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet:
                        all_aspects.append(triplet['Aspect'])
                    if 'Polarity' in triplet:
                        all_sentiments.append(triplet['Polarity'])

# Count aspect occurrences
aspect_counts = Counter(all_aspects)

# Get top 25 aspects
top_25_aspects = aspect_counts.most_common(25)

# Prepare data for plot
aspects = [aspect for aspect, count in top_25_aspects]
counts = [count for aspect, count in top_25_aspects]

# Count sentiment occurrences
sentiment_counts = Counter(all_sentiments)
sentiments = list(sentiment_counts.keys())
sentiment_sizes = list(sentiment_counts.values())

# Function for percentage labels in pie chart
def make_autopct(values):
    def my_autopct(pct):
        total = sum(values)
        val = int(round(pct * total / 100.0))
        return f'{pct:.1f}%'
    return my_autopct

# Create and display word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(aspect_counts)
plt.figure(figsize=(12, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title('Word Cloud of Aspects', fontsize=16)
plt.show()

# Create and display horizontal bar chart
plt.figure(figsize=(12, 8))
bars = plt.barh(aspects, counts)
plt.xlabel('Count', fontsize=14)
plt.ylabel('Aspect', fontsize=14)
# plt.title('Top 20 Aspects Across All Data', fontsize=16)
plt.gca().invert_yaxis()
plt.tick_params(axis='y', labelsize=14)
for i, v in enumerate(counts):
    plt.text(v + 0.5, i, str(v), color='black', va='center', size=14)
plt.show()

# Create and display pie chart for sentiments
plt.figure(figsize=(12, 8))
wedges, texts, autotexts = plt.pie(sentiment_sizes, labels=sentiments, autopct=make_autopct(sentiment_sizes), colors=['green', 'red', 'yellow'])
# plt.title('Sentiment Distribution', fontsize=16)
for text in texts:
    text.set_fontsize(14)
for autotext in autotexts:
    autotext.set_fontweight('bold')
    autotext.set_fontsize(14)
plt.show()

import json
import matplotlib.pyplot as plt
from collections import Counter

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Names corresponding to the file paths
names = ["Anies Baswedan", "Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"]

# Number of top aspects to display
top_n = 10

# List to store top aspects per candidate
top_aspects_per_candidate = []

# Iterate over files and extract top aspects for each candidate
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)
        aspects = []
        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet:
                        aspects.append(triplet['Aspect'])
        top_aspects = Counter(aspects).most_common(top_n)
        top_aspects_per_candidate.append((name, top_aspects))

# Plotting
fig, axes = plt.subplots(2, 3, figsize=(15, 8))  # 2 rows, 3 columns for 6 candidates

for i, (name, top_aspects) in enumerate(top_aspects_per_candidate):
    row = i // 3
    col = i % 3
    ax = axes[row, col]

    aspect_labels = [aspect for aspect, count in top_aspects]
    aspect_counts = [count for aspect, count in top_aspects]

    ax.bar(aspect_labels, aspect_counts)
    # ax.set_xlabel("Aspects")
    # ax.set_ylabel("Frequency")
    ax.set_title(f"Top {top_n} Aspects for {name}")
    ax.tick_params(axis='x', rotation=45, labelsize=12)

plt.tight_layout()
plt.show()

import json
import matplotlib.pyplot as plt
from collections import Counter

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Names corresponding to the file paths
names = ["Anies Baswedan", "Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"]

# Number of top aspects to display
top_n = 10

# List to store top aspects per candidate
top_aspects_per_candidate = []

# Iterate over files and extract top aspects for each candidate
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)
        aspects = []
        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet:
                        aspects.append(triplet['Aspect'])
        top_aspects = Counter(aspects).most_common(top_n)
        top_aspects_per_candidate.append((name, top_aspects))

# Plotting
fig, axes = plt.subplots(3, 2, figsize=(15, 15))  # 3 rows, 2 columns for 6 candidates

for i, (name, top_aspects) in enumerate(top_aspects_per_candidate):
    row = i % 3
    col = i // 3
    ax = axes[row, col]

    aspect_labels = [aspect for aspect, count in top_aspects]
    aspect_counts = [count for aspect, count in top_aspects]

    ax.bar(aspect_labels, aspect_counts)
    ax.set_xlabel("Aspects")
    ax.set_ylabel("Frequency")
    ax.set_title(f"Top {top_n} Aspects for {name}")

    ax.tick_params(axis='x', rotation=45, labelsize=12)
    ax.tick_params(axis='y', labelsize=12)

# Adding overall title
plt.suptitle('Top Aspects Analysis for Candidates', fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust the layout and position of the overall title

# Show plot
plt.show()

import json
import matplotlib.pyplot as plt
from collections import Counter

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Names corresponding to the file paths
names = ["Anies Baswedan", "Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"]

# Number of top aspects to display
top_n = 25

# List to store top aspects per candidate
top_aspects_per_candidate = []

# Iterate over files and extract top aspects for each candidate
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)
        aspects = []
        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet:
                        aspects.append(triplet['Aspect'])
        top_aspects = Counter(aspects).most_common(top_n)
        top_aspects_per_candidate.append((name, top_aspects))

# Determine the min and max frequency values for setting x-axis limits
# all_counts = [count for candidate in top_aspects_per_candidate for aspect, count in candidate[1]]
# min_value = 0  # since frequency can't be negative, we start from 0
# max_value = 600

# Plotting
fig, axes = plt.subplots(3, 2, figsize=(25, 25))  # 3 rows, 2 columns for 6 candidates

for i, (name, top_aspects) in enumerate(top_aspects_per_candidate):
    row = i % 3
    col = i // 3
    ax = axes[row, col]

    aspect_labels = [aspect for aspect, count in top_aspects]
    aspect_counts = [count for aspect, count in top_aspects]

    # Reverse the order to have the most frequent aspect on top
    aspect_labels.reverse()
    aspect_counts.reverse()

    ax.barh(aspect_labels, aspect_counts)
    ax.set_xlabel("Frequency")
    ax.set_ylabel("Aspects")
    ax.set_title(f"Top {top_n} Aspects for {name}")

    # ax.set_xlim(min_value, max_value)  # Set the x-axis limits to be the same for all plots
    ax.tick_params(axis='x', rotation=0, labelsize=14)
    ax.tick_params(axis='y', labelsize=14)

    # Add count labels next to each bar
    for j, count in enumerate(aspect_counts):
        ax.text(count + 0.1, j, str(count), ha='left', va='center', fontsize=14)

# Adding overall title
plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust the layout and position of the overall title

# Show plot
plt.show()

import json
import matplotlib.pyplot as plt
from collections import Counter, defaultdict
import numpy as np

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Names corresponding to the file paths
names = ["Anies Baswedan", "Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"]

# Number of top aspects to display
top_n = 25

# Initialize dictionaries to store top aspects and sentiment counts
top_aspects_per_candidate = []
sentiment_distribution_per_candidate = []

# Iterate over files and extract top aspects and sentiment counts for each candidate
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)

        # Extract aspects and sentiment counts
        aspects = []
        sentiment_counts = defaultdict(lambda: {"Positive": 0, "Neutral": 0, "Negative": 0})

        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet:
                        aspect = triplet['Aspect']
                        aspects.append(aspect)
                        if 'Polarity' in triplet:
                            sentiment_counts[aspect][triplet['Polarity']] += 1

        # Get top aspects
        top_aspects = Counter(aspects).most_common(top_n)
        top_aspects_per_candidate.append((name, top_aspects))

        # Keep only sentiment counts for top aspects
        top_aspect_keys = [aspect for aspect, count in top_aspects]  # Maintain order
        filtered_sentiment_counts = {aspect: sentiment_counts[aspect] for aspect in top_aspect_keys}
        sentiment_distribution_per_candidate.append((name, filtered_sentiment_counts))

# Plotting top aspects
fig, axes = plt.subplots(2, 3, figsize=(25, 25))  # 2 rows, 3 columns for 6 candidates

for i, (name, top_aspects) in enumerate(top_aspects_per_candidate):
    row = i // 3
    col = i % 3
    ax = axes[row, col]

    aspect_labels = [aspect for aspect, count in top_aspects]
    aspect_counts = [count for aspect, count in top_aspects]

    ax.bar(aspect_labels, aspect_counts)
    ax.set_xlabel("Aspects")
    ax.set_ylabel("Frequency")
    ax.set_title(f"Top {top_n} Aspects for {name}")
    ax.tick_params(axis='x', rotation=45, labelsize=12)

    # Add count labels above each bar
    for j, count in enumerate(aspect_counts):
        ax.text(j, count + 0.1, str(count), ha='center', va='bottom')

plt.tight_layout()
plt.show()

# Plotting sentiment distribution for top aspects
fig, axes = plt.subplots(2, 3, figsize=(25, 25))  # 2 rows, 3 columns for 6 candidates

for i, (name, sentiment_counts) in enumerate(sentiment_distribution_per_candidate):
    row = i // 3
    col = i % 3
    ax = axes[row, col]

    # Prepare data for plotting
    aspects = list(sentiment_counts.keys())
    positive_counts = [sentiment_counts[aspect]["Positive"] for aspect in aspects]
    neutral_counts = [sentiment_counts[aspect]["Neutral"] for aspect in aspects]
    negative_counts = [sentiment_counts[aspect]["Negative"] for aspect in aspects]

    # Set bar width
    bar_width = 0.2

    # Set position of bar on X axis
    br1 = np.arange(len(aspects))
    br2 = [x + bar_width for x in br1]
    br3 = [x + bar_width for x in br2]

    # Plot bars for each sentiment
    ax.bar(br1, positive_counts, color='g', width=bar_width, label='Positive')
    ax.bar(br2, neutral_counts, color='y', width=bar_width, label='Neutral')
    ax.bar(br3, negative_counts, color='r', width=bar_width, label='Negative')

    # Set x-axis labels and rotate
    ax.set_xticks([r + bar_width for r in range(len(aspects))])
    ax.set_xticklabels(aspects, rotation=45, ha='right', fontsize=14)

    ax.set_xlabel("Aspects")
    ax.set_ylabel("Frequency")
    ax.set_title(f"Sentiment Distribution for Top Aspects of {name}")
    ax.legend()

    # Add count labels above each bar
    for j, (pos_count, neu_count, neg_count) in enumerate(zip(positive_counts, neutral_counts, negative_counts)):
        ax.text(br1[j], pos_count + 0.1, str(pos_count), ha='center', va='bottom', color='black')
        ax.text(br2[j], neu_count + 0.1, str(neu_count), ha='center', va='bottom', color='black')
        ax.text(br3[j], neg_count + 0.1, str(neg_count), ha='center', va='bottom', color='black')

plt.tight_layout()
plt.show()

import json
import matplotlib.pyplot as plt
from collections import Counter, defaultdict
import numpy as np

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Names corresponding to the file paths
names = ["Anies Baswedan", "Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"]

# Number of top aspects to display
top_n = 25

# Initialize dictionaries to store top aspects and sentiment counts
top_aspects_per_candidate = []
sentiment_distribution_per_candidate = []

# Iterate over files and extract top aspects and sentiment counts for each candidate
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)

        # Extract aspects and sentiment counts
        aspects = []
        sentiment_counts = defaultdict(lambda: {"Positive": 0, "Neutral": 0, "Negative": 0})

        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet:
                        aspect = triplet['Aspect']
                        aspects.append(aspect)
                        if 'Polarity' in triplet:
                            sentiment_counts[aspect][triplet['Polarity']] += 1

        # Get top aspects
        top_aspects = Counter(aspects).most_common(top_n)
        top_aspects_per_candidate.append((name, top_aspects))

        # Keep only sentiment counts for top aspects
        top_aspect_keys = [aspect for aspect, count in top_aspects]  # Maintain order
        filtered_sentiment_counts = {aspect: sentiment_counts[aspect] for aspect in top_aspect_keys}
        sentiment_distribution_per_candidate.append((name, filtered_sentiment_counts))

# Plotting top aspects
fig, axes = plt.subplots(2, 3, figsize=(35, 15))  # 2 rows, 3 columns for 6 candidates

for i, (name, top_aspects) in enumerate(top_aspects_per_candidate):
    row = i // 3
    col = i % 3
    ax = axes[row, col]

    aspect_labels = [aspect for aspect, count in top_aspects]
    aspect_counts = [count for aspect, count in top_aspects]

    # Reverse the order to have the most frequent aspect on top
    aspect_labels.reverse()
    aspect_counts.reverse()

    ax.barh(aspect_labels, aspect_counts)
    ax.set_xlabel("Frequency")
    ax.set_ylabel("Aspects")
    ax.set_title(f"Top {top_n} Aspects for {name}")
    ax.tick_params(axis='x', rotation=45, labelsize=16)

    # Add count labels next to each bar
    for j, count in enumerate(aspect_counts):
        ax.text(count + 0.1, j, str(count), ha='left', va='center', fontsize=14)

plt.tight_layout()
plt.show()

# Plotting sentiment distribution for top aspects
fig, axes = plt.subplots(2, 3, figsize=(35, 15))  # 2 rows, 3 columns for 6 candidates

for i, (name, sentiment_counts) in enumerate(sentiment_distribution_per_candidate):
    row = i // 3
    col = i % 3
    ax = axes[row, col]

    # Prepare data for plotting
    aspects = list(sentiment_counts.keys())
    positive_counts = [sentiment_counts[aspect]["Positive"] for aspect in aspects]
    neutral_counts = [sentiment_counts[aspect]["Neutral"] for aspect in aspects]
    negative_counts = [sentiment_counts[aspect]["Negative"] for aspect in aspects]

    # Set bar width
    bar_width = 0.2

    # Set position of bar on Y axis
    br1 = np.arange(len(aspects))
    br2 = [x + bar_width for x in br1]
    br3 = [x + bar_width for x in br2]

    aspect_labels = [aspect for aspect, count in top_aspects]
    aspect_counts = [count for aspect, count in top_aspects]

    # Reverse the order to have the most frequent aspect on top
    aspects.reverse()
    positive_counts.reverse()
    neutral_counts.reverse()
    negative_counts.reverse()

    # Plot bars for each sentiment
    ax.barh(br1, positive_counts, color='g', height=bar_width, label='Positive')
    ax.barh(br2, neutral_counts, color='y', height=bar_width, label='Neutral')
    ax.barh(br3, negative_counts, color='r', height=bar_width, label='Negative')

    # Set y-axis labels and rotate
    ax.set_yticks([r + bar_width for r in range(len(aspects))])
    ax.set_yticklabels(aspects, fontsize=16)

    ax.set_xlabel("Frequency")
    ax.set_ylabel("Aspects")
    ax.set_title(f"Sentiment Distribution for Top Aspects of {name}")
    ax.legend()

    # Add count labels next to each bar
    for j, (pos_count, neu_count, neg_count) in enumerate(zip(positive_counts, neutral_counts, negative_counts)):
        ax.text(pos_count + 0.1, br1[j], str(pos_count), ha='left', va='center', color='black', fontsize=14)
        ax.text(neu_count + 0.1, br2[j], str(neu_count), ha='left', va='center', color='black', fontsize=14)
        ax.text(neg_count + 0.1, br3[j], str(neg_count), ha='left', va='center', color='black', fontsize=14)

plt.tight_layout()
plt.show()

import json
import matplotlib.pyplot as plt
from collections import Counter, defaultdict
import numpy as np

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Names corresponding to the file paths
names = ["Anies Baswedan", "Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"]

# Number of top aspects to display
top_n = 15

# Initialize dictionaries to store top aspects and sentiment counts
top_aspects_per_candidate = []
sentiment_distribution_per_candidate = []

# Iterate over files and extract top aspects and sentiment counts for each candidate
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)

        # Extract aspects and sentiment counts
        aspects = []
        sentiment_counts = defaultdict(lambda: {"Positive": 0, "Neutral": 0, "Negative": 0})

        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet:
                        aspect = triplet['Aspect']
                        aspects.append(aspect)
                        if 'Polarity' in triplet:
                            sentiment_counts[aspect][triplet['Polarity']] += 1

        # Get top aspects
        top_aspects = Counter(aspects).most_common(top_n)
        top_aspects_per_candidate.append((name, top_aspects))

        # Keep only sentiment counts for top aspects
        top_aspect_keys = [aspect for aspect, count in top_aspects]  # Maintain order
        filtered_sentiment_counts = {aspect: sentiment_counts[aspect] for aspect in top_aspect_keys}
        sentiment_distribution_per_candidate.append((name, filtered_sentiment_counts))

# Plotting top aspects
fig, axes = plt.subplots(3, 2, figsize=(18, 20))  # 3 rows, 2 columns for 6 candidates

for i, (name, top_aspects) in enumerate(top_aspects_per_candidate):
    row = i % 3
    col = i // 3
    ax = axes[row, col]

    aspect_labels = [aspect for aspect, count in top_aspects]
    aspect_counts = [count for aspect, count in top_aspects]
    total_aspects = sum(aspect_counts)  # Total number of aspects

    # Reverse the order to have the most frequent aspect on top
    aspect_labels.reverse()
    aspect_counts.reverse()

    ax.barh(aspect_labels, aspect_counts)
    ax.set_xlabel("Frequency")
    ax.set_ylabel("Aspects")
    ax.set_title(f"Top {top_n} Aspects for {name}")
    ax.tick_params(axis='x', rotation=45, labelsize=12)

    # Add count and percentage labels
    # for j, count in enumerate(aspect_counts):
    #     percentage = (count / total_aspects) * 100
    #     ax.text(count + 0.1, j, str(count), ha='left', va='center', fontsize=10)
    #     ax.text(count + 0.1, j - 0.3, f'({percentage:.1f}%)', ha='left', va='center', fontsize=10)
    for j, count in enumerate(aspect_counts):
        percentage = (count / total_aspects) * 100
        ax.text(count + 0.1, j, f'{count} ({percentage:.1f}%)', ha='left', va='center', fontsize=10)

plt.tight_layout()
plt.show()

# Plotting sentiment distribution for top aspects
fig, axes = plt.subplots(3, 2, figsize=(16, 25))  # 3 rows, 2 columns for 6 candidates

for i, (name, sentiment_counts) in enumerate(sentiment_distribution_per_candidate):
    row = i % 3
    col = i // 3
    ax = axes[row, col]

    # Prepare data for plotting
    aspects = list(sentiment_counts.keys())
    positive_counts = [sentiment_counts[aspect]["Positive"] for aspect in aspects]
    neutral_counts = [sentiment_counts[aspect]["Neutral"] for aspect in aspects]
    negative_counts = [sentiment_counts[aspect]["Negative"] for aspect in aspects]

    # Ensure the order of aspects is the same as the top aspects plot
    aspects.reverse()
    positive_counts.reverse()
    neutral_counts.reverse()
    negative_counts.reverse()

    # Set bar width
    bar_width = 0.2

    # Set position of bar on Y axis
    br1 = np.arange(len(aspects))
    br2 = [x + bar_width for x in br1]
    br3 = [x + bar_width for x in br2]

    # Plot bars for each sentiment
    ax.barh(br1, positive_counts, color='g', height=bar_width, label='Positive')
    ax.barh(br2, neutral_counts, color='y', height=bar_width, label='Neutral')
    ax.barh(br3, negative_counts, color='r', height=bar_width, label='Negative')

    # Set y-axis labels and rotate
    ax.set_yticks([r + bar_width for r in range(len(aspects))])
    ax.set_yticklabels(aspects, fontsize=14)

    ax.set_xlabel("Frequency")
    ax.set_ylabel("Aspects")
    ax.set_title(f"Sentiment Distribution for Top Aspects of {name}")
    ax.legend()

    # Add count labels next to each bar
    for j, (pos_count, neu_count, neg_count) in enumerate(zip(positive_counts, neutral_counts, negative_counts)):
        ax.text(pos_count + 0.1, br1[j], str(pos_count), ha='left', va='center', color='black', fontsize=11)
        ax.text(neu_count + 0.1, br2[j], str(neu_count), ha='left', va='center', color='black', fontsize=11)
        ax.text(neg_count + 0.1, br3[j], str(neg_count), ha='left', va='center', color='black', fontsize=11)

plt.tight_layout()
plt.show()

import json
import matplotlib.pyplot as plt
from collections import Counter, defaultdict
import numpy as np

# File paths
file_paths = [
    "results_filtered_anis.json",
    "results_filtered_prabowo.json",
    "results_filtered_ganjar.json",
    "results_filtered_imin.json",
    "results_filtered_gibran.json",
    "results_filtered_mahfud.json"
]

# Names corresponding to the file paths
names = [
    "Anies Baswedan",
    "Prabowo Subianto",
    "Ganjar Pranowo",
    "Muhaimin Iskandar",
    "Gibran Rakabuming",
    "Mahfud MD"
]

# Number of top aspects to display
top_n = 10

# Initialize dictionaries to store top aspects and sentiment counts
top_aspects_per_candidate = []
sentiment_distribution_per_candidate = []

# Iterate over files and extract top aspects and sentiment counts for each candidate
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)

        # Extract aspects and sentiment counts
        aspects = []
        sentiment_counts = defaultdict(lambda: {"Positive": 0, "Neutral": 0, "Negative": 0})

        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet:
                        aspect = triplet['Aspect']
                        aspects.append(aspect)
                        if 'Polarity' in triplet:
                            sentiment_counts[aspect][triplet['Polarity']] += 1

        # Get top aspects
        top_aspects = Counter(aspects).most_common(top_n)
        top_aspects_per_candidate.append((name, top_aspects))

        # Keep only sentiment counts for top aspects
        top_aspect_keys = [aspect for aspect, count in top_aspects]  # Maintain order
        filtered_sentiment_counts = {aspect: sentiment_counts[aspect] for aspect in top_aspect_keys}
        sentiment_distribution_per_candidate.append((name, filtered_sentiment_counts))

# Plotting top aspects
fig, axes = plt.subplots(3, 2, figsize=(15, 18))  # 3 rows, 2 columns for 6 candidates


# Define the order of candidates for the subplot arrangement
candidate_order = [
    (0, 0, "Anies Baswedan"),
    (0, 1, "Muhaimin Iskandar"),
    (1, 0, "Prabowo Subianto"),
    (1, 1, "Gibran Rakabuming"),
    (2, 0, "Ganjar Pranowo"),
    (2, 1, "Mahfud MD")
]

for row, col, name in candidate_order:
    ax = axes[row, col]

    # Find the top aspects for the current candidate
    top_aspects = next(top_aspects for candidate_name, top_aspects in top_aspects_per_candidate if candidate_name == name)
    aspect_labels = [aspect for aspect, count in top_aspects]
    aspect_counts = [count for aspect, count in top_aspects]

    ax.bar(aspect_labels, aspect_counts)
    # ax.set_xlabel("Aspects", fontsize=font_size)
    # ax.set_ylabel("Frequency", fontsize=font_size)
    ax.set_title(f"Top {top_n} Aspects for {name}", fontsize=16)
    ax.tick_params(axis='x', rotation=45, labelsize=14)
    ax.tick_params(axis='y', labelsize=14)

    # Add count labels above each bar
    for j, count in enumerate(aspect_counts):
        ax.text(j, count + 0.1, str(count), ha='center', va='bottom', fontsize=12)

plt.tight_layout()
plt.show()

# Plotting sentiment distribution for top aspects
fig, axes = plt.subplots(3, 2, figsize=(18, 18))  # 3 rows, 2 columns for 6 candidates

for row, col, name in candidate_order:
    ax = axes[row, col]

    # Find the sentiment counts for the current candidate
    sentiment_counts = next(sentiment_counts for candidate_name, sentiment_counts in sentiment_distribution_per_candidate if candidate_name == name)

    # Prepare data for plotting
    aspects = list(sentiment_counts.keys())
    positive_counts = [sentiment_counts[aspect]["Positive"] for aspect in aspects]
    neutral_counts = [sentiment_counts[aspect]["Neutral"] for aspect in aspects]
    negative_counts = [sentiment_counts[aspect]["Negative"] for aspect in aspects]

    # Set bar width
    bar_width = 0.2

    # Set position of bar on X axis
    br1 = np.arange(len(aspects))
    br2 = [x + bar_width for x in br1]
    br3 = [x + bar_width for x in br2]

    # Plot bars for each sentiment
    ax.bar(br1, positive_counts, color='g', width=bar_width, label='Positive')
    ax.bar(br2, neutral_counts, color='y', width=bar_width, label='Neutral')
    ax.bar(br3, negative_counts, color='r', width=bar_width, label='Negative')

    # Set x-axis labels and rotate
    ax.set_xticks([r + bar_width for r in range(len(aspects))])
    ax.set_xticklabels(aspects, rotation=45, ha='right', fontsize=14)


    # ax.set_xlabel("Aspects", fontsize=font_size)
    # ax.set_ylabel("Frequency", fontsize=font_size)
    ax.set_title(f"Sentiment Distribution for Top Aspects of {name}", fontsize=16)
    ax.legend(fontsize=font_size)

    # Add count labels above each bar
    for j, (pos_count, neu_count, neg_count) in enumerate(zip(positive_counts, neutral_counts, negative_counts)):
        ax.text(br1[j], pos_count + 0.1, str(pos_count), ha='center', va='bottom', color='black', fontsize=12)
        ax.text(br2[j], neu_count + 0.1, str(neu_count), ha='center', va='bottom', color='black', fontsize=12)
        ax.text(br3[j], neg_count + 0.1, str(neg_count), ha='center', va='bottom', color='black', fontsize=12)

plt.tight_layout()
plt.show()

import json
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np

# File paths
file_paths = [
    "results_filtered_anis.json",
    "results_filtered_prabowo.json",
    "results_filtered_ganjar.json",
    "results_filtered_imin.json",
    "results_filtered_gibran.json",
    "results_filtered_mahfud.json"
]

# Names corresponding to the file paths
names = [
    "Anies Baswedan",
    "Prabowo Subianto",
    "Ganjar Pranowo",
    "Muhaimin Iskandar",
    "Gibran Rakabuming",
    "Mahfud MD"
]

# Number of top aspects to display
top_n = 5

# Iterate over files and extract top aspects for each sentiment
top_aspects_per_candidate = []
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)

        # Initialize dictionary to store aspects for each sentiment
        aspects_by_sentiment = {"Positive": [], "Neutral": [], "Negative": []}

        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet and 'Polarity' in triplet:
                        sentiment = triplet['Polarity']
                        aspect = triplet['Aspect']
                        aspects_by_sentiment[sentiment].append(aspect)

        # Get top aspects for each sentiment
        top_aspects = {}
        for sentiment, aspects in aspects_by_sentiment.items():
            top_aspects[sentiment] = Counter(aspects).most_common(top_n)

        top_aspects_per_candidate.append((name, top_aspects))

# Plotting
fig, axes = plt.subplots(2, 3, figsize=(18, 10))  # 2 rows, 3 columns for 6 candidates

for i, (name, top_aspects) in enumerate(top_aspects_per_candidate):
    row = i // 3
    col = i % 3
    ax = axes[row, col]

    # Prepare data for plotting
    sentiments = list(top_aspects.keys())
    aspect_labels = [[aspect for aspect, count in top_aspects[sentiment]] for sentiment in sentiments]
    aspect_counts = [[count for aspect, count in top_aspects[sentiment]] for sentiment in sentiments]

    # Set bar width
    bar_width = 0.2

    # Set position of bar on X axis
    br1 = np.arange(len(aspect_labels[0]))
    br2 = [x + bar_width for x in br1]
    br3 = [x + bar_width for x in br2]

    # Plot bars for each sentiment
    ax.bar(br1, aspect_counts[0], color='g', width=bar_width, label='Positive')
    ax.bar(br2, aspect_counts[1], color='y', width=bar_width, label='Neutral')
    ax.bar(br3, aspect_counts[2], color='r', width=bar_width, label='Negative')

    # Set x-axis labels and rotate
    ax.set_xticks([r + bar_width for r in range(len(aspect_labels[0]))])
    ax.set_xticklabels(aspect_labels[0], rotation=45, ha='right', fontsize=12)
    ax.set_xticklabels(aspect_labels[1], rotation=45, ha='right', fontsize=12)
    ax.set_xticklabels(aspect_labels[2], rotation=45, ha='right', fontsize=12)

    # ax.set_xlabel("Aspects")
    # ax.set_ylabel("Frequency")
    ax.set_title(f"Top {top_n} Aspects for {name}")
    ax.legend()

plt.tight_layout()
plt.show()

import json
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np

# File paths
file_paths = [
    "results_filtered_anis.json",
    "results_filtered_prabowo.json",
    "results_filtered_ganjar.json",
    "results_filtered_imin.json",
    "results_filtered_gibran.json",
    "results_filtered_mahfud.json"
]

# Names corresponding to the file paths
names = [
    "Anies Baswedan",
    "Prabowo Subianto",
    "Ganjar Pranowo",
    "Muhaimin Iskandar",
    "Gibran Rakabuming",
    "Mahfud MD"
]

# Number of top aspects to display
top_n = 5

# Iterate over files and extract top aspects for each sentiment
top_aspects_per_candidate = []
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)

        # Initialize dictionary to store aspects for each sentiment
        aspects_by_sentiment = {"Positive": [], "Neutral": [], "Negative": []}

        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet and 'Polarity' in triplet:
                        sentiment = triplet['Polarity']
                        aspect = triplet['Aspect']
                        aspects_by_sentiment[sentiment].append(aspect)

        # Get top aspects for each sentiment
        top_aspects = {}
        for sentiment, aspects in aspects_by_sentiment.items():
            top_aspects[sentiment] = Counter(aspects).most_common(top_n)

        top_aspects_per_candidate.append((name, top_aspects))

# Plotting
fig, axes = plt.subplots(3, 2, figsize=(18, 15))  # 3 rows, 2 columns for 6 candidates

for i, (name, top_aspects) in enumerate(top_aspects_per_candidate):
    row = i % 3
    col = i // 3
    ax = axes[row, col]

    # Prepare data for plotting
    sentiments = list(top_aspects.keys())
    aspect_labels = [[aspect for aspect, count in top_aspects[sentiment]] for sentiment in sentiments]
    aspect_counts = [[count for aspect, count in top_aspects[sentiment]] for sentiment in sentiments]

    # Set bar width
    bar_width = 0.2

    # Set position of bar on X axis
    br1 = np.arange(len(aspect_labels[0]))
    br2 = [x + bar_width for x in br1]
    br3 = [x + bar_width for x in br2]

    # Plot bars for each sentiment
    ax.bar(br1, aspect_counts[0], color='g', width=bar_width, label='Positive')
    ax.bar(br2, aspect_counts[1], color='y', width=bar_width, label='Neutral')
    ax.bar(br3, aspect_counts[2], color='r', width=bar_width, label='Negative')

    # Set x-axis labels and rotate
    ax.set_xticks([r + bar_width for r in range(len(aspect_labels[0]))])
    ax.set_xticklabels(aspect_labels[0], rotation=45, ha='right', fontsize=12)
    ax.set_xticks([r + bar_width for r in range(len(aspect_labels[1]))])
    ax.set_xticklabels(aspect_labels[1], rotation=45, ha='right', fontsize=12)
    ax.set_xticks([r + bar_width for r in range(len(aspect_labels[2]))])
    ax.set_xticklabels(aspect_labels[2], rotation=45, ha='right', fontsize=12)

    ax.set_title(f"Top {top_n} Aspects for {name}")
    ax.legend()

plt.tight_layout()
plt.show()

import json
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np

# File paths
file_paths = [
    "results_filtered_anis.json",
    "results_filtered_prabowo.json",
    "results_filtered_ganjar.json",
    "results_filtered_imin.json",
    "results_filtered_gibran.json",
    "results_filtered_mahfud.json"
]

# Names corresponding to the file paths
names = [
    "Anies Baswedan",
    "Prabowo Subianto",
    "Ganjar Pranowo",
    "Muhaimin Iskandar",
    "Gibran Rakabuming",
    "Mahfud MD"
]

# Number of top aspects to display
top_n = 5

# Iterate over files and extract top aspects for each sentiment
top_aspects_per_candidate = []
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)

        # Initialize dictionary to store aspects for each sentiment
        aspects_by_sentiment = {"Positive": [], "Neutral": [], "Negative": []}

        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet and 'Polarity' in triplet:
                        sentiment = triplet['Polarity']
                        aspect = triplet['Aspect']
                        aspects_by_sentiment[sentiment].append(aspect)

        # Get top aspects for each sentiment
        top_aspects = {}
        for sentiment, aspects in aspects_by_sentiment.items():
            top_aspects[sentiment] = Counter(aspects).most_common(top_n)

        top_aspects_per_candidate.append((name, top_aspects))

# Plotting
fig, axes = plt.subplots(3, 2, figsize=(18, 25))  # 3 rows, 2 columns for 6 candidates

for i, (name, top_aspects) in enumerate(top_aspects_per_candidate):
    row = i % 3
    col = i // 3
    ax = axes[row, col]

    # Prepare data for plotting
    sentiments = list(top_aspects.keys())
    aspect_labels = [[aspect for aspect, count in top_aspects[sentiment]] for sentiment in sentiments]
    aspect_counts = [[count for aspect, count in top_aspects[sentiment]] for sentiment in sentiments]

    # Calculate percentages
    total_counts = [sum(counts) for counts in zip(*aspect_counts)]
    aspect_percentages = [[count / total * 100 if total > 0 else 0 for count, total in zip(counts, total_counts)] for counts in aspect_counts]

    # Set position of bar on X axis
    br = np.arange(len(aspect_labels[0]))

    # Plot stacked bars for each sentiment
    bars_positive = ax.bar(br, aspect_counts[0], color='g', label='Positive')
    bars_neutral = ax.bar(br, aspect_counts[1], bottom=aspect_counts[0], color='y', label='Neutral')
    bars_negative = ax.bar(br, aspect_counts[2], bottom=np.array(aspect_counts[0]) + np.array(aspect_counts[1]), color='r', label='Negative')

    # Annotate bars with percentage labels
    for bars, percentages in zip([bars_positive, bars_neutral, bars_negative], aspect_percentages):
        for bar, percentage in zip(bars, percentages):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width() / 2., bar.get_y() + height / 2., f'{percentage:.1f}%', ha='center', va='center', fontsize=10, color='black')

    # Set x-axis labels and rotate
    ax.set_xticks(br)
    ax.set_xticklabels(aspect_labels[0], rotation=45, ha='right', fontsize=12)

    ax.set_title(f"Top {top_n} Aspects for {name}")
    ax.legend()

plt.tight_layout()
plt.show()

import json
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np

# File paths
file_paths = [
    "results_filtered_anis.json",
    "results_filtered_prabowo.json",
    "results_filtered_ganjar.json",
    "results_filtered_imin.json",
    "results_filtered_gibran.json",
    "results_filtered_mahfud.json"
]

# Names corresponding to the file paths
names = [
    "Anies Baswedan",
    "Prabowo Subianto",
    "Ganjar Pranowo",
    "Muhaimin Iskandar",
    "Gibran Rakabuming",
    "Mahfud MD"
]

# Number of top aspects to display
top_n = 15

# Iterate over files and extract top aspects for each sentiment
top_aspects_per_candidate = []
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)

        # Initialize dictionary to store aspects for each sentiment
        aspects_by_sentiment = {"Positive": [], "Neutral": [], "Negative": []}

        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet and 'Polarity' in triplet:
                        sentiment = triplet['Polarity']
                        aspect = triplet['Aspect']
                        aspects_by_sentiment[sentiment].append(aspect)

        # Get top aspects for each sentiment
        top_aspects = {}
        for sentiment, aspects in aspects_by_sentiment.items():
            top_aspects[sentiment] = Counter(aspects).most_common(top_n)

        top_aspects_per_candidate.append((name, top_aspects))

# Plotting
fig, axes = plt.subplots(3, 2, figsize=(18, 20))  # 3 rows, 2 columns for 6 candidates

for i, (name, top_aspects) in enumerate(top_aspects_per_candidate):
    row = i % 3
    col = i // 3
    ax = axes[row, col]

    # Prepare data for plotting
    sentiments = list(top_aspects.keys())
    aspect_labels = [[aspect for aspect, count in top_aspects[sentiment]] for sentiment in sentiments]
    aspect_counts = [[count for aspect, count in top_aspects[sentiment]] for sentiment in sentiments]

    # Calculate percentages
    total_counts = [sum(counts) for counts in zip(*aspect_counts)]
    aspect_percentages = [[count / total * 100 if total > 0 else 0 for count, total in zip(counts, total_counts)] for counts in aspect_counts]

    # Reverse the order of aspects for better visualization
    aspect_labels_reversed = [list(reversed(labels)) for labels in aspect_labels]
    aspect_counts_reversed = [list(reversed(counts)) for counts in aspect_counts]
    aspect_percentages_reversed = [list(reversed(percentages)) for percentages in aspect_percentages]

    # Set position of bar on Y axis
    br = np.arange(len(aspect_labels_reversed[0]))

    # Plot horizontal stacked bars for each sentiment
    bars_positive = ax.barh(br, aspect_counts_reversed[0], color='g', label='Positive')
    bars_neutral = ax.barh(br, aspect_counts_reversed[1], left=aspect_counts_reversed[0], color='y', label='Neutral')
    bars_negative = ax.barh(br, aspect_counts_reversed[2], left=np.array(aspect_counts_reversed[0]) + np.array(aspect_counts_reversed[1]), color='r', label='Negative')

    # Annotate bars with percentage labels
    # for bars, percentages in zip([bars_positive, bars_neutral, bars_negative], aspect_percentages_reversed):
    #     for bar, percentage in zip(bars, percentages):
    #         width = bar.get_width()
    #         ax.text(bar.get_x() + width / 2., bar.get_y() + bar.get_height() / 2., f'{percentage:.1f}%', ha='center', va='center', fontsize=9, color='black')

    # Set y-axis labels
    ax.set_yticks(br)
    ax.set_yticklabels(aspect_labels_reversed[0], fontsize=12)

    ax.set_title(f"Top {top_n} Aspects for {name}")
    ax.legend()

plt.tight_layout()
plt.show()

# prompt: dari code diatas buat plot stacked barnya berdasarkan sentimennya pada masing aspek

import ast
import csv
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np
import requests
import json
import random
import os
from collections import defaultdict
import pandas as pd
from wordcloud import WordCloud
import re
import string
from PIL import Image
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import classification_report, f1_score
from sklearn.metrics import confusion_matrix, classification_report, f1_score

# ... (previous code remains the same)

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Names corresponding to the file paths
names = ["Anies Baswedan", "Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"]

# Number of top aspects to display
top_n = 10

# Iterate over files and extract top aspects with sentiments
top_aspects_with_sentiments = []
for file_path in file_paths:
    with open(file_path, "r") as file:
        data = json.load(file)
        aspects_with_sentiments = defaultdict(lambda: {"Positive": 0, "Neutral": 0, "Negative": 0})
        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet and 'Polarity' in triplet:
                        aspect = triplet['Aspect']
                        sentiment = triplet['Polarity']
                        aspects_with_sentiments[aspect][sentiment] += 1
        top_aspects = sorted(aspects_with_sentiments.items(), key=lambda item: sum(item[1].values()), reverse=True)[:top_n]
        top_aspects_with_sentiments.append(top_aspects)

# Plotting stacked bar charts
fig, axes = plt.subplots(2, 3, figsize=(15, 8))  # 2 rows, 3 columns for 6 candidates

for i, (name, top_aspects) in enumerate(zip(names, top_aspects_with_sentiments)):
    row = i // 3
    col = i % 3
    ax = axes[row, col]

    aspect_labels = [aspect for aspect, _ in top_aspects]
    positive_counts = [counts["Positive"] for _, counts in top_aspects]
    neutral_counts = [counts["Neutral"] for _, counts in top_aspects]
    negative_counts = [counts["Negative"] for _, counts in top_aspects]

    ax.bar(aspect_labels, positive_counts, label='Positive', color='g')
    ax.bar(aspect_labels, neutral_counts, bottom=positive_counts, label='Neutral', color='y')
    ax.bar(aspect_labels, negative_counts, bottom=[p + n for p, n in zip(positive_counts, neutral_counts)], label='Negative', color='r')

    ax.set_xlabel("Aspects")
    ax.set_ylabel("Frequency")
    ax.set_title(f"Top {top_n} Aspects for {name}")
    ax.tick_params(axis='x', rotation=45)
    ax.legend()

plt.tight_layout()
plt.show()

import json
from collections import defaultdict
import matplotlib.pyplot as plt

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Names corresponding to the file paths
names = ["Anies Baswedan", "Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"]

# Number of top aspects to display
top_n = 10

# Iterate over files and extract top aspects with sentiments
top_aspects_with_sentiments = []
for file_path in file_paths:
    with open(file_path, "r") as file:
        data = json.load(file)
        aspects_with_sentiments = defaultdict(lambda: {"Positive": 0, "Neutral": 0, "Negative": 0})
        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet and 'Polarity' in triplet:
                        aspect = triplet['Aspect']
                        sentiment = triplet['Polarity']
                        aspects_with_sentiments[aspect][sentiment] += 1
        top_aspects = sorted(aspects_with_sentiments.items(), key=lambda item: sum(item[1].values()), reverse=True)[:top_n]
        top_aspects_with_sentiments.append(top_aspects)

# Plotting stacked bar charts
fig, axes = plt.subplots(2, 3, figsize=(15, 8))  # 2 rows, 3 columns for 6 candidates

for i, (name, top_aspects) in enumerate(zip(names, top_aspects_with_sentiments)):
    row = i // 3
    col = i % 3
    ax = axes[row, col]

    aspect_labels = [aspect for aspect, _ in top_aspects]
    positive_counts = [counts["Positive"] for _, counts in top_aspects]
    neutral_counts = [counts["Neutral"] for _, counts in top_aspects]
    negative_counts = [counts["Negative"] for _, counts in top_aspects]

    total_counts = [p + n + neg for p, n, neg in zip(positive_counts, neutral_counts, negative_counts)]
    positive_percents = [p / total * 100 if total > 0 else 0 for p, total in zip(positive_counts, total_counts)]
    neutral_percents = [n / total * 100 if total > 0 else 0 for n, total in zip(neutral_counts, total_counts)]
    negative_percents = [neg / total * 100 if total > 0 else 0 for neg, total in zip(negative_counts, total_counts)]

    ax.bar(aspect_labels, positive_counts, label='Positive', color='g')
    ax.bar(aspect_labels, neutral_counts, bottom=positive_counts, label='Neutral', color='y')
    ax.bar(aspect_labels, negative_counts, bottom=[p + n for p, n in zip(positive_counts, neutral_counts)], label='Negative', color='r')

    for j in range(len(aspect_labels)):
        ax.text(j, positive_counts[j] / 2, f'{positive_percents[j]:.1f}%', ha='center', va='center', color='white')
        ax.text(j, positive_counts[j] + (neutral_counts[j] / 2), f'{neutral_percents[j]:.1f}%', ha='center', va='center', color='black')
        ax.text(j, positive_counts[j] + neutral_counts[j] + (negative_counts[j] / 2), f'{negative_percents[j]:.1f}%', ha='center', va='center', color='white')

    ax.set_xlabel("Aspects")
    ax.set_ylabel("Frequency")
    ax.set_title(f"Top {top_n} Aspects for {name}")
    ax.tick_params(axis='x', rotation=45)
    ax.legend()

plt.tight_layout()
plt.show()

import json
import matplotlib.pyplot as plt

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Initialize sentiment counts for each person
sentiment_counts = {
    "Anies Baswedan": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Prabowo Subianto": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Ganjar Pranowo": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Muhaimin Iskandar": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Gibran Rakabuming": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Mahfud MD": {"Positive": 0, "Neutral": 0, "Negative": 0}
}

# Iterate over files and count sentiments
for i, file_path in enumerate(file_paths):
    name = ["Anies Baswedan","Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"][i]
    with open(file_path, "r") as file:
        data = json.load(file)
        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Polarity' in triplet:
                        sentiment = triplet['Polarity']
                        sentiment_counts[name][sentiment] += 1
                    else:
                        print(f"Warning: 'Polarity' key missing in entry for {name}")

# Prepare data for plot
names = list(sentiment_counts.keys())
positive_counts = [sentiment_counts[name]['Positive'] for name in names]
neutral_counts = [sentiment_counts[name]['Neutral'] for name in names]
negative_counts = [sentiment_counts[name]['Negative'] for name in names]

# Calculate total counts and sort data
total_counts = [positive_counts[i] + neutral_counts[i] + negative_counts[i] for i in range(len(names))]
sorted_indices = sorted(range(len(names)), key=lambda i: total_counts[i], reverse=True)

# Reorder data based on sorted indices
names_sorted = [names[i] for i in sorted_indices]
positive_counts_sorted = [positive_counts[i] for i in sorted_indices]
neutral_counts_sorted = [neutral_counts[i] for i in sorted_indices]
negative_counts_sorted = [negative_counts[i] for i in sorted_indices]
total_counts_sorted = [total_counts[i] for i in sorted_indices]

# Calculate percentages for the sorted data
positive_percents = [positive_counts_sorted[i] / total_counts_sorted[i] * 100 if total_counts_sorted[i] > 0 else 0 for i in range(len(names_sorted))]
neutral_percents = [neutral_counts_sorted[i] / total_counts_sorted[i] * 100 if total_counts_sorted[i] > 0 else 0 for i in range(len(names_sorted))]
negative_percents = [negative_counts_sorted[i] / total_counts_sorted[i] * 100 if total_counts_sorted[i] > 0 else 0 for i in range(len(names_sorted))]

# Create horizontal stacked bar chart
fig, ax = plt.subplots(figsize=(10, 8))

bars_positive = ax.barh(names_sorted, positive_counts_sorted, label='Positive', color='g')
bars_neutral = ax.barh(names_sorted, neutral_counts_sorted, left=positive_counts_sorted, label='Neutral', color='y')
bars_negative = ax.barh(names_sorted, negative_counts_sorted, left=[i+j for i,j in zip(positive_counts_sorted, neutral_counts_sorted)], label='Negative', color='r')

# Add value labels on the bars
for i, name in enumerate(names_sorted):
    ax.text(positive_counts_sorted[i] / 2, i, f'{positive_percents[i]:.1f}%', ha='center', va='center', color='white')
    ax.text(positive_counts_sorted[i] + neutral_counts_sorted[i] / 2, i, f'{neutral_percents[i]:.1f}%', ha='center', va='center', color='black')
    ax.text(positive_counts_sorted[i] + neutral_counts_sorted[i] + negative_counts_sorted[i] / 2, i, f'{negative_percents[i]:.1f}%', ha='center', va='center', color='white')
    ax.text(total_counts_sorted[i] + 0.05, i, f'{total_counts_sorted[i]}', ha='left', va='center')

plt.xlabel('Sentiment Counts')
plt.ylabel('Candidates Names')
plt.title('Sentiment Comparison for Individuals (Horizontal Stacked)')
plt.legend()

plt.tight_layout()
plt.show()

import json
import matplotlib.pyplot as plt

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Initialize sentiment counts for each person
sentiment_counts = {
    "Anies Baswedan": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Prabowo Subianto": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Ganjar Pranowo": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Muhaimin Iskandar": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Gibran Rakabuming": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Mahfud MD": {"Positive": 0, "Neutral": 0, "Negative": 0}
}

# Iterate over files and count sentiments
for i, file_path in enumerate(file_paths):
    name = ["Anies Baswedan","Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"][i]
    with open(file_path, "r") as file:
        data = json.load(file)
        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Polarity' in triplet:
                        sentiment = triplet['Polarity']
                        sentiment_counts[name][sentiment] += 1
                    else:
                        print(f"Warning: 'Polarity' key missing in entry for {name}")

# Prepare data for plot
names = list(sentiment_counts.keys())
positive_counts = [sentiment_counts[name]['Positive'] for name in names]
neutral_counts = [sentiment_counts[name]['Neutral'] for name in names]
negative_counts = [sentiment_counts[name]['Negative'] for name in names]

# Calculate total counts and sort data
total_counts = [positive_counts[i] + neutral_counts[i] + negative_counts[i] for i in range(len(names))]
sorted_indices = sorted(range(len(names)), key=lambda i: total_counts[i], reverse=True)

# Reorder data based on sorted indices
names_sorted = [names[i] for i in sorted_indices]
positive_counts_sorted = [positive_counts[i] for i in sorted_indices]
neutral_counts_sorted = [neutral_counts[i] for i in sorted_indices]
negative_counts_sorted = [negative_counts[i] for i in sorted_indices]
total_counts_sorted = [total_counts[i] for i in sorted_indices]

# Calculate percentages for the sorted data
positive_percents = [positive_counts_sorted[i] / total_counts_sorted[i] * 100 if total_counts_sorted[i] > 0 else 0 for i in range(len(names_sorted))]
neutral_percents = [neutral_counts_sorted[i] / total_counts_sorted[i] * 100 if total_counts_sorted[i] > 0 else 0 for i in range(len(names_sorted))]
negative_percents = [negative_counts_sorted[i] / total_counts_sorted[i] * 100 if total_counts_sorted[i] > 0 else 0 for i in range(len(names_sorted))]

# Create stacked bar chart
fig, ax = plt.subplots(figsize=(10, 8))

bars_positive = ax.bar(names_sorted, positive_counts_sorted, label='Positive', color='g')
bars_neutral = ax.bar(names_sorted, neutral_counts_sorted, bottom=positive_counts_sorted, label='Neutral', color='y')
bars_negative = ax.bar(names_sorted, negative_counts_sorted, bottom=[i+j for i,j in zip(positive_counts_sorted, neutral_counts_sorted)], label='Negative', color='r')

# Add value labels on top of bars
for i, name in enumerate(names_sorted):
    ax.text(i, total_counts_sorted[i] + 0.05, f'{total_counts_sorted[i]}', ha='center', va='bottom')
    ax.text(i, positive_counts_sorted[i] / 2, f'{positive_percents[i]:.1f}%', ha='center', va='center', color='black')
    ax.text(i, positive_counts_sorted[i] + neutral_counts_sorted[i] / 2, f'{neutral_percents[i]:.1f}%', ha='center', va='center', color='black')
    ax.text(i, positive_counts_sorted[i] + neutral_counts_sorted[i] + negative_counts_sorted[i] / 2, f'{negative_percents[i]:.1f}%', ha='center', va='center', color='black')

plt.xlabel('Candidates Names')
plt.ylabel('Sentiment Counts')
plt.title('Sentiment Comparison for Individuals (Stacked)')
plt.xticks(rotation=45, ha='right')
plt.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import json
from collections import Counter

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Names corresponding to the file paths
names = ["Anies Baswedan", "Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"]

# Number of top aspects to display
top_n = 10

# Sentiment colors
sentiment_colors = {'Positive': 'green', 'Neutral': 'y', 'Negative': 'red'}

# Iterate over files and extract top aspects for each sentiment
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)

        # Initialize dictionaries to store aspects for each sentiment
        positive_aspects = []
        neutral_aspects = []
        negative_aspects = []

        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet and 'Polarity' in triplet:
                        aspect = triplet['Aspect']
                        sentiment = triplet['Polarity']
                        if sentiment == 'Positive':
                            positive_aspects.append(aspect)
                        elif sentiment == 'Neutral':
                            neutral_aspects.append(aspect)
                        elif sentiment == 'Negative':
                            negative_aspects.append(aspect)

        # Count top aspects for each sentiment
        top_positive_aspects = Counter(positive_aspects).most_common(top_n)
        top_neutral_aspects = Counter(neutral_aspects).most_common(top_n)
        top_negative_aspects = Counter(negative_aspects).most_common(top_n)

        # Plotting
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # 1 row, 3 columns for sentiments

        # Positive Aspects
        ax = axes[0]
        aspect_labels = [aspect for aspect, count in top_positive_aspects]
        aspect_counts = [count for aspect, count in top_positive_aspects]
        ax.bar(aspect_labels, aspect_counts, color=sentiment_colors['Positive'])
        ax.set_xlabel("Aspects")
        ax.set_ylabel("Frequency")
        ax.set_title(f"Top {top_n} Positive Aspects", fontsize=16)
        ax.tick_params(axis='x', rotation=45, labelsize=14)

        # Neutral Aspects
        ax = axes[1]
        aspect_labels = [aspect for aspect, count in top_neutral_aspects]
        aspect_counts = [count for aspect, count in top_neutral_aspects]
        ax.bar(aspect_labels, aspect_counts, color=sentiment_colors['Neutral'])
        ax.set_xlabel("Aspects")
        ax.set_ylabel("Frequency")
        ax.set_title(f"Top {top_n} Neutral Aspects", fontsize=16)
        ax.tick_params(axis='x', rotation=45, labelsize=14)

        # Negative Aspects
        ax = axes[2]
        aspect_labels = [aspect for aspect, count in top_negative_aspects]
        aspect_counts = [count for aspect, count in top_negative_aspects]
        ax.bar(aspect_labels, aspect_counts, color=sentiment_colors['Negative'])
        ax.set_xlabel("Aspects")
        ax.set_ylabel("Frequency")
        ax.set_title(f"Top {top_n} Negative Aspects", fontsize=16)
        ax.tick_params(axis='x', rotation=45, labelsize=14)

        # Set overall title for the figure
        fig.suptitle(f"Aspect by Sentiment for {name}", fontsize=18)

        plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the title
        plt.show()

import matplotlib.pyplot as plt
import json
from collections import Counter

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Names corresponding to the file paths
names = ["Anies Baswedan", "Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"]

# Number of top aspects to display
top_n = 15

# Sentiment colors
sentiment_colors = {'Positive': 'green', 'Neutral': 'yellow', 'Negative': 'red'}

# Iterate over files and extract top aspects for each sentiment
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)

        # Initialize lists to store aspects for each sentiment
        positive_aspects = []
        neutral_aspects = []
        negative_aspects = []

        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet and 'Polarity' in triplet:
                        aspect = triplet['Aspect']
                        sentiment = triplet['Polarity']
                        if sentiment == 'Positive':
                            positive_aspects.append(aspect)
                        elif sentiment == 'Neutral':
                            neutral_aspects.append(aspect)
                        elif sentiment == 'Negative':
                            negative_aspects.append(aspect)

        # Count top aspects for each sentiment
        top_positive_aspects = Counter(positive_aspects).most_common(top_n)
        top_neutral_aspects = Counter(neutral_aspects).most_common(top_n)
        top_negative_aspects = Counter(negative_aspects).most_common(top_n)

        # Reverse the order so the most frequent aspect is at the top
        top_positive_aspects = list(reversed(top_positive_aspects))
        top_neutral_aspects = list(reversed(top_neutral_aspects))
        top_negative_aspects = list(reversed(top_negative_aspects))

        # Plotting
        fig, axes = plt.subplots(1, 3, figsize=(18, 8))  # 1 row, 3 columns for sentiments

        # Positive Aspects
        ax = axes[0]
        aspect_labels = [aspect for aspect, count in top_positive_aspects]
        aspect_counts = [count for aspect, count in top_positive_aspects]
        ax.barh(aspect_labels, aspect_counts, color=sentiment_colors['Positive'])
        ax.set_xlabel("Frequency")
        ax.set_title(f"Top {top_n} Positive Aspects", fontsize=16)
        ax.tick_params(axis='y', labelsize=14)

        # Neutral Aspects
        ax = axes[1]
        aspect_labels = [aspect for aspect, count in top_neutral_aspects]
        aspect_counts = [count for aspect, count in top_neutral_aspects]
        ax.barh(aspect_labels, aspect_counts, color=sentiment_colors['Neutral'])
        ax.set_xlabel("Frequency")
        ax.set_title(f"Top {top_n} Neutral Aspects", fontsize=16)
        ax.tick_params(axis='y', labelsize=14)

        # Negative Aspects
        ax = axes[2]
        aspect_labels = [aspect for aspect, count in top_negative_aspects]
        aspect_counts = [count for aspect, count in top_negative_aspects]
        ax.barh(aspect_labels, aspect_counts, color=sentiment_colors['Negative'])
        ax.set_xlabel("Frequency")
        ax.set_title(f"Top {top_n} Negative Aspects", fontsize=16)
        ax.tick_params(axis='y', labelsize=14)

        # Set overall title for the figure
        # fig.suptitle(f"Aspect by Sentiment for {name}", fontsize=18)

        plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the title
        plt.show()

import matplotlib.pyplot as plt
import json
from collections import Counter

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Names corresponding to the file paths
names = ["Anies Baswedan", "Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"]

# Number of top aspects to display
top_n = 10

# Sentiment colors
sentiment_colors = {'Positive': 'green', 'Neutral': 'yellow', 'Negative': 'red'}

# Iterate over files and extract top aspects for each sentiment
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)

        # Initialize lists to store aspects for each sentiment
        positive_aspects = []
        neutral_aspects = []
        negative_aspects = []

        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet and 'Polarity' in triplet:
                        aspect = triplet['Aspect']
                        sentiment = triplet['Polarity']
                        if sentiment == 'Positive':
                            positive_aspects.append(aspect)
                        elif sentiment == 'Neutral':
                            neutral_aspects.append(aspect)
                        elif sentiment == 'Negative':
                            negative_aspects.append(aspect)

        # Count top aspects for each sentiment
        top_positive_aspects = Counter(positive_aspects).most_common(top_n)
        top_neutral_aspects = Counter(neutral_aspects).most_common(top_n)
        top_negative_aspects = Counter(negative_aspects).most_common(top_n)

        # Reverse the order so the most frequent aspect is at the top
        top_positive_aspects = list(reversed(top_positive_aspects))
        top_neutral_aspects = list(reversed(top_neutral_aspects))
        top_negative_aspects = list(reversed(top_negative_aspects))

        # Plotting Positive Aspects
        fig, ax = plt.subplots(figsize=(10, 7))
        aspect_labels = [aspect for aspect, count in top_positive_aspects]
        aspect_counts = [count for aspect, count in top_positive_aspects]
        ax.barh(aspect_labels, aspect_counts, color=sentiment_colors['Positive'])
        ax.set_xlabel("Frequency")
        ax.set_title(f"Top {top_n} Positive Aspects for {name}", fontsize=16)
        ax.tick_params(axis='y', labelsize=14)
        plt.tight_layout()
        # plt.savefig(f"top_positive_aspects_{name}.png")
        plt.show()

        # Plotting Neutral Aspects
        fig, ax = plt.subplots(figsize=(10, 7))
        aspect_labels = [aspect for aspect, count in top_neutral_aspects]
        aspect_counts = [count for aspect, count in top_neutral_aspects]
        ax.barh(aspect_labels, aspect_counts, color=sentiment_colors['Neutral'])
        ax.set_xlabel("Frequency")
        ax.set_title(f"Top {top_n} Neutral Aspects for {name}", fontsize=16)
        ax.tick_params(axis='y', labelsize=14)
        plt.tight_layout()
        # plt.savefig(f"top_neutral_aspects_{name}.png")
        plt.show()

        # Plotting Negative Aspects
        fig, ax = plt.subplots(figsize=(10, 7))
        aspect_labels = [aspect for aspect, count in top_negative_aspects]
        aspect_counts = [count for aspect, count in top_negative_aspects]
        ax.barh(aspect_labels, aspect_counts, color=sentiment_colors['Negative'])
        ax.set_xlabel("Frequency")
        ax.set_title(f"Top {top_n} Negative Aspects for {name}", fontsize=16)
        ax.tick_params(axis='y', labelsize=14)
        plt.tight_layout()
        # plt.savefig(f"top_negative_aspects_{name}.png")
        plt.show()

import matplotlib.pyplot as plt
import json
from collections import Counter
from wordcloud import WordCloud

# File paths
file_paths = [
    "results_filtered_anis.json",
    "results_filtered_prabowo.json",
    "results_filtered_ganjar.json",
    "results_filtered_imin.json",
    "results_filtered_gibran.json",
    "results_filtered_mahfud.json"
]

# Names corresponding to the file paths
names = [
    "Anies Baswedan",
    "Prabowo Subianto",
    "Ganjar Pranowo",
    "Muhaimin Iskandar",
    "Gibran Rakabuming",
    "Mahfud MD"
]

# Number of top aspects to display
top_n = 10

# Iterate over files and extract top aspects for each sentiment
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)

    # Initialize dictionaries to store aspects for each sentiment
    positive_aspects = []
    neutral_aspects = []
    negative_aspects = []

    for entry in data:
        if 'Triplets' in entry:
            for triplet in entry['Triplets']:
                if 'Aspect' in triplet and 'Polarity' in triplet:
                    aspect = triplet['Aspect']
                    sentiment = triplet['Polarity']
                    if sentiment == 'Positive':
                        positive_aspects.append(aspect)
                    elif sentiment == 'Neutral':
                        neutral_aspects.append(aspect)
                    elif sentiment == 'Negative':
                        negative_aspects.append(aspect)

    # Count top aspects for each sentiment
    top_positive_aspects = [aspect for aspect, count in Counter(positive_aspects).most_common(top_n)]
    top_neutral_aspects = [aspect for aspect, count in Counter(neutral_aspects).most_common(top_n)]
    top_negative_aspects = [aspect for aspect, count in Counter(negative_aspects).most_common(top_n)]

    # Extract opinions for each top aspect
    opinions_by_aspect = {
        'Positive': {aspect: [] for aspect in top_positive_aspects},
        'Neutral': {aspect: [] for aspect in top_neutral_aspects},
        'Negative': {aspect: [] for aspect in top_negative_aspects}
    }

    for entry in data:
        if 'Triplets' in entry:
            for triplet in entry['Triplets']:
                if 'Aspect' in triplet and 'Polarity' in triplet and 'Opinion' in triplet:
                    aspect = triplet['Aspect']
                    sentiment = triplet['Polarity']
                    opinion = triplet['Opinion']
                    if sentiment == 'Positive' and aspect in top_positive_aspects:
                        opinions_by_aspect['Positive'][aspect].append(opinion)
                    elif sentiment == 'Neutral' and aspect in top_neutral_aspects:
                        opinions_by_aspect['Neutral'][aspect].append(opinion)
                    elif sentiment == 'Negative' and aspect in top_negative_aspects:
                        opinions_by_aspect['Negative'][aspect].append(opinion)

    # Create and display wordclouds for each sentiment and each top aspect
    fig, axes = plt.subplots(3, 10, figsize=(18, 9))  # 3 rows, 5 columns for aspects and sentiments

    for idx, sentiment in enumerate(['Positive', 'Neutral', 'Negative']):
        for i, aspect in enumerate(opinions_by_aspect[sentiment]):
            # Generate wordcloud for the opinions of each aspect
            text = " ".join(opinions_by_aspect[sentiment][aspect])
            wordcloud = WordCloud(background_color='white').generate(text)

            # Plot wordcloud in corresponding subplot
            subplot_idx = idx * 10 + i  # Calculate the correct subplot index

            axes[idx, i].imshow(wordcloud, interpolation='bilinear')
            axes[idx, i].set_title(f"{aspect} - {sentiment}")
            axes[idx, i].axis("off")

    fig.suptitle(f"Word Clouds for Opinion Top {top_n} Aspects by Sentiment of {name}", fontsize=16)
    plt.tight_layout()
    plt.show()

import json
import matplotlib.pyplot as plt
from collections import Counter, defaultdict
from wordcloud import WordCloud

# File paths
file_paths = [
    "results_filtered_anis.json",
    "results_filtered_prabowo.json",
    "results_filtered_ganjar.json",
    "results_filtered_imin.json",
    "results_filtered_gibran.json",
    "results_filtered_mahfud.json"
]

# Names corresponding to the file paths
names = [
    "Anies Baswedan",
    "Prabowo Subianto",
    "Ganjar Pranowo",
    "Muhaimin Iskandar",
    "Gibran Rakabuming",
    "Mahfud MD"
]

# Number of top aspects to display
top_n = 15

# Iterate over files and extract top aspects for each sentiment
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)

    # Initialize dictionaries to store aspects for each sentiment
    positive_aspects = []
    neutral_aspects = []
    negative_aspects = []

    for entry in data:
        if 'Triplets' in entry:
            for triplet in entry['Triplets']:
                if 'Aspect' in triplet and 'Polarity' in triplet:
                    aspect = triplet['Aspect']
                    sentiment = triplet['Polarity']
                    if sentiment == 'Positive':
                        positive_aspects.append(aspect)
                    elif sentiment == 'Neutral':
                        neutral_aspects.append(aspect)
                    elif sentiment == 'Negative':
                        negative_aspects.append(aspect)

    # Count top aspects for each sentiment
    top_positive_aspects = [aspect for aspect, count in Counter(positive_aspects).most_common(top_n)]
    top_neutral_aspects = [aspect for aspect, count in Counter(neutral_aspects).most_common(top_n)]
    top_negative_aspects = [aspect for aspect, count in Counter(negative_aspects).most_common(top_n)]

    # Extract opinions for each top aspect
    opinions_by_aspect = {
        'Positive': {aspect: [] for aspect in top_positive_aspects},
        'Neutral': {aspect: [] for aspect in top_neutral_aspects},
        'Negative': {aspect: [] for aspect in top_negative_aspects}
    }

    for entry in data:
        if 'Triplets' in entry:
            for triplet in entry['Triplets']:
                if 'Aspect' in triplet and 'Polarity' in triplet and 'Opinion' in triplet:
                    aspect = triplet['Aspect']
                    sentiment = triplet['Polarity']
                    opinion = triplet['Opinion']
                    if sentiment == 'Positive' and aspect in top_positive_aspects:
                        opinions_by_aspect['Positive'][aspect].append(opinion)
                    elif sentiment == 'Neutral' and aspect in top_neutral_aspects:
                        opinions_by_aspect['Neutral'][aspect].append(opinion)
                    elif sentiment == 'Negative' and aspect in top_negative_aspects:
                        opinions_by_aspect['Negative'][aspect].append(opinion)

    # Create and display wordclouds for each sentiment and each top aspect
    fig, axes = plt.subplots(top_n, 3, figsize=(18, 5 * top_n))  # top_n rows, 3 columns for sentiments

    for i, sentiment in enumerate(['Positive', 'Neutral', 'Negative']):
        for j, aspect in enumerate(opinions_by_aspect[sentiment]):
            # Generate wordcloud for the opinions of each aspect with collocations enabled for bigrams
            text = " ".join(opinions_by_aspect[sentiment][aspect])
            wordcloud = WordCloud(width=800, height=400, background_color='white', collocations=True).generate(text)

            # Plot wordcloud in corresponding subplot
            axes[j, i].imshow(wordcloud, interpolation='bilinear')
            axes[j, i].set_title(f"{aspect} - {sentiment}", fontsize=18)
            axes[j, i].axis("off")

    # fig.suptitle(f"Word Clouds for Opinion Top {top_n} Aspects by Sentiment of {name}", fontsize=20)
    # fig.subplots_adjust(hspace=0.3)
    plt.tight_layout()
    plt.show()

import json
import matplotlib.pyplot as plt
from collections import Counter
from wordcloud import WordCloud

# File paths
file_paths = [
    "results_filtered_anis.json",
    "results_filtered_prabowo.json",
    "results_filtered_ganjar.json",
    "results_filtered_imin.json",
    "results_filtered_gibran.json",
    "results_filtered_mahfud.json"
]

# Names corresponding to the file paths
names = [
    "Anies Baswedan",
    "Prabowo Subianto",
    "Ganjar Pranowo",
    "Muhaimin Iskandar",
    "Gibran Rakabuming",
    "Mahfud MD"
]

# Number of top aspects to display
top_n = 10

# Iterate over files and extract top aspects for each sentiment
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)

    # Initialize dictionaries to store aspects for each sentiment
    positive_aspects = []
    neutral_aspects = []
    negative_aspects = []

    for entry in data:
        if 'Triplets' in entry:
            for triplet in entry['Triplets']:
                if 'Aspect' in triplet and 'Polarity' in triplet:
                    aspect = triplet['Aspect']
                    sentiment = triplet['Polarity']
                    if sentiment == 'Positive':
                        positive_aspects.append(aspect)
                    elif sentiment == 'Neutral':
                        neutral_aspects.append(aspect)
                    elif sentiment == 'Negative':
                        negative_aspects.append(aspect)

    # Count top aspects for each sentiment
    top_positive_aspects = [aspect for aspect, count in Counter(positive_aspects).most_common(top_n)]
    top_neutral_aspects = [aspect for aspect, count in Counter(neutral_aspects).most_common(top_n)]
    top_negative_aspects = [aspect for aspect, count in Counter(negative_aspects).most_common(top_n)]

    # Extract opinions for each top aspect
    opinions_by_aspect = {
        'Positive': {aspect: [] for aspect in top_positive_aspects},
        'Neutral': {aspect: [] for aspect in top_neutral_aspects},
        'Negative': {aspect: [] for aspect in top_negative_aspects}
    }

    for entry in data:
        if 'Triplets' in entry:
            for triplet in entry['Triplets']:
                if 'Aspect' in triplet and 'Polarity' in triplet and 'Opinion' in triplet:
                    aspect = triplet['Aspect']
                    sentiment = triplet['Polarity']
                    opinion = triplet['Opinion']
                    if sentiment == 'Positive' and aspect in top_positive_aspects:
                        opinions_by_aspect['Positive'][aspect].append(opinion)
                    elif sentiment == 'Neutral' and aspect in top_neutral_aspects:
                        opinions_by_aspect['Neutral'][aspect].append(opinion)
                    elif sentiment == 'Negative' and aspect in top_negative_aspects:
                        opinions_by_aspect['Negative'][aspect].append(opinion)

    # Create and display wordclouds for each sentiment and each top aspect
    fig, axes = plt.subplots(top_n, 3, figsize=(15, 5))  # top_n rows, 3 columns for sentiments

    # Ensure axes is 2D array even if top_n is 1
    if top_n == 1:
        axes = [axes]

    for i, sentiment in enumerate(['Positive', 'Neutral', 'Negative']):
        for j, aspect in enumerate(opinions_by_aspect[sentiment]):
            # Generate wordcloud for the opinions of each aspect with collocations enabled for bigrams
            text = " ".join(opinions_by_aspect[sentiment][aspect])
            wordcloud = WordCloud(width=800, height=400, background_color='white', collocations=True).generate(text)

            # Plot wordcloud in corresponding subplot
            axes[j][i].imshow(wordcloud, interpolation='bilinear')
            axes[j][i].set_title(f"{aspect} - {sentiment}", fontsize=18)
            axes[j][i].axis("off")

    fig.suptitle(f"Word Clouds for Opinion Top Aspects by Sentiment of {name}", fontsize=20)
    fig.subplots_adjust(hspace=0.3)
    plt.tight_layout()
    plt.show()

# prompt: dari code diatas buat wordcloud opini untuk setiap top aspeknya dan gabung menjadi 1 figur untuk masing kandidat

import ast
import csv
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np
import requests
import json
import random
import os
from collections import defaultdict
import pandas as pd
from wordcloud import WordCloud
import re
import string
from PIL import Image
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import classification_report, f1_score
from sklearn.metrics import confusion_matrix, classification_report, f1_score

# ... (previous code remains the same)

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Names corresponding to the file paths
names = ["Anies Baswedan", "Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"]

# Number of top aspects to display
top_n = 5

# Iterate over files and extract top aspects with opinions
top_aspects_with_opinions = []
for file_path in file_paths:
    with open(file_path, "r") as file:
        data = json.load(file)
        aspects_with_opinions = defaultdict(list)
        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet and 'Opinion' in triplet:
                        aspect = triplet['Aspect']
                        opinion = triplet['Opinion']
                        aspects_with_opinions[aspect].append(opinion)
        top_aspects = sorted(aspects_with_opinions.items(), key=lambda item: len(item[1]), reverse=True)[:top_n]
        top_aspects_with_opinions.append(top_aspects)

# Create word clouds and combine them into figures
for i, (name, top_aspects) in enumerate(zip(names, top_aspects_with_opinions)):
    fig, axes = plt.subplots(1, top_n, figsize=(20, 4))  # 1 row, top_n columns
    fig.suptitle(f"Word Clouds for Top {top_n} Aspects of {name}", fontsize=16)

    for j, (aspect, opinions) in enumerate(top_aspects):
        text = " ".join(opinions)
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        axes[j].imshow(wordcloud, interpolation='bilinear')
        axes[j].set_title(aspect)
        axes[j].axis("off")

    plt.tight_layout()
    plt.show()

import json
from collections import defaultdict
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Names corresponding to the file paths
names = ["Anies Baswedan", "Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"]

# Specify aspects for which you want to generate Word Clouds
specific_aspects = {
    "Anies Baswedan": ["ngomong", "Aspect2"],  # Replace with actual aspects for Anies Baswedan
    "Prabowo Subianto": ["Aspect1", "Aspect2"],  # Replace with actual aspects for Prabowo Subianto
    "Ganjar Pranowo": ["Aspect1", "Aspect2"],  # Replace with actual aspects for Ganjar Pranowo
    "Muhaimin Iskandar": ["Aspect1", "Aspect2"],  # Replace with actual aspects for Muhaimin Iskandar
    "Gibran Rakabuming": ["Aspect1", "Aspect2"],  # Replace with actual aspects for Gibran Rakabuming
    "Mahfud MD": ["Aspect1", "Aspect2"]  # Replace with actual aspects for Mahfud MD
}

# Create word clouds and combine them into figures
for file_path, name in zip(file_paths, names):
    with open(file_path, "r") as file:
        data = json.load(file)
        aspects_with_opinions = defaultdict(list)
        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet and 'Opinion' in triplet:
                        aspect = triplet['Aspect']
                        opinion = triplet['Opinion']
                        if aspect in specific_aspects[name]:
                            aspects_with_opinions[aspect].append(opinion)

    # Create word clouds for specified aspects
    top_aspects = specific_aspects[name]
    fig, axes = plt.subplots(1, len(top_aspects), figsize=(20, 4))  # 1 row, len(top_aspects) columns
    fig.suptitle(f"Word Clouds for Specified Aspects of {name}", fontsize=16)

    for j, aspect in enumerate(top_aspects):
        opinions = aspects_with_opinions.get(aspect, [])
        text = " ".join(opinions)
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        axes[j].imshow(wordcloud, interpolation='bilinear')
        axes[j].set_title(aspect)
        axes[j].axis("off")

    plt.tight_layout()
    plt.show()

import json
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# File paths
file_paths = [
    "results_filtered_anis.json",
    "results_filtered_prabowo.json",
    "results_filtered_ganjar.json",
    "results_filtered_imin.json",
    "results_filtered_gibran.json",
    "results_filtered_mahfud.json"
]

# Names corresponding to the file paths
names = [
    "Anies Baswedan",
    "Prabowo Subianto",
    "Ganjar Pranowo",
    "Muhaimin Iskandar",
    "Gibran Rakabuming",
    "Mahfud MD"
]

# Specify the individual, aspect, and sentiment you want to focus on
target_individual = "Anies Baswedan"  # Change this to the desired individual
target_aspect = "ngomong"  # Change this to the desired aspect
target_sentiment = "Neutral"  # Change this to the desired sentiment

# Find the index of the target individual
try:
    file_path = file_paths[names.index(target_individual)]
except ValueError:
    raise ValueError(f"Individual {target_individual} not found in the list.")

# Load data
with open(file_path, "r") as file:
    data = json.load(file)

# Initialize a list to store opinions for the target aspect and sentiment
opinions = []

# Extract opinions for the specified aspect and sentiment
for entry in data:
    if 'Triplets' in entry:
        for triplet in entry['Triplets']:
            if 'Aspect' in triplet and 'Polarity' in triplet and 'Opinion' in triplet:
                aspect = triplet['Aspect']
                sentiment = triplet['Polarity']
                opinion = triplet['Opinion']
                if aspect == target_aspect and sentiment == target_sentiment:
                    opinions.append(opinion)

# Generate word cloud for the specified aspect and sentiment
text = " ".join(opinions)
wordcloud = WordCloud(width=800, height=400, background_color='white', collocations=True).generate(text)

# Plot the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title(f"Word Cloud for {target_aspect} - {target_sentiment} of {target_individual}", fontsize=20)
plt.axis("off")
plt.tight_layout()
plt.show()

import json
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# File paths and names
file_paths = [
    "results_filtered_anis.json",
    "results_filtered_prabowo.json",
    "results_filtered_ganjar.json",
    "results_filtered_imin.json",
    "results_filtered_gibran.json",
    "results_filtered_mahfud.json"
]

names = [
    "Anies Baswedan",
    "Prabowo Subianto",
    "Ganjar Pranowo",
    "Muhaimin Iskandar",
    "Gibran Rakabuming",
    "Mahfud MD"
]

# Define targets
targets = [
    {"individual": "Anies Baswedan", "aspect": "kerja", "sentiment": "Positive"},
    {"individual": "Prabowo Subianto", "aspect": "hati", "sentiment": "Positive"},
    {"individual": "Ganjar Pranowo", "aspect": "teori", "sentiment": "Positive"},
    {"individual": "Muhaimin Iskandar", "aspect": "visi dan misi", "sentiment": "Positive"},
    {"individual": "Gibran Rakabuming", "aspect": "anak muda", "sentiment": "Positive"},
    {"individual": "Mahfud MD", "aspect": "kontrol waktu", "sentiment": "Positive"},
    {"individual": "Anies Baswedan", "aspect": "Kejeniusan", "sentiment": "Neutral"},
    {"individual": "Prabowo Subianto", "aspect": "kerjanya", "sentiment": "Neutral"},
    {"individual": "Ganjar Pranowo", "aspect": "jateng", "sentiment": "Neutral"},
    {"individual": "Muhaimin Iskandar", "aspect": "bahasanya", "sentiment": "Neutral"},
    {"individual": "Gibran Rakabuming", "aspect": "suara", "sentiment": "Neutral"},
    {"individual": "Mahfud MD", "aspect": "solusi", "sentiment": "Neutral"},
    {"individual": "Anies Baswedan", "aspect": "muka", "sentiment": "Negative"},
    {"individual": "Prabowo Subianto", "aspect": "etika", "sentiment": "Negative"},
    {"individual": "Ganjar Pranowo", "aspect": "muka", "sentiment": "Negative"},
    {"individual": "Muhaimin Iskandar", "aspect": "visi misi", "sentiment": "Negative"},
    {"individual": "Gibran Rakabuming", "aspect": "etika", "sentiment": "Negative"},
    {"individual": "Mahfud MD", "aspect": "JAWAB KEJELEKAN", "sentiment": "Negative"}
]

# Iterate through targets and generate word clouds
for target in targets:
    try:
        file_path = file_paths[names.index(target["individual"])]
    except ValueError:
        raise ValueError(f"Individual {target['individual']} not found in the list.")

    # Load data
    with open(file_path, "r") as file:
        data = json.load(file)

    # Initialize a list to store opinions for the target aspect and sentiment
    opinions = []

    # Extract opinions for the specified aspect and sentiment
    for entry in data:
        if 'Triplets' in entry:
            for triplet in entry['Triplets']:
                if 'Aspect' in triplet and 'Polarity' in triplet and 'Opinion' in triplet:
                    aspect = triplet['Aspect']
                    sentiment = triplet['Polarity']
                    opinion = triplet['Opinion']
                    if aspect == target["aspect"] and sentiment == target["sentiment"]:
                        opinions.append(opinion)

    # Generate word cloud for the specified aspect and sentiment
    if opinions:
        text = " ".join(opinions)
        wordcloud = WordCloud(width=800, height=400, background_color='white', collocations=True).generate(text)

        # Plot the word cloud
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.title(f"Word Cloud for {target['aspect']} - {target['sentiment']} of {target['individual']}", fontsize=20)
        plt.axis("off")
        plt.tight_layout()
        plt.show()
    else:
        print(f"No opinions found for {target['aspect']} - {target['sentiment']} of {target['individual']}")

import json
import matplotlib.pyplot as plt
import numpy as np
from collections import Counter

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Names corresponding to each file
names = ["Anies Baswedan", "Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"]

# Initialize aspect counts for each person
aspect_counts = {name: Counter() for name in names}

# Iterate over files and count aspects
for i, file_path in enumerate(file_paths):
    name = names[i]
    with open(file_path, "r") as file:
        data = json.load(file)
        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet:
                        aspect = triplet['Aspect']
                        aspect_counts[name][aspect] += 1
                    else:
                        print(f"Warning: 'Aspect' key missing in entry for {name}")

# Get top 3 aspects for each candidate
top_aspects_per_candidate = {name: aspect_counts[name].most_common(3) for name in names}

# Prepare data for plot
candidates = []
aspects = []
counts = []

# Collect data for each candidate
for name in names:
    for aspect, count in top_aspects_per_candidate[name]:
        candidates.append(name)
        aspects.append(aspect)
        counts.append(count)

# Create color map for bars
colors = plt.cm.tab20.colors[:len(names)] * 3  # Generate colors for each candidate

# Group data by candidate and aspect
grouped_data = {}
for candidate, aspect, count in zip(candidates, aspects, counts):
    if candidate not in grouped_data:
        grouped_data[candidate] = {}
    grouped_data[candidate][aspect] = count

# Prepare data for stacked bar plot
candidate_labels = list(grouped_data.keys())
aspect_labels = list(grouped_data[candidate_labels[0]].keys())
bar_data = [[grouped_data[candidate].get(aspect, 0) for aspect in aspect_labels] for candidate in candidate_labels]

# Create stacked bar chart
fig, ax = plt.subplots(figsize=(12, 8))

bar_width = 0.35  # Adjust bar width as needed
index = np.arange(len(aspect_labels))

for i, candidate in enumerate(candidate_labels):
    bars = ax.bar(index + i * bar_width, bar_data[i], width=bar_width, color=colors[i], label=candidate)

# Add aspect labels on top of bars
for bar, aspect_label in zip(bars, aspect_labels):
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.05, aspect_label, ha='center', va='bottom', rotation=90)

# Adding Xticks
plt.xlabel('Aspects')
plt.ylabel('Aspect Counts')
plt.title('Top 3 Aspects Comparison for Individuals')
plt.xticks(index + bar_width / 2, aspect_labels)
plt.legend()

plt.tight_layout()
plt.show()

import json
import matplotlib.pyplot as plt
from collections import Counter

# File paths
file_paths = ["results_filtered_anis.json", "results_filtered_prabowo.json", "results_filtered_ganjar.json", "results_filtered_imin.json", "results_filtered_gibran.json", "results_filtered_mahfud.json"]

# Names corresponding to each file
names = ["Anies Baswedan", "Prabowo Subianto", "Ganjar Pranowo", "Muhaimin Iskandar", "Gibran Rakabuming", "Mahfud MD"]

# Initialize aspect counts for each person
aspect_counts = {name: Counter() for name in names}

# Iterate over files and count aspects
for i, file_path in enumerate(file_paths):
    name = names[i]
    with open(file_path, "r") as file:
        data = json.load(file)
        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Aspect' in triplet:
                        aspect = triplet['Aspect']
                        aspect_counts[name][aspect] += 1
                    else:
                        print(f"Warning: 'Aspect' key missing in entry for {name}")

# Function to plot top aspects for each candidate
def plot_top_aspects(name, aspects, counts):
    plt.figure(figsize=(10, 6))
    plt.bar(aspects, counts, color='skyblue')
    plt.xlabel('Aspects')
    plt.ylabel('Count')
    plt.title(f'Top Aspects for {name}')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

# Create a separate plot for each candidate
for name in names:
    top_aspects = aspect_counts[name].most_common(5)  # Change the number to get top N aspects
    aspects, counts = zip(*top_aspects)
    plot_top_aspects(name, aspects, counts)

import json
import matplotlib.pyplot as plt
import numpy as np

# File paths
file_paths = ["results_filtered_gibran.json", "results_filtered_imin.json", "results_filtered_mahfud.json"]

# Initialize sentiment counts for each person
sentiment_counts = {
    "Gibran": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Imin": {"Positive": 0, "Neutral": 0, "Negative": 0},
    "Mahfud": {"Positive": 0, "Neutral": 0, "Negative": 0}
}

# Iterate over files and count sentiments
for i, file_path in enumerate(file_paths):
    name = ["Gibran", "Imin", "Mahfud"][i]
    with open(file_path, "r") as file:
        data = json.load(file)
        for entry in data:
            if 'Triplets' in entry:
                for triplet in entry['Triplets']:
                    if 'Polarity' in triplet:
                        sentiment = triplet['Polarity']
                        sentiment_counts[name][sentiment] += 1
                    else:
                        print(f"Warning: 'Polarity' key missing in entry for {name}")

# Prepare data for plot
names = list(sentiment_counts.keys())
sentiment_labels = ['Positive', 'Neutral', 'Negative']
counts = np.array([[sentiment_counts[name][label] for label in sentiment_labels] for name in names])

# Define colors for each sentiment
colors = {'Positive': 'green', 'Neutral': 'yellow', 'Negative': 'red'}

# Create stacked bar chart
fig, ax = plt.subplots()

# Plot each bar separately to stack them with defined colors
bars = []
bottom = np.zeros(len(names))  # Starting point for bottom of bars
for i, label in enumerate(sentiment_labels):
    bar = ax.bar(names, counts[:, i], bottom=bottom, color=colors[label], label=label)
    bars.append(bar)
    bottom += counts[:, i]  # Update bottom for next iteration

# Add value labels on top of bars
# for bar in bars:
#     for b in bar:
#         yval = b.get_height()
#         plt.text(b.get_x() + b.get_width()/2, yval + 0.05, round(yval, 2), ha='center', va='bottom')

# Add legend
ax.legend()

plt.xlabel('Names')
plt.ylabel('Sentiment Counts')
plt.title('Stacked Sentiment Comparison for Individuals')
plt.tight_layout()
plt.show()

import json
import matplotlib.pyplot as plt
from collections import Counter
from wordcloud import WordCloud
import numpy as np

# Load data dari JSON
input_file = "results_filtered_prabowo.json"
with open(input_file, "r") as file:
    data = json.load(file)

# Mengumpulkan aspek dan opini berdasarkan sentimen
aspects_pos, aspects_neu, aspects_neg = [], [], []
opinions_pos, opinions_neu, opinions_neg = [], [], []

for obj in data:
    for triplet in obj["Triplets"]:
        aspect = triplet["Aspect"]
        opinion = triplet["Opinion"]
        polarity = triplet["Polarity"]
        if polarity == "Positive":
            aspects_pos.append(aspect)
            opinions_pos.append(opinion)
        elif polarity == "Neutral":
            aspects_neu.append(aspect)
            opinions_neu.append(opinion)
        elif polarity == "Negative":
            aspects_neg.append(aspect)
            opinions_neg.append(opinion)

# Fungsi untuk plot top 10 aspek berdasarkan sentimen
def plot_top_aspects(aspects, sentiment_label, color):
    aspect_counts = Counter(aspects)
    top_aspects = aspect_counts.most_common(3)

    labels, values = zip(*top_aspects)
    x = np.arange(len(labels))

    plt.figure(figsize=(12, 8))
    plt.bar(x, values, color=color)
    plt.xlabel('Aspects')
    plt.ylabel('Frequency')
    plt.title(f'Top 10 Aspects ({sentiment_label})')
    plt.xticks(x, labels, rotation='vertical')
    plt.tight_layout()
    plt.show()

    return [aspect for aspect, _ in top_aspects]

# Fungsi untuk membuat wordcloud dari opini
def create_wordcloud(opinions, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(opinions))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title)
    plt.axis('off')
    plt.show()

# Plot top 10 aspek berdasarkan sentimen
top_aspects_pos = plot_top_aspects(aspects_pos, 'Positif', '#006400')
top_aspects_neu = plot_top_aspects(aspects_neu, 'Netral', '#FF8C00')
top_aspects_neg = plot_top_aspects(aspects_neg, 'Negatif', '#8B0000')

# Mengumpulkan opini dari top aspek
top_opinions_pos = [opinion for aspect, opinion in zip(aspects_pos, opinions_pos) if aspect in top_aspects_pos]
top_opinions_neu = [opinion for aspect, opinion in zip(aspects_neu, opinions_neu) if aspect in top_aspects_neu]
top_opinions_neg = [opinion for aspect, opinion in zip(aspects_neg, opinions_neg) if aspect in top_aspects_neg]

# Membuat wordcloud untuk setiap sentimen
create_wordcloud(aspects_pos, "Wordcloud Aspek (Positif)")
create_wordcloud(opinions_pos, "Wordcloud Opini (Positif)")

create_wordcloud(aspects_neu, "Wordcloud Aspek (Netral)")
create_wordcloud(opinions_neu, "Wordcloud Opini (Netral)")

create_wordcloud(aspects_neg, "Wordcloud Aspek (Negatif)")
create_wordcloud(opinions_neg, "Wordcloud Opini (Negatif)")

# Fungsi untuk membuat wordcloud opini berdasarkan top aspek
def create_wordcloud(opinions, title):
    text = ' '.join(opinions)
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title)
    plt.axis('off')
    plt.tight_layout(pad=0)
    plt.show()

# Ekstraksi dan plot wordcloud untuk opini berdasarkan top aspek
for aspect in top_aspects_pos:
    opinions_for_aspect = [opinion for aspect_item, opinion in zip(aspects_pos, opinions_pos) if aspect_item == aspect]
    create_wordcloud(opinions_for_aspect, f'Word Cloud for Top Aspect: {aspect} (Positive)')

for aspect in top_aspects_neu:
    opinions_for_aspect = [opinion for aspect_item, opinion in zip(aspects_neu, opinions_neu) if aspect_item == aspect]
    create_wordcloud(opinions_for_aspect, f'Word Cloud for Top Aspect: {aspect} (Neutral)')

for aspect in top_aspects_neg:
    opinions_for_aspect = [opinion for aspect_item, opinion in zip(aspects_neg, opinions_neg) if aspect_item == aspect]
    create_wordcloud(opinions_for_aspect, f'Word Cloud for Top Aspect: {aspect} (Negative)')

# Fungsi untuk plot distribusi sentimen
def plot_sentiment_distribution(freq_pos, freq_neu, freq_neg):
    sentiment_counts = [freq_pos, freq_neu, freq_neg]
    sentiment_labels = ['Positif', 'Netral', 'Negatif']
    bars = plt.bar(sentiment_labels, sentiment_counts, color=['green', 'orange', 'red'])
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2, height, str(int(height)), ha='center', va='bottom')

    plt.legend(sentiment_labels)
    plt.title("Distribusi Sentimen")
    plt.show()

# Hitung frekuensi sentimen
freq_pos = len(aspects_pos)
freq_neu = len(aspects_neu)
freq_neg = len(aspects_neg)

# Plot distribusi sentimen
plot_sentiment_distribution(freq_pos, freq_neu, freq_neg)

# Fungsi untuk membuat WordCloud dari semua aspek
def create_aspect_wordcloud(aspects):
    all_aspects_text = ' '.join(aspects)
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_aspects_text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title('WordCloud Aspek (Tanpa Sentimen)')
    plt.axis('off')
    plt.show()

# Gabungkan semua aspek dari berbagai sentimen
all_aspects = aspects_pos + aspects_neu + aspects_neg

# Buat WordCloud untuk semua aspek
create_aspect_wordcloud(all_aspects)

aspects = []
for obj in data:
    for triplet in obj["Triplets"]:
        aspect = triplet["Aspect"]
        opinion = triplet["Opinion"]
        polarity = triplet["Polarity"]
        aspects.append(aspect)
        if polarity == "Positive":
            opinions_pos.append(opinion)
        elif polarity == "Neutral":
            opinions_neu.append(opinion)
        elif polarity == "Negative":
            opinions_neg.append(opinion)

# Fungsi untuk plot top 10 aspek
def plot_top_aspects(aspects):
    aspect_counts = Counter(aspects)
    top_aspects = aspect_counts.most_common(10)

    labels, values = zip(*top_aspects)
    x = np.arange(len(labels))

    plt.figure(figsize=(12, 8))
    plt.bar(x, values, color='blue')
    plt.xlabel('Aspects')
    plt.ylabel('Frequency')
    plt.title('Top 10 Aspects (Overall)')
    plt.xticks(x, labels, rotation='vertical')
    plt.tight_layout()
    plt.show()

    return [aspect for aspect, _ in top_aspects]

top_aspects = plot_top_aspects(aspects)

# Plot sentimen dari top 10 aspek secara keseluruhan
# Fungsi untuk plot bar bertumpuk membandingkan frekuensi sentimen untuk 10 aspek paling umum
def plot_sentiment_comparison(aspects, sentiments, sentiment_labels):
    aspect_sentiment_frequency = {}

    for aspect, sentiment in zip(aspects, sentiments):
        if aspect not in aspect_sentiment_frequency:
            aspect_sentiment_frequency[aspect] = {label: 0 for label in sentiment_labels}
        aspect_sentiment_frequency[aspect][sentiment] += 1

    # Ambil 10 aspek dengan frekuensi tertinggi secara keseluruhan
    overall_aspect_frequency = {aspect: sum(sentiments.values()) for aspect, sentiments in aspect_sentiment_frequency.items()}
    top_10_aspects_overall = Counter(overall_aspect_frequency).most_common(10)

    # Extract aspects and their sentiment frequencies
    top_aspects = [aspect[0] for aspect in top_10_aspects_overall]
    top_aspects_frequencies = [aspect_sentiment_frequency[aspect] for aspect in top_aspects]

    # Buat plot bar bertumpuk untuk membandingkan frekuensi sentimen untuk 10 aspek paling umum
    x = np.arange(len(top_aspects))  # Label locations
    width = 0.5  # Width of the bars

    fig, ax = plt.subplots(figsize=(14, 8))

    # Sentimen counts
    pos_counts = [freq['Positive'] for freq in top_aspects_frequencies]
    neu_counts = [freq['Neutral'] for freq in top_aspects_frequencies]
    neg_counts = [freq['Negative'] for freq in top_aspects_frequencies]

    # Buat plot bertumpuk
    ax.bar(x, pos_counts, width, label='Positive', color='g')
    ax.bar(x, neu_counts, width, label='Neutral', color='y', bottom=pos_counts)
    ax.bar(x, neg_counts, width, label='Negative', color='r', bottom=np.array(pos_counts) + np.array(neu_counts))

    # Add some text for labels, title and custom x-axis tick labels, etc.
    ax.set_xlabel('Aspects')
    ax.set_ylabel('Sentiment Counts')
    ax.set_title('Sentiment Comparison for Top 10 Aspects')
    ax.set_xticks(x)
    ax.set_xticklabels(top_aspects, rotation=45, ha='right')
    ax.legend()

    fig.tight_layout()
    plt.show()
# Plot perbandingan sentimen dari top 10 aspek secara keseluruhan
plot_sentiment_comparison(aspects_pos + aspects_neu + aspects_neg,
                          ['Positive'] * len(aspects_pos) + ['Neutral'] * len(aspects_neu) + ['Negative'] * len(aspects_neg),
                          ['Positive', 'Neutral', 'Negative'])

import json
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Muat data JSON
with open('data.json', 'r') as file:
    data = json.load(file)

# Gabungkan n-grams dengan tanda penghubung
combined_text = ' '.join(data['opini']).replace(' ', ' ')

# Generate wordcloud
wordcloud = WordCloud(collocations=True, background_color='white', max_words=200).generate(combined_text)

# Display wordcloud
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

